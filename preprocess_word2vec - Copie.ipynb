{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import time\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize,blankline_tokenize\n",
    "from nltk import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import json\n",
    "import re as regex\n",
    "import xgboost as xgb\n",
    "from sklearn import model_selection, preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_Initialize():\n",
    "    data = []\n",
    "    processed_data = []\n",
    "    wordlist = []\n",
    "\n",
    "    featureList = []\n",
    "    fea_vect=[]\n",
    "    \n",
    "    data_model = None\n",
    "    data_labels = None\n",
    "    is_testing = False\n",
    "    \n",
    "    def initialize(self, csv_file, is_testing_set=False, from_cached=None):\n",
    "        if from_cached is not None:\n",
    "            #self.data_model = pd.read_csv(from_cached)\n",
    "            self.data_model = pd.read_json(from_cached)\n",
    "\n",
    "            return\n",
    "\n",
    "        self.is_testing = is_testing_set\n",
    "\n",
    "        if not is_testing_set:\n",
    "            #self.data = pd.read_csv(csv_file, header=0, names=[\"id\", \"emotion\", \"text\"])\n",
    "            self.data = pd.read_json(csv_file)\n",
    "\n",
    "            #self.data = self.data[self.data[\"emotion\"].isin([\"positive\", \"negative\", \"neutral\"])]\n",
    "        \n",
    "\n",
    "        self.processed_data = self.data\n",
    "        self.wordlist = []\n",
    "        self.data_model = None\n",
    "        self.data_labels = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def do_process(self):\n",
    "        start_time = time.time()\n",
    "        def stem_and_join(row,stemmer=nltk.PorterStemmer()):\n",
    "            row[\"spans\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"spans\"]))\n",
    "            return row\n",
    "    \n",
    "        def tokenize_grams(row):\n",
    "                \n",
    "                # turn a doc into clean tokens\n",
    "                def clean_doc(doc):\n",
    "                    # split into tokens by white space\n",
    "                    tokens = doc.split()\n",
    "                    # remove punctuation from each token\n",
    "                    table = str.maketrans('', '', string.punctuation)\n",
    "                    tokens = [w.translate(table) for w in tokens]\n",
    "                    # remove remaining tokens that are not alphabetic\n",
    "                    tokens = [word for word in tokens if word.isalpha()]\n",
    "                    # filter out stop words\n",
    "                    #whitelist=[\"to\",\"on\",\"n't\",\"not\",\"don't\",\"for\",\"up\",\"below\",\"short\",\"long\"]\n",
    "                    whitelist=[]\n",
    "                    stop_words = set(stopwords.words('english'))\n",
    "                    \n",
    "                    tokens = [w for w in tokens if (not w in stop_words or w in whitelist)]\n",
    "                    # filter out short tokens\n",
    "                    tokens = [word.lower() for word in tokens if len(word) > 1]\n",
    "                    return tokens\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "               \n",
    "                # Function to apply lemmatization to a list of words\n",
    "                def words_lemmatizer(words, encoding=\"utf8\"):\n",
    "                    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "                    lemma_words = []\n",
    "                    wl = WordNetLemmatizer()\n",
    "                    for word in words:\n",
    "                        pos = find_pos(word)\n",
    "                        lemma_words.append(wl.lemmatize(word, pos))\n",
    "                    return lemma_words\n",
    "                \n",
    "                # Function to find part of speech tag for a word\n",
    "                def find_pos(word):\n",
    "                    # Part of Speech constants\n",
    "                    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "   \n",
    "                    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "                    # Adjective tags - 'JJ', 'JJR', 'JJS'\n",
    "                    if pos.lower()[0] == 'j':\n",
    "                        return 'a'\n",
    "                    # Adverb tags - 'RB', 'RBR', 'RBS'\n",
    "                    elif pos.lower()[0] == 'r':\n",
    "                        return 'r'\n",
    "                    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "                    elif pos.lower()[0] == 'v':\n",
    "                        return 'v'\n",
    "                    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'\n",
    "                    else:\n",
    "                        return 'n'\n",
    "    \n",
    "            \n",
    "                #convert to string\n",
    "                idx=row[\"spans\"]\n",
    "                #ch=\"\".join(x for x in idx if x)\n",
    "                ch=' '.join(idx)\n",
    "               \n",
    "    \n",
    "                \n",
    "                \n",
    "                n_grams=clean_doc(ch)\n",
    "                tokens=words_lemmatizer(n_grams)\n",
    "\n",
    "                #print(\"nombre of tokens\",len(n_grams))\n",
    "\n",
    "                row[\"token_spans\"] = tokens\n",
    "    \n",
    "                return row\n",
    "        #self.processed_data = self.processed_data.apply(stem_and_join, axis=1)\n",
    "        self.processed_data = self.processed_data.apply(tokenize_grams, axis=1)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    \n",
    "\n",
    "    def build_wordlist(self, min_occurrences=0, max_occurences=500, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                       ):\n",
    "       \n",
    "        self.wordlist = []\n",
    "        whitelist=[]\n",
    "        \n",
    "        stopwords=[]\n",
    "       \n",
    "        words = Counter()\n",
    "      \n",
    "        for idx in self.processed_data.index:\n",
    "            words.update(self.processed_data.loc[idx, \"token_spans\"])\n",
    "\n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "        \n",
    "        \n",
    "        print(words.most_common())        \n",
    "\n",
    "        word_df = pd.DataFrame(data={\"word\": [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                     \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                               columns=[\"word\", \"occurrences\"])\n",
    "\n",
    "        word_df.to_csv(\"wordlist_copie.csv\", index_label=\"idx\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build_data_model(self):\n",
    "        label_column = []\n",
    "        Id_column=[\"ID\"]\n",
    "        label_column = [\"label\"]\n",
    "\n",
    "        columns = Id_column + label_column + list(\n",
    "            map(lambda w: w ,self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "\n",
    "            if True:\n",
    "                # add label\n",
    "                current_label = self.processed_data.loc[idx, \"sentiment score\"]\n",
    "                current_id = self.processed_data.loc[idx, \"id\"]\n",
    "                \n",
    "                labels.append(current_id)\n",
    "                labels.append(current_label)\n",
    "                \n",
    "                current_row.append(current_id)\n",
    "                current_row.append(current_label)\n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"token_spans\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        return self.data_model, self.data_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_Initialize_test(TwitterData_Initialize):\n",
    "\n",
    "    \n",
    "    \n",
    "    def do_process(self):\n",
    "        def stem_and_join(row,stemmer=nltk.PorterStemmer()):\n",
    "            \n",
    "            row[\"spans\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"spans\"]))\n",
    "            return row\n",
    "    \n",
    "        def tokenize_grams(row):\n",
    "                \n",
    "                # Function to remove stop words\n",
    "                def remove_stopwords(text, lang='english'):\n",
    "                    whitelist = [\"n't\",\"not\",\"below\"]    \n",
    "\n",
    "                    stop_words = set(stopwords.words('english'))\n",
    "                    word_tokens = word_tokenize(text)\n",
    "                    #filtered_sentence = [w for w in word_tokens if ((not w in stop_words) or (w in whitelist))]\n",
    "                    filtered_sentence = []\n",
    "                    for w in word_tokens:\n",
    "                        if ((w not in stop_words) or (w in whitelist)):\n",
    "                            filtered_sentence.append(w)\n",
    "                \n",
    "                    ch=\" \".join(filtered_sentence)\n",
    "\n",
    "                    return ch\n",
    "#40\n",
    "\n",
    "                def clean_doc(doc):\n",
    "                    # split into tokens by white space\n",
    "                    tokens = doc.split()\n",
    "                    # remove punctuation from each token\n",
    "                    table = str.maketrans('', '', string.punctuation)\n",
    "                    tokens = [w.translate(table) for w in tokens]\n",
    "                    # remove remaining tokens that are not alphabetic\n",
    "                    tokens = [word for word in tokens if word.isalpha()]\n",
    "                    # filter out stop words\n",
    "                    #whitelist=[\"to\",\"on\",\"n't\",\"not\",\"don't\",\"for\",\"up\",\"below\",\"short\",\"long\"]\n",
    "                    whitelist=[]\n",
    "                    stop_words = set(stopwords.words('english'))\n",
    "                    \n",
    "                    tokens = [w for w in tokens if (not w in stop_words or w in whitelist)]\n",
    "                    # filter out short tokens\n",
    "                    tokens = [word.lower() for word in tokens if len(word) > 1]\n",
    "                    return tokens\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                # Function to apply lemmatization to a list of words\n",
    "                def words_lemmatizer(words, encoding=\"utf8\"):\n",
    "                    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "                    lemma_words = []\n",
    "                    wl = WordNetLemmatizer()\n",
    "                    for word in words:\n",
    "                        pos = find_pos(word)\n",
    "                        lemma_words.append(wl.lemmatize(word, pos))\n",
    "                    return lemma_words\n",
    "\n",
    "                # Function to find part of speech tag for a word\n",
    "                def find_pos(word):\n",
    "                    # Part of Speech constants\n",
    "                    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "   \n",
    "                    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "                    # Adjective tags - 'JJ', 'JJR', 'JJS'\n",
    "                    if pos.lower()[0] == 'j':\n",
    "                        return 'a'\n",
    "                    # Adverb tags - 'RB', 'RBR', 'RBS'\n",
    "                    elif pos.lower()[0] == 'r':\n",
    "                        return 'r'\n",
    "                    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "                    elif pos.lower()[0] == 'v':\n",
    "                        return 'v'\n",
    "                    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'\n",
    "                    else:\n",
    "                        return 'n'\n",
    "    \n",
    "            \n",
    "                token=[]\n",
    "                idx=row[\"spans\"]\n",
    "                ch=' '.join(idx)\n",
    "\n",
    "\n",
    "    \n",
    "                #words_stemmer(ch, type=\"PorterStemmer\", lang=\"english\", encoding=\"utf8\")\n",
    "                #Convert to lower case\n",
    "                n_grams=clean_doc(ch)\n",
    "                tokens=words_lemmatizer(n_grams)\n",
    "\n",
    "                #print(\"list ngrams--------------\\n\",n_grams)\n",
    "                row[\"token_spans\"] = tokens\n",
    "    \n",
    "                return row\n",
    "        #self.processed_data = self.processed_data.apply(stem_and_join, axis=1)\n",
    "        self.processed_data = self.processed_data.apply(tokenize_grams, axis=1)\n",
    "        \n",
    "    def build_wordlist(self, min_occurrences=0, max_occurences=500, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                      ):\n",
    "        \n",
    "        whitelist=[]\n",
    "        self.wordlist = []\n",
    "        stopwords=[]\n",
    "        words = Counter()\n",
    "      \n",
    "        for idx in self.processed_data.index:\n",
    "            words.update(self.processed_data.loc[idx, \"token_spans\"])\n",
    "\n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "        \n",
    "        \n",
    "        print(words.most_common())        \n",
    "\n",
    "        word_df = pd.DataFrame(data={\"word\": [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                     \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                               columns=[\"word\", \"occurrences\"])\n",
    "\n",
    "        word_df.to_csv(\"wordlist_test.csv\", index_label=\"idx\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]\n",
    "        \n",
    "        \n",
    "    def build_data_model(self):\n",
    "        \n",
    "        label_id = [\"ID\"]\n",
    "\n",
    "        columns = label_id + list(\n",
    "            map(lambda w: w ,self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "            if True:\n",
    "                # add label\n",
    "                current_id = self.processed_data.loc[idx, \"id\"]\n",
    "                \n",
    "                labels.append(current_id)\n",
    "                \n",
    "                current_row.append(current_id)\n",
    "\n",
    "            \n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"token_spans\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        return self.data_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$FB</td>\n",
       "      <td>719659409228451840</td>\n",
       "      <td>0.366</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[watching for bounce tomorrow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$LUV</td>\n",
       "      <td>719904304207962112</td>\n",
       "      <td>0.638</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[record number of passengers served in 2015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$NFLX</td>\n",
       "      <td>5329774</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[out $NFLX -.35]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$DIA</td>\n",
       "      <td>719891468173844480</td>\n",
       "      <td>0.460</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Looking for a strong bounce, Lunchtime rally ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$PLUG</td>\n",
       "      <td>20091246</td>\n",
       "      <td>0.403</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Very intrigued with the technology and growth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$GMCR</td>\n",
       "      <td>5819749</td>\n",
       "      <td>0.000</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short worked, puts up]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$IBM</td>\n",
       "      <td>709741154393133056</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[overbought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$JOSB</td>\n",
       "      <td>17892972</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[absolute garbage still up, stores TOTALLY EMP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$CSTM</td>\n",
       "      <td>709834259687710720</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Biggest Market Losers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>$PYPL</td>\n",
       "      <td>708481442079068160</td>\n",
       "      <td>0.408</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Love this company long time.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>$GOOGL</td>\n",
       "      <td>31971935</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[$GOOG $GOOGL would suck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>$ENDP</td>\n",
       "      <td>710187873492983808</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[who won't pay anymore, REAL risk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>$XLI</td>\n",
       "      <td>13915103</td>\n",
       "      <td>0.025</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[No edge offered]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>$PCLN</td>\n",
       "      <td>10448993</td>\n",
       "      <td>0.486</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[runs into the 50sma on the acquisition news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>$AA</td>\n",
       "      <td>24886266</td>\n",
       "      <td>0.308</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[t can't go down]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>12793642</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[now seems like its helping the downtrend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>9408369</td>\n",
       "      <td>0.461</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[mastered their supply chain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>$GOLD</td>\n",
       "      <td>719909604654624768</td>\n",
       "      <td>0.408</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Most bullish stocks on Twitter during this dip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>$AMD</td>\n",
       "      <td>9674584</td>\n",
       "      <td>-0.699</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[big dumping, would not touch it for a while]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>$SPY</td>\n",
       "      <td>10041008</td>\n",
       "      <td>0.495</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[trade continuing very nicely from yesterday, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cashtag                  id  sentiment score      source  \\\n",
       "0      $FB  719659409228451840            0.366     twitter   \n",
       "1     $LUV  719904304207962112            0.638     twitter   \n",
       "2    $NFLX             5329774           -0.494  stocktwits   \n",
       "3     $DIA  719891468173844480            0.460     twitter   \n",
       "4    $PLUG            20091246            0.403  stocktwits   \n",
       "5    $GMCR             5819749            0.000  stocktwits   \n",
       "6     $IBM  709741154393133056           -0.296     twitter   \n",
       "7    $JOSB            17892972           -0.546  stocktwits   \n",
       "8    $CSTM  709834259687710720           -0.438     twitter   \n",
       "9    $PYPL  708481442079068160            0.408     twitter   \n",
       "10  $GOOGL            31971935           -0.398  stocktwits   \n",
       "11   $ENDP  710187873492983808           -0.349     twitter   \n",
       "12    $XLI            13915103            0.025  stocktwits   \n",
       "13   $PCLN            10448993            0.486  stocktwits   \n",
       "14     $AA            24886266            0.308  stocktwits   \n",
       "15   $AAPL            12793642           -0.372  stocktwits   \n",
       "16   $AAPL             9408369            0.461  stocktwits   \n",
       "17   $GOLD  719909604654624768            0.408     twitter   \n",
       "18    $AMD             9674584           -0.699  stocktwits   \n",
       "19    $SPY            10041008            0.495  stocktwits   \n",
       "\n",
       "                                                spans  \n",
       "0                      [watching for bounce tomorrow]  \n",
       "1        [record number of passengers served in 2015]  \n",
       "2                                    [out $NFLX -.35]  \n",
       "3   [Looking for a strong bounce, Lunchtime rally ...  \n",
       "4   [Very intrigued with the technology and growth...  \n",
       "5                             [short worked, puts up]  \n",
       "6                                        [overbought]  \n",
       "7   [absolute garbage still up, stores TOTALLY EMP...  \n",
       "8                             [Biggest Market Losers]  \n",
       "9                      [Love this company long time.]  \n",
       "10                          [$GOOG $GOOGL would suck]  \n",
       "11                 [who won't pay anymore, REAL risk]  \n",
       "12                                  [No edge offered]  \n",
       "13      [runs into the 50sma on the acquisition news]  \n",
       "14                                  [t can't go down]  \n",
       "15         [now seems like its helping the downtrend]  \n",
       "16                      [mastered their supply chain]  \n",
       "17   [Most bullish stocks on Twitter during this dip]  \n",
       "18      [big dumping, would not touch it for a while]  \n",
       "19  [trade continuing very nicely from yesterday, ...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TwitterData_Initialize()\n",
    "data.initialize(\"Microblog_Trainingdata.json\")\n",
    "\n",
    "data.processed_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 195.05737805366516 seconds ---\n"
     ]
    }
   ],
   "source": [
    "data.do_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "      <th>token_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$FB</td>\n",
       "      <td>719659409228451840</td>\n",
       "      <td>0.366</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[watching for bounce tomorrow]</td>\n",
       "      <td>[watch, bounce, tomorrow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$LUV</td>\n",
       "      <td>719904304207962112</td>\n",
       "      <td>0.638</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[record number of passengers served in 2015]</td>\n",
       "      <td>[record, number, passenger, serve]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$NFLX</td>\n",
       "      <td>5329774</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[out $NFLX -.35]</td>\n",
       "      <td>[nflx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$DIA</td>\n",
       "      <td>719891468173844480</td>\n",
       "      <td>0.460</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Looking for a strong bounce, Lunchtime rally ...</td>\n",
       "      <td>[look, strong, bounce, lunchtime, rally, come]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$PLUG</td>\n",
       "      <td>20091246</td>\n",
       "      <td>0.403</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Very intrigued with the technology and growth...</td>\n",
       "      <td>[very, intrigue, technology, growth, potential]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cashtag                  id  sentiment score      source  \\\n",
       "0     $FB  719659409228451840            0.366     twitter   \n",
       "1    $LUV  719904304207962112            0.638     twitter   \n",
       "2   $NFLX             5329774           -0.494  stocktwits   \n",
       "3    $DIA  719891468173844480            0.460     twitter   \n",
       "4   $PLUG            20091246            0.403  stocktwits   \n",
       "\n",
       "                                               spans  \\\n",
       "0                     [watching for bounce tomorrow]   \n",
       "1       [record number of passengers served in 2015]   \n",
       "2                                   [out $NFLX -.35]   \n",
       "3  [Looking for a strong bounce, Lunchtime rally ...   \n",
       "4  [Very intrigued with the technology and growth...   \n",
       "\n",
       "                                       token_spans  \n",
       "0                        [watch, bounce, tomorrow]  \n",
       "1               [record, number, passenger, serve]  \n",
       "2                                           [nflx]  \n",
       "3   [look, strong, bounce, lunchtime, rally, come]  \n",
       "4  [very, intrigue, technology, growth, potential]  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "      <th>token_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$F</td>\n",
       "      <td>5540055</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Putting on a little $F short]</td>\n",
       "      <td>[put, little, short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>10752226</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short some]</td>\n",
       "      <td>[short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$BAC</td>\n",
       "      <td>10920221</td>\n",
       "      <td>0.445</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[buying opportunity]</td>\n",
       "      <td>[buying, opportunity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$SHOR</td>\n",
       "      <td>12971398</td>\n",
       "      <td>0.661</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Scaling Up on Long Position]</td>\n",
       "      <td>[scale, up, long, position]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$JPM</td>\n",
       "      <td>16142438</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[its time to sell banks]</td>\n",
       "      <td>[time, sell, bank]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cashtag        id  sentiment score      source  \\\n",
       "0      $F   5540055           -0.454  stocktwits   \n",
       "1   $AAPL  10752226           -0.464  stocktwits   \n",
       "2    $BAC  10920221            0.445  stocktwits   \n",
       "3   $SHOR  12971398            0.661  stocktwits   \n",
       "4    $JPM  16142438           -0.763  stocktwits   \n",
       "\n",
       "                            spans                  token_spans  \n",
       "0  [Putting on a little $F short]         [put, little, short]  \n",
       "1                    [short some]                      [short]  \n",
       "2            [buying opportunity]        [buying, opportunity]  \n",
       "3   [Scaling Up on Long Position]  [scale, up, long, position]  \n",
       "4        [its time to sell banks]           [time, sell, bank]  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = TwitterData_Initialize_test()\n",
    "data_test.initialize(\"Microblog_Trialdata2.json\")\n",
    "\n",
    "data_test.processed_data.head()\n",
    "#test_data=data_test.processed_data.copy()\n",
    "data_test.do_process()\n",
    "data_test.processed_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"# Importing the built-in logging module\\nimport logging\\nlogging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\\n\""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Importing the built-in logging module\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7096\n"
     ]
    }
   ],
   "source": [
    "#list_tweet=get_tweets(data)\n",
    "list_tweet=list(data.processed_data[\"token_spans\"])\n",
    "#sentences=review_sentences(list_tweet, remove_stopwords=True)\n",
    "sentences=list_tweet\n",
    "taille=0\n",
    "for tokens in sentences: \n",
    "    taille+=len(tokens)\n",
    "print(taille)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Creating the model and setting values for the various parameters\\nnum_features = 200  # Word vector dimensionality\\nmin_word_count = 2 # Minimum word count\\nnum_workers = 4     # Number of parallel threads\\ncontext = 10        # Context window size\\ndownsampling = 1e-3 # (0.001) Downsample setting for frequent words\\n\\n# Initializing the train model\\nfrom gensim.models import word2vec\\nprint(\"Training model....\")\\nw2v_model = word2vec.Word2Vec(sentences,                          workers=num_workers,                          size=num_features,                          min_count=min_word_count,                          window=context,\\n                          sample=downsampling)\\n\\n# To make the model memory efficient\\nw2v_model.init_sims(replace=True)\\n\\n# Saving the model for later use. Can be loaded using Word2Vec.load()\\nmodel_name = \"microblogs_word_vector\"\\nw2v_model.save(model_name)\\n#model = gensim.models.Word2Vec.load(model_name)\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Creating the model and setting values for the various parameters\n",
    "num_features = 200  # Word vector dimensionality\n",
    "min_word_count = 2 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 10        # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "# Initializing the train model\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model....\")\n",
    "w2v_model = word2vec.Word2Vec(sentences,\\\n",
    "                          workers=num_workers,\\\n",
    "                          size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "\n",
    "# To make the model memory efficient\n",
    "w2v_model.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"microblogs_word_vector\"\n",
    "w2v_model.save(model_name)\n",
    "#model = gensim.models.Word2Vec.load(model_name)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Few tests: This will print the odd word among them \\nw2v_model.wv.doesnt_match(\"Thanks for the short upper\".split())\\nlen(w2v_model.wv.vocab)'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"# Few tests: This will print the odd word among them \n",
    "w2v_model.wv.doesnt_match(\"Thanks for the short upper\".split())\n",
    "len(w2v_model.wv.vocab)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'w2v_model.wv.most_similar(\"big\")\\n#len(model.wv.index2word)\\nlist(map(lambda i: \"word2vec_{0}\".format(i), range(0, w2v_model.vector_size)))'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"w2v_model.wv.most_similar(\"big\")\n",
    "#len(model.wv.index2word)\n",
    "list(map(lambda i: \"word2vec_{0}\".format(i), range(0, w2v_model.vector_size)))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1197: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "goo_model = KeyedVectors.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  \n",
    "#goo_model=gensim.models.Word2Vec.load('GoogleNews-vectors-negative300.bin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08007812  0.10498047  0.04980469  0.0534668  -0.06738281 -0.12060547\n",
      "  0.03515625 -0.11865234  0.04394531  0.03015137 -0.05688477 -0.07617188\n",
      "  0.01287842  0.04980469 -0.08496094 -0.06347656  0.00628662 -0.04321289\n",
      "  0.02026367  0.01330566 -0.01953125  0.09277344 -0.171875   -0.00131989\n",
      "  0.06542969  0.05834961 -0.08251953  0.0859375  -0.00318909  0.05859375\n",
      " -0.03491211 -0.0123291  -0.0480957  -0.00302124  0.05639648  0.01495361\n",
      " -0.07226562 -0.05224609  0.09667969  0.04296875 -0.03540039 -0.07324219\n",
      "  0.03271484 -0.06176758  0.00787354  0.0035553  -0.00878906  0.0390625\n",
      "  0.03833008  0.04443359  0.06982422  0.01263428 -0.00445557 -0.03320312\n",
      " -0.04272461  0.09765625 -0.02160645 -0.0378418   0.01190186 -0.01391602\n",
      " -0.11328125  0.09326172 -0.03930664 -0.11621094  0.02331543 -0.01599121\n",
      "  0.02636719  0.10742188 -0.00466919  0.09619141  0.0279541  -0.05395508\n",
      "  0.08544922 -0.03686523 -0.02026367 -0.08544922  0.125       0.14453125\n",
      "  0.0267334   0.15039062  0.05273438 -0.18652344  0.08154297 -0.01062012\n",
      " -0.03735352 -0.07324219 -0.07519531  0.03613281 -0.13183594  0.00616455\n",
      "  0.05078125  0.04516602  0.0100708  -0.15039062 -0.06005859  0.05761719\n",
      " -0.00692749  0.01586914 -0.0213623   0.10351562 -0.00029182 -0.046875\n",
      " -0.01635742 -0.07861328 -0.06933594  0.01635742 -0.03149414 -0.01373291\n",
      " -0.03662109 -0.08886719 -0.0480957  -0.01318359 -0.07177734  0.00588989\n",
      " -0.04614258  0.03979492  0.10058594 -0.04931641  0.07568359  0.03881836\n",
      " -0.16699219 -0.09619141 -0.10107422  0.02905273 -0.05786133 -0.01928711\n",
      " -0.04296875 -0.08398438 -0.01989746  0.05151367  0.00848389 -0.03613281\n",
      " -0.14941406 -0.01855469 -0.03637695 -0.07666016 -0.03955078 -0.06152344\n",
      " -0.02001953  0.04150391  0.03686523 -0.07226562  0.00592041 -0.06298828\n",
      "  0.00738525 -0.01586914  0.01611328 -0.01452637  0.00772095  0.10107422\n",
      " -0.00558472  0.01428223 -0.07617188  0.05639648 -0.01293945  0.03063965\n",
      " -0.02490234 -0.09863281  0.0324707  -0.02807617 -0.08105469  0.02062988\n",
      "  0.01611328 -0.04199219 -0.03491211 -0.03759766  0.05493164  0.01373291\n",
      "  0.02685547 -0.05859375 -0.07177734 -0.12011719 -0.02282715 -0.1640625\n",
      " -0.00361633 -0.05981445  0.07080078 -0.07714844  0.05175781 -0.04296875\n",
      " -0.04833984  0.0300293  -0.06591797 -0.03173828 -0.04882812 -0.03491211\n",
      "  0.05883789 -0.01464844  0.18066406  0.05688477  0.05249023  0.05786133\n",
      "  0.11669922  0.05200195 -0.0534668   0.01867676 -0.015625    0.00576782\n",
      " -0.07324219 -0.11621094  0.04052734  0.0625     -0.04321289  0.01055908\n",
      "  0.02172852  0.04248047  0.03271484  0.04418945  0.05761719  0.02612305\n",
      " -0.01831055 -0.02697754 -0.00674438  0.00509644 -0.11621094  0.00364685\n",
      "  0.05761719 -0.05957031 -0.08837891  0.0135498   0.04541016 -0.04638672\n",
      " -0.0177002  -0.0625      0.03442383 -0.02416992  0.03088379  0.09570312\n",
      "  0.07958984  0.03930664  0.0279541  -0.0859375   0.08105469  0.06640625\n",
      " -0.00041962 -0.06933594  0.03588867 -0.03417969  0.04492188 -0.00772095\n",
      " -0.00741577 -0.04760742  0.01397705 -0.09960938  0.0246582  -0.09960938\n",
      "  0.11474609  0.03173828  0.02209473  0.07226562  0.03686523  0.02563477\n",
      "  0.01367188 -0.02734375  0.00592041 -0.06738281  0.05053711 -0.02832031\n",
      " -0.04516602 -0.01733398  0.02111816  0.03515625 -0.04296875  0.06640625\n",
      "  0.12207031  0.12353516  0.0039978   0.04516602 -0.01855469  0.04833984\n",
      "  0.04516602  0.08691406  0.02941895  0.03759766  0.03442383 -0.07373047\n",
      " -0.0402832  -0.14648438 -0.02441406 -0.01953125  0.0065918  -0.0018158\n",
      " -0.01092529  0.09326172  0.06542969  0.01843262 -0.09326172 -0.01574707\n",
      " -0.07128906 -0.08935547 -0.07128906 -0.03015137 -0.01300049  0.01635742\n",
      " -0.01831055  0.01483154  0.00500488  0.00366211  0.04760742 -0.06884766]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  \n",
      "C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#goo_model.wv.most_similar(\"the\")\n",
    "print(goo_model.wv['the'])\n",
    "len(goo_model.wv.vocab)\n",
    "num_features=300\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 1000 of 1700\n"
     ]
    }
   ],
   "source": [
    "#transformer les donner de train en moyenne de vecteurs word2vec \n",
    "trainDataVecs = getAvgFeatureVecs(list_tweet, goo_model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.03369141,  0.05423991,  0.09147135, ..., -0.03519694,\n",
       "         0.0546875 , -0.16031902],\n",
       "       [ 0.11431885, -0.05249023,  0.12463379, ...,  0.04917145,\n",
       "         0.04614258, -0.19024658],\n",
       "       [        nan,         nan,         nan, ...,         nan,\n",
       "                nan,         nan],\n",
       "       ...,\n",
       "       [ 0.036026  , -0.04504395, -0.00216293, ...,  0.0113678 ,\n",
       "         0.06007195, -0.05272675],\n",
       "       [ 0.03466797,  0.1184082 , -0.14648438, ..., -0.1875    ,\n",
       "         0.01748657, -0.20214844],\n",
       "       [-0.5859375 , -0.27148438, -0.62109375, ...,  0.38867188,\n",
       "         0.25      ,  0.07080078]], dtype=float32)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train=trainDataVecs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=data.processed_data[\"sentiment score\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(X, Y,train_size=0.7)                                                  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=5, min_child_weight=1, missing=None, n_estimators=1000,\n",
      "       n_jobs=1, nthread=4, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
      "       subsample=0.8)\n"
     ]
    }
   ],
   "source": [
    "#---------------------------XGBregressor---------------------------------------------------\n",
    "\n",
    "#Fitting XGB regressor with parameters obtained by Grid searchCV\n",
    "\"\"\"\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "param_test2 = {\n",
    " 'max_depth':[4,5,6],\n",
    " 'min_child_weight':[4,5,6]\n",
    "}\n",
    "param_test3 = {\n",
    "    'gamma':[i/10.0 for i in range(0,5)],\n",
    "    'n_estimators' : [int(x) for x in np.linspace(start = 200, stop = 1400, num = 7)]\n",
    "}\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "model = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, max_depth=4,\n",
    " min_child_weight=4,  subsample=0.8, colsample_bytree=1.0, nthread=4), \n",
    " param_grid = param_test3,scoring='neg_mean_squared_error',cv=5)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "\"\"\"\n",
    "#model = xgb.XGBRegressor(max_depth=4, min_child_weight= 4,gamma=0.3,subsample=0.8,colsample_bytree=1.0,n_estimators=1000)\n",
    "XGBR=xgb.XGBRegressor(learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "XGBR.fit(X_train,y_train)\n",
    "\n",
    "print (XGBR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#------------------RandomForest--------------------------\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import numpy as np\n",
    "\"\"\"\n",
    "# Number of trees in random forest\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "\n",
    "# Number of features to consider at every split\n",
    "max_features = ['auto', 'sqrt']\n",
    "\n",
    "# Maximum number of levels in tree\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "max_depth.append(None)\n",
    "\n",
    "# Minimum number of samples required to split a node\n",
    "min_samples_split = [2, 5, 10]\n",
    "\n",
    "# Minimum number of samples required at each leaf node\n",
    "min_samples_leaf = [1, 2, 4]\n",
    "\n",
    "# Method of selecting samples for training each tree\n",
    "bootstrap = [True, False]\n",
    "\n",
    "# Create the random grid\n",
    "random_grid = {'n_estimators': n_estimators,\n",
    "               'max_features': max_features,\n",
    "               'max_depth': max_depth,\n",
    "               'min_samples_split': min_samples_split,\n",
    "               'min_samples_leaf': min_samples_leaf,\n",
    "               'bootstrap': bootstrap}\n",
    "print(random_grid)\n",
    "\n",
    "# Use the random grid to search for best hyperparameters\n",
    "# First create the base model to tune\n",
    "rf = RandomForestRegressor()\n",
    "# Random search of parameters, using 3 fold cross validation, \n",
    "# search across 100 different combinations, and use all available cores\n",
    "rf_random = RandomizedSearchCV(estimator = rf, param_distributions = random_grid, n_iter = 100, cv = 3, verbose=2, random_state=42, n_jobs = -1)\n",
    "# Fit the random search model\n",
    "rf_random.fit(X_train,y_train)\n",
    "\n",
    "\"\"\"\n",
    "# Import the model we are using\n",
    "# Instantiate model with 1000 decision trees\n",
    "RF = RandomForestRegressor(random_state = 42,bootstrap= True,\n",
    " max_depth= None,\n",
    " max_features= 'sqrt',\n",
    " min_samples_leaf= 1,\n",
    " min_samples_split= 10,\n",
    " n_estimators= 1600)\n",
    "# Train the model on training data\n",
    "RF.fit(X_train, y_train);\n",
    "print (RF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------DecisionTree----------------------------\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "# set of parameters to test\n",
    "param_grid = {\n",
    "              \"min_samples_split\": [2, 10, 20],\n",
    "              \"max_depth\": [None, 2, 5, 10],\n",
    "              \"min_samples_leaf\": [1, 5, 10],\n",
    "              \"max_leaf_nodes\": [None, 5, 10, 20],\n",
    "              }\n",
    "\n",
    "model = DecisionTreeRegressor()\n",
    "DTR = GridSearchCV(model, param_grid)\n",
    "DTR.fit(X_train, y_train)\n",
    "#accuracy = [cross_val_score(model, X_train, y_train, cv=8, n_jobs=-1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error: 0.005772898478391591\n",
      "R2 score: 0.9569408884704746\n"
     ]
    }
   ],
   "source": [
    "#test model accuracy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "print(\"mean squared error:\" ,mean_squared_error(XGBR.predict(X_train), y_train))\n",
    "print(\"R2 score:\" ,r2_score(XGBR.predict(X_train),y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "      <th>token_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$F</td>\n",
       "      <td>5540055</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Putting on a little $F short]</td>\n",
       "      <td>[put, little, short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>10752226</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short some]</td>\n",
       "      <td>[short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$BAC</td>\n",
       "      <td>10920221</td>\n",
       "      <td>0.445</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[buying opportunity]</td>\n",
       "      <td>[buying, opportunity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$SHOR</td>\n",
       "      <td>12971398</td>\n",
       "      <td>0.661</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Scaling Up on Long Position]</td>\n",
       "      <td>[scale, up, long, position]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$JPM</td>\n",
       "      <td>16142438</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[its time to sell banks]</td>\n",
       "      <td>[time, sell, bank]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cashtag        id  sentiment score      source  \\\n",
       "0      $F   5540055           -0.454  stocktwits   \n",
       "1   $AAPL  10752226           -0.464  stocktwits   \n",
       "2    $BAC  10920221            0.445  stocktwits   \n",
       "3   $SHOR  12971398            0.661  stocktwits   \n",
       "4    $JPM  16142438           -0.763  stocktwits   \n",
       "\n",
       "                            spans                  token_spans  \n",
       "0  [Putting on a little $F short]         [put, little, short]  \n",
       "1                    [short some]                      [short]  \n",
       "2            [buying opportunity]        [buying, opportunity]  \n",
       "3   [Scaling Up on Long Position]  [scale, up, long, position]  \n",
       "4        [its time to sell banks]           [time, sell, bank]  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.do_process()\n",
    "data_test.processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tweet=list(data_test.processed_data[\"token_spans\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:10: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "testDataVecs = getAvgFeatureVecs(test_tweet, goo_model, num_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test=testDataVecs\n",
    "X_test[0].size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#isoler les tweets du test et leur vrai valeur de sentiment \n",
    "labels=[\"real pred\",\"spans\"]\n",
    "pred=list(data_test.processed_data[\"sentiment score\"])\n",
    "twt=list(data_test.processed_data[\"spans\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>real prediction</th>\n",
       "      <th>spans</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.454</td>\n",
       "      <td>[Putting on a little $F short]</td>\n",
       "      <td>-0.259628</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.464</td>\n",
       "      <td>[short some]</td>\n",
       "      <td>-0.502769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.445</td>\n",
       "      <td>[buying opportunity]</td>\n",
       "      <td>0.348896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.661</td>\n",
       "      <td>[Scaling Up on Long Position]</td>\n",
       "      <td>0.487097</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.763</td>\n",
       "      <td>[its time to sell banks]</td>\n",
       "      <td>-0.748225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.627</td>\n",
       "      <td>[Entering long]</td>\n",
       "      <td>0.276600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.653</td>\n",
       "      <td>[picked some up]</td>\n",
       "      <td>0.210565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.668</td>\n",
       "      <td>[time to accumulate for a long position, far m...</td>\n",
       "      <td>0.624018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.460</td>\n",
       "      <td>[Looking for a strong bounce, Lunchtime rally ...</td>\n",
       "      <td>0.473562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.403</td>\n",
       "      <td>[Very intrigued with the technology and growth...</td>\n",
       "      <td>0.399923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.000</td>\n",
       "      <td>[short2 48 + - ***worked, puts up]</td>\n",
       "      <td>0.134602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.438</td>\n",
       "      <td>[Biggest Market Losers]</td>\n",
       "      <td>-0.438664</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>-0.398</td>\n",
       "      <td>[$GOOG $GOOGL would suck]</td>\n",
       "      <td>-0.396191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.483</td>\n",
       "      <td>[Buying $SBUX on dip]</td>\n",
       "      <td>0.205753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.480</td>\n",
       "      <td>[is a short below 740, and is overbought]</td>\n",
       "      <td>-0.461767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.454</td>\n",
       "      <td>[don't Putting on a down little $F short]</td>\n",
       "      <td>-0.261041</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    real prediction                                              spans  \\\n",
       "0            -0.454                     [Putting on a little $F short]   \n",
       "1            -0.464                                       [short some]   \n",
       "2             0.445                               [buying opportunity]   \n",
       "3             0.661                      [Scaling Up on Long Position]   \n",
       "4            -0.763                           [its time to sell banks]   \n",
       "5             0.627                                    [Entering long]   \n",
       "6             0.653                                   [picked some up]   \n",
       "7             0.668  [time to accumulate for a long position, far m...   \n",
       "8             0.460  [Looking for a strong bounce, Lunchtime rally ...   \n",
       "9             0.403  [Very intrigued with the technology and growth...   \n",
       "10            0.000                 [short2 48 + - ***worked, puts up]   \n",
       "11           -0.438                            [Biggest Market Losers]   \n",
       "12           -0.398                          [$GOOG $GOOGL would suck]   \n",
       "13            0.483                              [Buying $SBUX on dip]   \n",
       "14           -0.480          [is a short below 740, and is overbought]   \n",
       "15           -0.454          [don't Putting on a down little $F short]   \n",
       "\n",
       "    Prediction  \n",
       "0    -0.259628  \n",
       "1    -0.502769  \n",
       "2     0.348896  \n",
       "3     0.487097  \n",
       "4    -0.748225  \n",
       "5     0.276600  \n",
       "6     0.210565  \n",
       "7     0.624018  \n",
       "8     0.473562  \n",
       "9     0.399923  \n",
       "10    0.134602  \n",
       "11   -0.438664  \n",
       "12   -0.396191  \n",
       "13    0.205753  \n",
       "14   -0.461767  \n",
       "15   -0.261041  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict \n",
    "output_2 = XGBR.predict(X_test)\n",
    "test_df = pd.DataFrame(pred,columns={\"real prediction\"})\n",
    "#final_df[\"ID\"] = id_vals\n",
    "#final_df[\"cashtag\"]=row2\n",
    "\n",
    "test_df[\"spans\"]=twt\n",
    "test_df[\"Prediction\"] = output_2\n",
    "#final_df.to_csv(\"Output_1.csv\",sep=\",\")\n",
    "test_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error: 0.033261851149993055\n",
      "R2 score: 0.8115568040385457\n"
     ]
    }
   ],
   "source": [
    "#tester la performance de prediction avec R2 score et MSE\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "print(\"mean squared error:\" ,mean_squared_error(output_2, pred))\n",
    "print(\"R2 score:\" ,r2_score(output_2,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
