{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize,blankline_tokenize\n",
    "from nltk import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "import string\n",
    "import json\n",
    "import re as regex\n",
    "# This will score 0.556+ on the LB, you can further raise this score\n",
    "# by using PCA and SVD as features.\n",
    "\n",
    "# From https://github.com/rhiever/datacleaner\n",
    "from datacleaner import autoclean\n",
    "from sklearn.model_selection import GridSearchCV,train_test_split, cross_val_score, RandomizedSearchCV\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_Initialize():\n",
    "    data = []\n",
    "    processed_data = []\n",
    "    wordlist = []\n",
    "\n",
    "    featureList = []\n",
    "    fea_vect=[]\n",
    "    \n",
    "    data_model = None\n",
    "    data_labels = None\n",
    "    is_testing = False\n",
    "    \n",
    "    def initialize(self, csv_file, is_testing_set=False, from_cached=None):\n",
    "        if from_cached is not None:\n",
    "            #self.data_model = pd.read_csv(from_cached)\n",
    "            self.data_model = pd.read_json(from_cached)\n",
    "\n",
    "            return\n",
    "\n",
    "        self.is_testing = is_testing_set\n",
    "\n",
    "        if not is_testing_set:\n",
    "            #self.data = pd.read_csv(csv_file, header=0, names=[\"id\", \"emotion\", \"text\"])\n",
    "            self.data = pd.read_json(csv_file)\n",
    "\n",
    "            #self.data = self.data[self.data[\"emotion\"].isin([\"positive\", \"negative\", \"neutral\"])]\n",
    "        \n",
    "\n",
    "        self.processed_data = self.data\n",
    "        self.wordlist = []\n",
    "        self.data_model = None\n",
    "        self.data_labels = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def do_process(self):\n",
    "        start_time = time.time()\n",
    "        def stem_and_join(row,stemmer=nltk.PorterStemmer()):\n",
    "            row[\"spans\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"spans\"]))\n",
    "            return row\n",
    "    \n",
    "        def tokenize_grams(row):\n",
    "                \n",
    "                # turn a doc into clean tokens\n",
    "                def clean_doc(doc):\n",
    "                    # split into tokens by white space\n",
    "                    tokens = doc.split()\n",
    "                    # remove punctuation from each token\n",
    "                    table = str.maketrans('', '', string.punctuation)\n",
    "                    tokens = [w.translate(table) for w in tokens]\n",
    "                    # remove remaining tokens that are not alphabetic\n",
    "                    tokens = [word for word in tokens if word.isalpha()]\n",
    "                    # filter out stop words\n",
    "                    #stop_words = set(stopwords.words('english'))\n",
    "                    \n",
    "                    #tokens = [w for w in tokens if not w in stop_words]\n",
    "                    \n",
    "                    \n",
    "                    # filter out short tokens\n",
    "                    tokens = [word.lower() for word in tokens if len(word) > 1]\n",
    "                    return tokens\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "               \n",
    "                # Function to apply lemmatization to a list of words\n",
    "                def words_lemmatizer(words, encoding=\"utf8\"):\n",
    "                    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "                    lemma_words = []\n",
    "                    wl = WordNetLemmatizer()\n",
    "                    for word in words:\n",
    "                        pos = find_pos(word)\n",
    "                        lemma_words.append(wl.lemmatize(word, pos))\n",
    "                    return lemma_words\n",
    "                \n",
    "                # Function to find part of speech tag for a word\n",
    "                def find_pos(word):\n",
    "                    # Part of Speech constants\n",
    "                    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "   \n",
    "                    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "                    # Adjective tags - 'JJ', 'JJR', 'JJS'\n",
    "                    if pos.lower()[0] == 'j':\n",
    "                        return 'a'\n",
    "                    # Adverb tags - 'RB', 'RBR', 'RBS'\n",
    "                    elif pos.lower()[0] == 'r':\n",
    "                        return 'r'\n",
    "                    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "                    elif pos.lower()[0] == 'v':\n",
    "                        return 'v'\n",
    "                    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'\n",
    "                    else:\n",
    "                        return 'n'\n",
    "    \n",
    "            \n",
    "                #convert to string\n",
    "                idx=row[\"spans\"]\n",
    "                #ch=\"\".join(x for x in idx if x)\n",
    "                ch=' '.join(idx)\n",
    "               \n",
    "    \n",
    "                \n",
    "                \n",
    "                n_grams=clean_doc(ch)\n",
    "                tokens=words_lemmatizer(n_grams)\n",
    "\n",
    "                #print(\"nombre of tokens\",len(n_grams))\n",
    "\n",
    "                row[\"token_spans\"] = tokens\n",
    "    \n",
    "                return row\n",
    "        #self.processed_data = self.processed_data.apply(stem_and_join, axis=1)\n",
    "        self.processed_data = self.processed_data.apply(tokenize_grams, axis=1)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    \n",
    "\n",
    "    def build_wordlist(self, min_occurrences=0, max_occurences=500, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                       ):\n",
    "       \n",
    "        self.wordlist = []\n",
    "        whitelist=[\"to\",\"on\",\"for\",\"up\",\"below\",\"short\",\"long\"]\n",
    "        \n",
    "        #stopwords=[]\n",
    "       \n",
    "        words = Counter()\n",
    "      \n",
    "        for idx in self.processed_data.index:\n",
    "            words.update(self.processed_data.loc[idx, \"token_spans\"])\n",
    "\n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "        \n",
    "        \n",
    "        print(words.most_common())        \n",
    "\n",
    "        word_df = pd.DataFrame(data={\"word\": [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                     \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                               columns=[\"word\", \"occurrences\"])\n",
    "\n",
    "        word_df.to_csv(\"wordlist_copie.csv\", index_label=\"idx\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build_data_model(self):\n",
    "        label_column = []\n",
    "        Id_column=[\"ID\"]\n",
    "        label_column = [\"label\"]\n",
    "\n",
    "        columns = Id_column + label_column + list(\n",
    "            map(lambda w: w ,self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "\n",
    "            if True:\n",
    "                # add label\n",
    "                current_label = self.processed_data.loc[idx, \"sentiment score\"]\n",
    "                current_id = self.processed_data.loc[idx, \"id\"]\n",
    "                \n",
    "                labels.append(current_id)\n",
    "                labels.append(current_label)\n",
    "                \n",
    "                current_row.append(current_id)\n",
    "                current_row.append(current_label)\n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"token_spans\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        return self.data_model, self.data_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_Initialize_test(TwitterData_Initialize):\n",
    "\n",
    "    \n",
    "    \n",
    "    def do_process(self):\n",
    "        def stem_and_join(row,stemmer=nltk.PorterStemmer()):\n",
    "            \n",
    "            row[\"spans\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"spans\"]))\n",
    "            return row\n",
    "    \n",
    "        def tokenize_grams(row):\n",
    "                \n",
    "                # Function to remove stop words\n",
    "                def remove_stopwords(text, lang='english'):\n",
    "                    whitelist = [\"n't\",\"not\",\"below\"]    \n",
    "\n",
    "                    stop_words = set(stopwords.words('english'))\n",
    "                    word_tokens = word_tokenize(text)\n",
    "                    #filtered_sentence = [w for w in word_tokens if ((not w in stop_words) or (w in whitelist))]\n",
    "                    filtered_sentence = []\n",
    "                    for w in word_tokens:\n",
    "                        if ((w not in stop_words) or (w in whitelist)):\n",
    "                            filtered_sentence.append(w)\n",
    "                \n",
    "                    ch=\" \".join(filtered_sentence)\n",
    "\n",
    "                    return ch\n",
    "#40\n",
    "\n",
    "                def clean_doc(doc):\n",
    "                    # split into tokens by white space\n",
    "                    tokens = doc.split()\n",
    "                    # remove punctuation from each token\n",
    "                    table = str.maketrans('', '', string.punctuation)\n",
    "                    tokens = [w.translate(table) for w in tokens]\n",
    "                    # remove remaining tokens that are not alphabetic\n",
    "                    tokens = [word for word in tokens if word.isalpha()]\n",
    "                    # filter out stop words\n",
    "                    #stop_words = set(stopwords.words('english'))\n",
    "                    #tokens = [w for w in tokens if not w in stop_words]\n",
    "                    \n",
    "                    # filter out short tokens\n",
    "                    tokens = [word.lower() for word in tokens if len(word) > 1]\n",
    "                    return tokens\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                # Function to apply lemmatization to a list of words\n",
    "                def words_lemmatizer(words, encoding=\"utf8\"):\n",
    "                    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "                    lemma_words = []\n",
    "                    wl = WordNetLemmatizer()\n",
    "                    for word in words:\n",
    "                        pos = find_pos(word)\n",
    "                        lemma_words.append(wl.lemmatize(word, pos))\n",
    "                    return lemma_words\n",
    "\n",
    "                # Function to find part of speech tag for a word\n",
    "                def find_pos(word):\n",
    "                    # Part of Speech constants\n",
    "                    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "   \n",
    "                    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "                    # Adjective tags - 'JJ', 'JJR', 'JJS'\n",
    "                    if pos.lower()[0] == 'j':\n",
    "                        return 'a'\n",
    "                    # Adverb tags - 'RB', 'RBR', 'RBS'\n",
    "                    elif pos.lower()[0] == 'r':\n",
    "                        return 'r'\n",
    "                    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "                    elif pos.lower()[0] == 'v':\n",
    "                        return 'v'\n",
    "                    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'\n",
    "                    else:\n",
    "                        return 'n'\n",
    "    \n",
    "            \n",
    "                token=[]\n",
    "                idx=row[\"spans\"]\n",
    "                ch=' '.join(idx)\n",
    "\n",
    "\n",
    "    \n",
    "                #words_stemmer(ch, type=\"PorterStemmer\", lang=\"english\", encoding=\"utf8\")\n",
    "                #Convert to lower case\n",
    "                n_grams=clean_doc(ch)\n",
    "                tokens=words_lemmatizer(n_grams)\n",
    "\n",
    "                #print(\"list ngrams--------------\\n\",n_grams)\n",
    "                row[\"token_spans\"] = tokens\n",
    "    \n",
    "                return row\n",
    "        #self.processed_data = self.processed_data.apply(stem_and_join, axis=1)\n",
    "        self.processed_data = self.processed_data.apply(tokenize_grams, axis=1)\n",
    "        \n",
    "    def build_wordlist(self, min_occurrences=0, max_occurences=500, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                      ):\n",
    "        \n",
    "        whitelist=[\"to\",\"on\",\"for\",\"up\",\"below\",\"short\",\"long\"]\n",
    "        self.wordlist = []\n",
    "        #stopwords=[]\n",
    "        words = Counter()\n",
    "      \n",
    "        for idx in self.processed_data.index:\n",
    "            words.update(self.processed_data.loc[idx, \"token_spans\"])\n",
    "\n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "        \n",
    "        \n",
    "        print(words.most_common())        \n",
    "\n",
    "        word_df = pd.DataFrame(data={\"word\": [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                     \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                               columns=[\"word\", \"occurrences\"])\n",
    "\n",
    "        word_df.to_csv(\"wordlist_test.csv\", index_label=\"idx\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]\n",
    "        \n",
    "        \n",
    "    def build_data_model(self):\n",
    "        \n",
    "        label_id = [\"ID\"]\n",
    "\n",
    "        columns = label_id + list(\n",
    "            map(lambda w: w ,self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "            if True:\n",
    "                # add label\n",
    "                current_id = self.processed_data.loc[idx, \"id\"]\n",
    "                \n",
    "                labels.append(current_id)\n",
    "                \n",
    "                current_row.append(current_id)\n",
    "\n",
    "            \n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"token_spans\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        return self.data_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1700"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TwitterData_Initialize()\n",
    "data.initialize(\"Microblog_Trainingdata.json\")\n",
    "\n",
    "len(data.processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = TwitterData_Initialize_test()\n",
    "data_test.initialize(\"Microblog_Trialdata.json\")\n",
    "\n",
    "data_test.processed_data.head(5)\n",
    "len(data_test.processed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 285.96952724456787 seconds ---\n"
     ]
    }
   ],
   "source": [
    "data.do_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "      <th>token_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$FB</td>\n",
       "      <td>719659409228451840</td>\n",
       "      <td>0.366</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[watching for bounce tomorrow]</td>\n",
       "      <td>[watch, for, bounce, tomorrow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$LUV</td>\n",
       "      <td>719904304207962112</td>\n",
       "      <td>0.638</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[record number of passengers served in 2015]</td>\n",
       "      <td>[record, number, of, passenger, serve, in]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$NFLX</td>\n",
       "      <td>5329774</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[out $NFLX -.35]</td>\n",
       "      <td>[out, nflx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$DIA</td>\n",
       "      <td>719891468173844480</td>\n",
       "      <td>0.460</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Looking for a strong bounce, Lunchtime rally ...</td>\n",
       "      <td>[look, for, strong, bounce, lunchtime, rally, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$PLUG</td>\n",
       "      <td>20091246</td>\n",
       "      <td>0.403</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Very intrigued with the technology and growth...</td>\n",
       "      <td>[very, intrigue, with, the, technology, and, g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cashtag                  id  sentiment score      source  \\\n",
       "0     $FB  719659409228451840            0.366     twitter   \n",
       "1    $LUV  719904304207962112            0.638     twitter   \n",
       "2   $NFLX             5329774           -0.494  stocktwits   \n",
       "3    $DIA  719891468173844480            0.460     twitter   \n",
       "4   $PLUG            20091246            0.403  stocktwits   \n",
       "\n",
       "                                               spans  \\\n",
       "0                     [watching for bounce tomorrow]   \n",
       "1       [record number of passengers served in 2015]   \n",
       "2                                   [out $NFLX -.35]   \n",
       "3  [Looking for a strong bounce, Lunchtime rally ...   \n",
       "4  [Very intrigued with the technology and growth...   \n",
       "\n",
       "                                         token_spans  \n",
       "0                     [watch, for, bounce, tomorrow]  \n",
       "1         [record, number, of, passenger, serve, in]  \n",
       "2                                        [out, nflx]  \n",
       "3  [look, for, strong, bounce, lunchtime, rally, ...  \n",
       "4  [very, intrigue, with, the, technology, and, g...  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.processed_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test.do_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "      <th>token_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$F</td>\n",
       "      <td>5540055</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Putting on a little $F short]</td>\n",
       "      <td>[put, on, little, short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>10752226</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short some]</td>\n",
       "      <td>[short, some]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$BAC</td>\n",
       "      <td>10920221</td>\n",
       "      <td>0.445</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[buying opportunity]</td>\n",
       "      <td>[buying, opportunity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$SHOR</td>\n",
       "      <td>12971398</td>\n",
       "      <td>0.661</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Scaling Up on Long Position]</td>\n",
       "      <td>[scale, up, on, long, position]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$JPM</td>\n",
       "      <td>16142438</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[its time to sell banks]</td>\n",
       "      <td>[it, time, to, sell, bank]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cashtag        id  sentiment score      source  \\\n",
       "0      $F   5540055           -0.454  stocktwits   \n",
       "1   $AAPL  10752226           -0.464  stocktwits   \n",
       "2    $BAC  10920221            0.445  stocktwits   \n",
       "3   $SHOR  12971398            0.661  stocktwits   \n",
       "4    $JPM  16142438           -0.763  stocktwits   \n",
       "\n",
       "                            spans                      token_spans  \n",
       "0  [Putting on a little $F short]         [put, on, little, short]  \n",
       "1                    [short some]                    [short, some]  \n",
       "2            [buying opportunity]            [buying, opportunity]  \n",
       "3   [Scaling Up on Long Position]  [scale, up, on, long, position]  \n",
       "4        [its time to sell banks]       [it, time, to, sell, bank]  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.processed_data.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('to', 235), ('on', 133), ('for', 128), ('stock', 126), ('up', 109), ('short', 95), ('look', 83), ('buy', 76), ('long', 74), ('today', 68), ('sell', 66), ('high', 66), ('call', 59), ('go', 54), ('market', 49), ('good', 47), ('nice', 44), ('new', 44), ('still', 43), ('get', 43), ('day', 43), ('add', 42), ('week', 38), ('top', 38), ('bullish', 37), ('like', 33), ('put', 32), ('close', 31), ('big', 30), ('rally', 29), ('hold', 29), ('low', 29), ('trade', 28), ('take', 28), ('move', 27), ('break', 26), ('breakout', 26), ('strong', 25), ('tsla', 25), ('setup', 25), ('price', 25), ('buying', 24), ('see', 24), ('fb', 24), ('play', 24), ('aapl', 24), ('run', 23), ('support', 23), ('back', 23), ('start', 23), ('come', 22), ('time', 22), ('could', 22), ('downgrade', 22), ('weak', 21), ('lead', 21), ('bad', 20), ('one', 20), ('bull', 20), ('make', 20), ('growth', 19), ('overbought', 19), ('may', 19), ('share', 19), ('year', 19), ('next', 18), ('great', 18), ('sale', 18), ('well', 18), ('sector', 18), ('gain', 18), ('bought', 18), ('earnings', 17), ('position', 17), ('morning', 17), ('point', 17), ('bounce', 16), ('company', 16), ('dip', 16), ('unusual', 16), ('positive', 16), ('apple', 16), ('ready', 16), ('watch', 15), ('loser', 15), ('money', 15), ('another', 15), ('last', 15), ('cover', 15), ('bear', 15), ('stochastic', 14), ('activity', 14), ('want', 14), ('friday', 14), ('longs', 14), ('trend', 13), ('fall', 13), ('google', 13), ('oil', 13), ('drop', 13), ('interest', 13), ('keep', 13), ('best', 13), ('far', 13), ('would', 12), ('continue', 12), ('insider', 12), ('dont', 12), ('think', 12), ('bottom', 12), ('set', 12), ('resistance', 12), ('china', 12), ('broke', 12), ('rating', 12), ('gap', 12), ('upside', 12), ('chart', 12), ('name', 12), ('since', 12), ('small', 12), ('sign', 12), ('green', 11), ('rate', 11), ('cuba', 11), ('end', 11), ('right', 11), ('potential', 10), ('news', 10), ('target', 10), ('head', 10), ('gold', 10), ('fail', 10), ('gainer', 10), ('line', 10), ('recall', 10), ('profit', 10), ('show', 10), ('work', 9), ('goog', 9), ('pay', 9), ('technical', 9), ('let', 9), ('term', 9), ('follow', 9), ('deal', 9), ('cash', 9), ('lot', 9), ('tech', 9), ('large', 9), ('business', 9), ('around', 9), ('find', 9), ('flag', 9), ('try', 9), ('pick', 9), ('number', 8), ('tesla', 8), ('stay', 8), ('volume', 8), ('facebook', 8), ('nicereally', 8), ('increase', 8), ('debt', 8), ('need', 8), ('squeeze', 8), ('bollinger', 8), ('user', 8), ('first', 8), ('weakness', 8), ('rise', 8), ('yahoo', 8), ('bid', 8), ('confident', 8), ('fast', 8), ('load', 8), ('major', 8), ('signal', 8), ('say', 8), ('industry', 8), ('daily', 8), ('cheap', 8), ('equity', 8), ('cloud', 8), ('dividend', 8), ('tomorrow', 7), ('love', 7), ('googl', 7), ('real', 7), ('risk', 7), ('decline', 7), ('double', 7), ('level', 7), ('base', 7), ('im', 7), ('beat', 7), ('loss', 7), ('oversold', 7), ('upper', 7), ('band', 7), ('ratio', 7), ('quality', 7), ('upgrade', 7), ('internet', 7), ('enter', 7), ('billion', 7), ('spy', 7), ('monday', 7), ('winner', 7), ('amzn', 7), ('afternoon', 7), ('cap', 7), ('globally', 7), ('every', 7), ('bank', 7), ('airplane', 7), ('way', 7), ('option', 7), ('sight', 7), ('investor', 7), ('yet', 7), ('near', 7), ('bearish', 7), ('side', 7), ('reason', 7), ('performer', 7), ('lose', 7), ('expect', 7), ('index', 7), ('weekly', 7), ('raise', 7), ('retailer', 7), ('nflx', 6), ('offer', 6), ('seem', 6), ('help', 6), ('twitter', 6), ('yesterday', 6), ('swing', 6), ('careful', 6), ('already', 6), ('entry', 6), ('flow', 6), ('cut', 6), ('forecast', 6), ('little', 6), ('pie', 6), ('sky', 6), ('premarket', 6), ('million', 6), ('patent', 6), ('pop', 6), ('fill', 6), ('twtr', 6), ('remains', 6), ('amaze', 6), ('bet', 6), ('below', 6), ('stop', 6), ('black', 6), ('soon', 6), ('turn', 6), ('analyst', 6), ('ripe', 6), ('hospitality', 6), ('pe', 6), ('favorite', 6), ('model', 6), ('car', 6), ('less', 6), ('street', 6), ('msft', 6), ('crossover', 6), ('yhoo', 6), ('push', 6), ('worth', 6), ('hod', 6), ('grow', 6), ('record', 5), ('wont', 5), ('momentum', 5), ('possible', 5), ('report', 5), ('bidu', 5), ('found', 5), ('eps', 5), ('fed', 5), ('tight', 5), ('second', 5), ('return', 5), ('light', 5), ('exit', 5), ('red', 5), ('watchlist', 5), ('patience', 5), ('might', 5), ('really', 5), ('awaits', 5), ('job', 5), ('netpayoutyields', 5), ('wait', 5), ('act', 5), ('outperform', 5), ('upgraded', 5), ('iphone', 5), ('bidding', 5), ('valuation', 5), ('shipment', 5), ('slip', 5), ('miner', 5), ('bot', 5), ('nothing', 5), ('ecommerce', 5), ('hard', 5), ('result', 5), ('much', 5), ('amazon', 5), ('party', 5), ('conviction', 5), ('portfolio', 5), ('slid', 5), ('reversal', 5), ('vrx', 5), ('almost', 5), ('eye', 5), ('massive', 5), ('hope', 5), ('opportunity', 5), ('passenger', 4), ('serve', 4), ('lunchtime', 4), ('longerterm', 4), ('basis', 4), ('trading', 4), ('outcome', 4), ('chinese', 4), ('place', 4), ('revenue', 4), ('percent', 4), ('mkt', 4), ('pull', 4), ('luv', 4), ('guess', 4), ('shareholder', 4), ('japan', 4), ('ibb', 4), ('improve', 4), ('asset', 4), ('biz', 4), ('crisis', 4), ('dangerous', 4), ('phase', 4), ('shld', 4), ('worried', 4), ('icloud', 4), ('catch', 4), ('finally', 4), ('u', 4), ('future', 4), ('two', 4), ('channel', 4), ('chevron', 4), ('held', 4), ('stake', 4), ('early', 4), ('overvalue', 4), ('even', 4), ('fear', 4), ('three', 4), ('dollar', 4), ('fuel', 4), ('use', 4), ('didnt', 4), ('surge', 4), ('competition', 4), ('total', 4), ('list', 4), ('fund', 4), ('issue', 4), ('outperformers', 4), ('ranked', 4), ('average', 4), ('possibly', 4), ('conversation', 4), ('downbeaten', 4), ('zone', 4), ('retail', 4), ('heat', 4), ('hit', 4), ('volatility', 4), ('co', 4), ('progress', 4), ('launch', 4), ('blood', 4), ('uptrend', 4), ('thing', 4), ('enough', 4), ('ever', 4), ('jump', 4), ('triangle', 4), ('hang', 4), ('coal', 4), ('aggressive', 4), ('roll', 4), ('receives', 4), ('cautious', 4), ('class', 4), ('ahead', 4), ('quarterly', 4), ('store', 3), ('anymore', 3), ('edge', 3), ('acquisition', 3), ('cant', 3), ('downtrend', 3), ('nicely', 3), ('feed', 3), ('lung', 3), ('filing', 3), ('soros', 3), ('condition', 3), ('imagine', 3), ('order', 3), ('downturn', 3), ('watchingand', 3), ('skf', 3), ('wish', 3), ('fell', 3), ('due', 3), ('slow', 3), ('consolidation', 3), ('crm', 3), ('also', 3), ('airline', 3), ('battle', 3), ('route', 3), ('aal', 3), ('save', 3), ('jblu', 3), ('blackberry', 3), ('min', 3), ('bulb', 3), ('consider', 3), ('later', 3), ('drag', 3), ('qqq', 3), ('outlook', 3), ('people', 3), ('cost', 3), ('area', 3), ('recently', 3), ('november', 3), ('unemployment', 3), ('plan', 3), ('noncore', 3), ('ascend', 3), ('pcln', 3), ('downside', 3), ('notable', 3), ('among', 3), ('liquid', 3), ('alcohol', 3), ('jet', 3), ('eligible', 3), ('commercial', 3), ('flight', 3), ('pivotal', 3), ('research', 3), ('story', 3), ('extends', 3), ('deadline', 3), ('grows', 3), ('mrkt', 3), ('washout', 3), ('wfm', 3), ('test', 3), ('capital', 3), ('pc', 3), ('building', 3), ('trigger', 3), ('consistent', 3), ('adbe', 3), ('neutral', 3), ('change', 3), ('beast', 3), ('trail', 3), ('relative', 3), ('value', 3), ('profitable', 3), ('nov', 3), ('trader', 3), ('trendline', 3), ('everyone', 3), ('february', 3), ('resilient', 3), ('doesnt', 3), ('endp', 3), ('rumor', 3), ('huge', 3), ('march', 3), ('quarter', 3), ('software', 3), ('problem', 3), ('sharp', 3), ('leave', 3), ('inc', 3), ('goldman', 3), ('sachs', 3), ('reiterates', 3), ('couple', 3), ('deutsche', 3), ('five', 3), ('starboard', 3), ('r', 3), ('form', 3), ('confirm', 3), ('excite', 3), ('clear', 3), ('advantage', 3), ('rsi', 3), ('engulf', 3), ('production', 3), ('alphabet', 3), ('decision', 3), ('accumulate', 3), ('win', 3), ('candle', 3), ('global', 3), ('least', 3), ('likely', 3), ('lift', 3), ('limp', 3), ('crutch', 3), ('pain', 3), ('worry', 3), ('medium', 3), ('thats', 3), ('flush', 3), ('accumulation', 3), ('amd', 3), ('holding', 3), ('finish', 3), ('consumer', 3), ('wold', 3), ('anly', 3), ('pzzi', 3), ('screen', 3), ('soar', 3), ('underperform', 3), ('dethrones', 3), ('round', 3), ('buyer', 3), ('sound', 3), ('auto', 3), ('reverse', 3), ('empty', 2), ('suck', 2), ('master', 2), ('dump', 2), ('reserve', 2), ('key', 2), ('idea', 2), ('half', 2), ('bac', 2), ('alibaba', 2), ('ipo', 2), ('fence', 2), ('mdxg', 2), ('without', 2), ('defy', 2), ('action', 2), ('volatile', 2), ('estimize', 2), ('consensus', 2), ('float', 2), ('fxp', 2), ('ure', 2), ('fcx', 2), ('step', 2), ('messenger', 2), ('loyalty', 2), ('within', 2), ('breadth', 2), ('catalyst', 2), ('strategy', 2), ('piper', 2), ('rebound', 2), ('joint', 2), ('governance', 2), ('core', 2), ('igh', 2), ('concern', 2), ('feel', 2), ('toppy', 2), ('consolidate', 2), ('decent', 2), ('beating', 2), ('tell', 2), ('someone', 2), ('ea', 2), ('game', 2), ('creep', 2), ('bo', 2), ('supermarket', 2), ('tesco', 2), ('axe', 2), ('uk', 2), ('brand', 2), ('pressure', 2), ('along', 2), ('undervalue', 2), ('crack', 2), ('intc', 2), ('gild', 2), ('bwld', 2), ('online', 2), ('micron', 2), ('pound', 2), ('hot', 2), ('expe', 2), ('mix', 2), ('ctrp', 2), ('trip', 2), ('reiterate', 2), ('fly', 2), ('santa', 2), ('blow', 2), ('personal', 2), ('peak', 2), ('smartphone', 2), ('lol', 2), ('net', 2), ('hedge', 2), ('case', 2), ('joe', 2), ('wks', 2), ('inflow', 2), ('prim', 2), ('domestic', 2), ('automaker', 2), ('fwd', 2), ('htz', 2), ('lvmh', 2), ('affordable', 2), ('luxe', 2), ('fine', 2), ('dutyfree', 2), ('mainstream', 2), ('shopper', 2), ('cry', 2), ('happen', 2), ('wouldnt', 2), ('selloff', 2), ('recommendation', 2), ('wave', 2), ('sentiment', 2), ('parabolic', 2), ('chase', 2), ('boy', 2), ('never', 2), ('announce', 2), ('monthly', 2), ('etf', 2), ('rather', 2), ('international', 2), ('exposure', 2), ('range', 2), ('detector', 2), ('autonomous', 2), ('handle', 2), ('fit', 2), ('nugt', 2), ('cross', 2), ('ill', 2), ('word', 2), ('hype', 2), ('promising', 2), ('orcl', 2), ('must', 2), ('caution', 2), ('congrats', 2), ('picked', 2), ('mgm', 2), ('io', 2), ('player', 2), ('bit', 2), ('risky', 2), ('flop', 2), ('sq', 2), ('toprated', 2), ('impact', 2), ('financial', 2), ('lnkd', 2), ('grpn', 2), ('track', 2), ('exceed', 2), ('ive', 2), ('railroad', 2), ('retirement', 2), ('acct', 2), ('slowly', 2), ('men', 2), ('g', 2), ('plus', 2), ('irish', 2), ('rid', 2), ('generic', 2), ('ur', 2), ('mu', 2), ('judge', 2), ('forward', 2), ('post', 2), ('sad', 2), ('zuck', 2), ('vol', 2), ('mark', 2), ('whole', 2), ('space', 2), ('combine', 2), ('pullback', 2), ('ooks', 2), ('pretty', 2), ('penny', 2), ('tv', 2), ('multiple', 2), ('anyone', 2), ('product', 2), ('avail', 2), ('ride', 2), ('hike', 2), ('recent', 2), ('tfm', 2), ('wise', 2), ('compete', 2), ('se', 2), ('bubble', 2), ('ndx', 2), ('internals', 2), ('bullshit', 2), ('runner', 2), ('refuse', 2), ('opening', 2), ('service', 2), ('tough', 2), ('economy', 2), ('wall', 2), ('st', 2), ('dry', 2), ('stare', 2), ('abyss', 2), ('war', 2), ('spread', 2), ('takeover', 2), ('longterm', 2), ('cupandhandle', 2), ('shape', 2), ('favour', 2), ('bail', 2), ('pypl', 2), ('profitability', 2), ('dirt', 2), ('minute', 2), ('horizontal', 2), ('rig', 2), ('intraday', 2), ('though', 2), ('investment', 2), ('nobody', 2), ('timing', 2), ('active', 2), ('venture', 2), ('month', 2), ('highflyer', 2), ('chance', 2), ('bidder', 2), ('pride', 2), ('social', 2), ('gettin', 2), ('bleeding', 2), ('scenario', 2), ('ah', 2), ('reward', 2), ('content', 2), ('nervous', 2), ('biotech', 2), ('trouble', 2), ('moody', 2), ('scalp', 2), ('elon', 2), ('comment', 2), ('ta', 2), ('ceo', 2), ('tax', 2), ('brutal', 2), ('ton', 2), ('balance', 2), ('impressive', 2), ('chromebooks', 2), ('walmart', 2), ('staple', 2), ('faster', 2), ('shine', 2), ('create', 2), ('stomp', 2), ('dream', 2), ('motor', 2), ('old', 2), ('relatively', 2), ('compare', 2), ('broad', 2), ('cure', 2), ('enjoy', 2), ('tremendous', 2), ('hi', 2), ('beta', 2), ('bbry', 2), ('barclays', 2), ('institution', 2), ('shoulder', 2), ('highly', 2), ('demand', 2), ('update', 2), ('everything', 2), ('leadership', 2), ('sure', 2), ('yeah', 2), ('solid', 2), ('bmw', 2), ('rideshare', 2), ('po', 2), ('isnt', 2), ('wake', 2), ('hat', 2), ('announces', 2), ('hour', 2), ('riskreward', 2), ('initiate', 2), ('whose', 2), ('yeartodate', 2), ('multiyear', 2), ('trust', 2), ('maker', 2), ('together', 2), ('brick', 2), ('mortar', 2), ('cheaper', 2), ('earn', 2), ('guidance', 2), ('give', 2), ('infringement', 2), ('damage', 2), ('serious', 2), ('ibm', 2), ('arm', 2), ('begin', 2), ('assault', 2), ('intels', 2), ('server', 2), ('franchise', 2), ('produce', 2), ('macd', 2), ('crush', 2), ('recession', 2), ('upward', 2), ('suitor', 2), ('excellent', 2), ('drive', 2), ('mail', 2), ('shipping', 2), ('rat', 2), ('tag', 2), ('customer', 2), ('omg', 2), ('intrigue', 1), ('technology', 1), ('absolute', 1), ('garbage', 1), ('totally', 1), ('mispriced', 1), ('supply', 1), ('chain', 1), ('touch', 1), ('lately', 1), ('alltime', 1), ('chum', 1), ('hoopla', 1), ('hide', 1), ('drought', 1), ('soil', 1), ('hip', 1), ('sinking', 1), ('weekend', 1), ('care', 1), ('simo', 1), ('pour', 1), ('direction', 1), ('man', 1), ('wasnt', 1), ('appeal', 1), ('tap', 1), ('reenter', 1), ('earlier', 1), ('wmt', 1), ('frustrate', 1), ('bar', 1), ('immediately', 1), ('thanks', 1), ('currently', 1), ('rejoice', 1), ('shit', 1), ('open', 1), ('rg', 1), ('mdlz', 1), ('welcome', 1), ('marriottstarwood', 1), ('peachy', 1), ('prospect', 1), ('steady', 1), ('gainsadvances', 1), ('slowe', 1), ('aggressively', 1), ('riser', 1), ('symantec', 1), ('renewal', 1), ('enterprise', 1), ('solar', 1), ('dnf', 1), ('crew', 1), ('finger', 1), ('button', 1), ('resist', 1), ('holiday', 1), ('breakup', 1), ('previous', 1), ('supt', 1), ('hereexpect', 1), ('steal', 1), ('multi', 1), ('mcd', 1), ('stability', 1), ('question', 1), ('mon', 1), ('bankruptcy', 1), ('gogo', 1), ('eu', 1), ('closer', 1), ('charge', 1), ('android', 1), ('precedes', 1), ('recover', 1), ('oct', 1), ('split', 1), ('lipstick', 1), ('pig', 1), ('concentrate', 1), ('correction', 1), ('sso', 1), ('tiffany', 1), ('dow', 1), ('crunch', 1), ('diamond', 1), ('rough', 1), ('uvxy', 1), ('leap', 1), ('pstv', 1), ('retreat', 1), ('clockwork', 1), ('ifts', 1), ('system', 1), ('terribly', 1), ('tnh', 1), ('ring', 1), ('register', 1), ('mtd', 1), ('bond', 1), ('course', 1), ('bsx', 1), ('brianwieser', 1), ('brian', 1), ('success', 1), ('galaxy', 1), ('factor', 1), ('anything', 1), ('repeat', 1), ('succeed', 1), ('dominate', 1), ('uco', 1), ('recuperates', 1), ('monger', 1), ('die', 1), ('usage', 1), ('approximately', 1), ('mln', 1), ('energy', 1), ('white', 1), ('crzo', 1), ('land', 1), ('transport', 1), ('always', 1), ('attention', 1), ('nyse', 1), ('bring', 1), ('spx', 1), ('itting', 1), ('retest', 1), ('warrant', 1), ('poise', 1), ('peg', 1), ('historical', 1), ('enthuse', 1), ('reliability', 1), ('avoid', 1), ('bios', 1), ('lucky', 1), ('enuff', 1), ('h', 1), ('quite', 1), ('implode', 1), ('stress', 1), ('carl', 1), ('kirst', 1), ('bmo', 1), ('cult', 1), ('stocktwits', 1), ('actually', 1), ('disgust', 1), ('suv', 1), ('seat', 1), ('financing', 1), ('widen', 1), ('alongside', 1), ('nsa', 1), ('leak', 1), ('false', 1), ('maintain', 1), ('yield', 1), ('divy', 1), ('subject', 1), ('decay', 1), ('rebalancin', 1), ('life', 1), ('emerge', 1), ('diversifies', 1), ('capped', 1), ('meltdown', 1), ('cup', 1), ('comparable', 1), ('consolidates', 1), ('past', 1), ('several', 1), ('flat', 1), ('suddenly', 1), ('realize', 1), ('sune', 1), ('fuck', 1), ('mode', 1), ('kick', 1), ('shirt', 1), ('pouch', 1), ('hospital', 1), ('reit', 1), ('slaughter', 1), ('important', 1), ('stockco', 1), ('choice', 1), ('positve', 1), ('allergen', 1), ('endo', 1), ('wynn', 1), ('crow', 1), ('breed', 1), ('room', 1), ('orgetting', 1), ('goo', 1), ('usio', 1), ('ops', 1), ('gmcr', 1), ('outperforms', 1), ('earish', 1), ('nt', 1), ('pas', 1), ('onto', 1), ('hum', 1), ('ease', 1), ('ron', 1), ('johnson', 1), ('inject', 1), ('culture', 1), ('jc', 1), ('penney', 1), ('prettiest', 1), ('candlestick', 1), ('agree', 1), ('closely', 1), ('posistion', 1), ('fader', 1), ('rest', 1), ('implies', 1), ('agn', 1), ('gist', 1), ('phone', 1), ('th', 1), ('rigl', 1), ('apr', 1), ('stx', 1), ('hipments', 1), ('sap', 1), ('disappoints', 1), ('license', 1), ('us', 1), ('turmoil', 1), ('mena', 1), ('region', 1), ('shipper', 1), ('perform', 1), ('alert', 1), ('farm', 1), ('apparently', 1), ('intel', 1), ('woman', 1), ('paid', 1), ('gender', 1), ('parity', 1), ('avenger', 1), ('sick', 1), ('stan', 1), ('lee', 1), ('court', 1), ('state', 1), ('alone', 1), ('unloaded', 1), ('ensign', 1), ('group', 1), ('nasdaqensg', 1), ('weekdid', 1), ('snatch', 1), ('lawsuit', 1), ('russian', 1), ('search', 1), ('engine', 1), ('kous', 1), ('cocacola', 1), ('margin', 1), ('redtogreen', 1), ('gate', 1), ('ffiv', 1), ('chrw', 1), ('thru', 1), ('dma', 1), ('reargument', 1), ('deny', 1), ('upcoming', 1), ('cafn', 1), ('cachet', 1), ('solution', 1), ('stochastics', 1), ('shopping', 1), ('intract', 1), ('aapls', 1), ('broker', 1), ('reduce', 1), ('picture', 1), ('brilliant', 1), ('boywonder', 1), ('visionary', 1), ('unhappy', 1), ('zuckerberg', 1), ('history', 1), ('lack', 1), ('pr', 1), ('shame', 1), ('belooooooowwww', 1), ('semiconductor', 1), ('awesomely', 1), ('grabbed', 1), ('tme', 1), ('triple', 1), ('sexy', 1), ('food', 1), ('vote', 1), ('activist', 1), ('initiative', 1), ('southwest', 1), ('fan', 1), ('tripadvisor', 1), ('vehicle', 1), ('upturn', 1), ('mjn', 1), ('reject', 1), ('commodity', 1), ('flong', 1), ('via', 1), ('flati', 1), ('kroger', 1), ('kr', 1), ('onetime', 1), ('zombie', 1), ('cat', 1), ('millennials', 1), ('tasty', 1), ('damn', 1), ('happily', 1), ('baba', 1), ('fouryear', 1), ('opec', 1), ('kept', 1), ('unchange', 1), ('sideways', 1), ('makeup', 1), ('ugliness', 1), ('slight', 1), ('innovator', 1), ('xboxlive', 1), ('gooo', 1), ('free', 1), ('brother', 1), ('shadow', 1), ('id', 1), ('kre', 1), ('something', 1), ('gonna', 1), ('cusp', 1), ('sit', 1), ('ema', 1), ('force', 1), ('poke', 1), ('exceptionally', 1), ('romney', 1), ('midsingle', 1), ('digit', 1), ('ebitda', 1), ('steep', 1), ('view', 1), ('btu', 1), ('paint', 1), ('fundamentally', 1), ('wound', 1), ('samsung', 1), ('proprietary', 1), ('liquidity', 1), ('ousy', 1), ('nine', 1), ('ra', 1), ('easonably', 1), ('institutional', 1), ('fave', 1), ('glamour', 1), ('gilead', 1), ('european', 1), ('dumped', 1), ('prlb', 1), ('cfo', 1), ('eat', 1), ('burn', 1), ('strictly', 1), ('tout', 1), ('rock', 1), ('cold', 1), ('dent', 1), ('tl', 1), ('bsmx', 1), ('applestores', 1), ('rail', 1), ('heard', 1), ('netflix', 1), ('rip', 1), ('contract', 1), ('splat', 1), ('poor', 1), ('stk', 1), ('clearly', 1), ('sue', 1), ('ft', 1), ('largecap', 1), ('fundholders', 1), ('witness', 1), ('truly', 1), ('stun', 1), ('momo', 1), ('leader', 1), ('mull', 1), ('surprised', 1), ('robot', 1), ('expose', 1), ('alpha', 1), ('adobe', 1), ('alot', 1), ('verizon', 1), ('b', 1), ('sectorbreadth', 1), ('industrials', 1), ('bullishness', 1), ('uch', 1), ('bloat', 1), ('vampirekeep', 1), ('ief', 1), ('everyday', 1), ('tue', 1), ('patient', 1), ('bright', 1), ('exp', 1), ('imo', 1), ('remember', 1), ('told', 1), ('fix', 1), ('email', 1), ('yesterda', 1), ('hop', 1), ('calm', 1), ('advance', 1), ('licensing', 1), ('agreement', 1), ('neither', 1), ('gale', 1), ('stack', 1), ('poorly', 1), ('ownership', 1), ('lease', 1), ('boeing', 1), ('cargo', 1), ('plane', 1), ('platform', 1), ('dailymail', 1), ('perfect', 1), ('demographic', 1), ('electronic', 1), ('art', 1), ('attracts', 1), ('expansion', 1), ('intact', 1), ('regn', 1), ('plunge', 1), ('tablet', 1), ('timberrrr', 1), ('wittine', 1), ('longbow', 1), ('hire', 1), ('former', 1), ('executive', 1), ('george', 1), ('stathakopoulos', 1), ('corporate', 1), ('data', 1), ('security', 1), ('spec', 1), ('wirs', 1), ('invest', 1), ('minuscule', 1), ('ziv', 1), ('handsome', 1), ('receivable', 1), ('zero', 1), ('cisco', 1), ('rht', 1), ('illustrate', 1), ('wtsl', 1), ('footprint', 1), ('drug', 1), ('perky', 1), ('negative', 1), ('pl', 1), ('hardware', 1), ('disposal', 1), ('appease', 1), ('city', 1), ('backer', 1), ('usual', 1), ('tank', 1), ('bollies', 1), ('hulu', 1), ('cable', 1), ('max', 1), ('stance', 1), ('broken', 1), ('roic', 1), ('lock', 1), ('tom', 1), ('upwards', 1), ('rotate', 1), ('strength', 1), ('october', 1), ('baird', 1), ('mom', 1), ('grandma', 1), ('yoku', 1), ('secure', 1), ('mover', 1), ('stellar', 1), ('beautifully', 1), ('orchestrate', 1), ('marke', 1), ('hardtotreat', 1), ('statement', 1), ('ceiling', 1), ('spot', 1), ('cheerleader', 1), ('actual', 1), ('mbly', 1), ('nly', 1), ('overall', 1), ('gots', 1), ('aezs', 1), ('erinn', 1), ('murphy', 1), ('jaffray', 1), ('finl', 1), ('longer', 1), ('quickie', 1), ('pattern', 1), ('note', 1), ('lighter', 1), ('finance', 1), ('unfilled', 1), ('july', 1), ('significant', 1), ('addding', 1), ('xoma', 1), ('blurb', 1), ('knife', 1), ('shortput', 1), ('conquer', 1), ('mobile', 1), ('astonish', 1), ('fortune', 1), ('biggies', 1), ('diminish', 1), ('inverse', 1), ('gpro', 1), ('embarrass', 1), ('bumping', 1), ('overhead', 1), ('robust', 1), ('hopefully', 1), ('peep', 1), ('camt', 1), ('tape', 1), ('infection', 1), ('prompt', 1), ('regulatory', 1), ('review', 1), ('gld', 1), ('yes', 1), ('brent', 1), ('premier', 1), ('development', 1), ('ezpw', 1), ('sma', 1), ('chunk', 1), ('dec', 1), ('quick', 1), ('competitor', 1), ('inevitable', 1), ('successor', 1), ('northern', 1), ('telecom', 1), ('competent', 1), ('tire', 1), ('traction', 1), ('anglo', 1), ('cleary', 1), ('surpass', 1), ('bb', 1), ('expands', 1), ('repurchase', 1), ('program', 1), ('safe', 1), ('ewz', 1), ('doom', 1), ('destroyed', 1), ('starbucks', 1), ('premium', 1), ('netapp', 1), ('macquarie', 1), ('confirmation', 1), ('beautiful', 1), ('trendbase', 1), ('obby', 1), ('garland', 1), ('probably', 1), ('loooooongggggg', 1), ('checkout', 1), ('recommend', 1), ('stomach', 1), ('left', 1), ('svxy', 1), ('guy', 1), ('kill', 1), ('atching', 1), ('sto', 1), ('whisperbeat', 1), ('jefferies', 1), ('till', 1), ('wkly', 1), ('downward', 1), ('completes', 1), ('icoa', 1), ('continuation', 1), ('actively', 1), ('tmrw', 1), ('beaten', 1), ('esports', 1), ('viewership', 1), ('seriously', 1), ('played', 1), ('credit', 1), ('somewhere', 1), ('annoy', 1), ('indicator', 1), ('logistics', 1), ('capex', 1), ('miss', 1), ('comparision', 1), ('stream', 1), ('mini', 1), ('retracement', 1), ('tail', 1), ('naturalgas', 1), ('settle', 1), ('hey', 1), ('barely', 1), ('bux', 1), ('impressed', 1), ('offshoring', 1), ('exdividend', 1), ('sina', 1), ('tol', 1), ('ask', 1), ('buddy', 1), ('outside', 1), ('toxic', 1), ('sweet', 1), ('jigginess', 1), ('many', 1), ('dive', 1), ('breach', 1), ('revisit', 1), ('bus', 1), ('partial', 1), ('afterhour', 1), ('shark', 1), ('ncreases', 1), ('premiumm', 1), ('kndi', 1), ('electric', 1), ('ciscosynata', 1), ('gamechanger', 1), ('csco', 1), ('baltic', 1), ('declares', 1), ('cyber', 1), ('kindle', 1), ('family', 1), ('giddy', 1), ('holy', 1), ('batman', 1), ('fck', 1), ('bird', 1), ('celg', 1), ('agio', 1), ('stableize', 1), ('transform', 1), ('appear', 1), ('exhaust', 1), ('quietly', 1), ('zigzag', 1), ('aug', 1), ('sink', 1), ('magically', 1), ('mww', 1), ('recovery', 1), ('toward', 1), ('encouragement', 1), ('questcor', 1), ('wilful', 1), ('calculation', 1), ('late', 1), ('targe', 1), ('tuesday', 1), ('night', 1), ('regional', 1), ('banking', 1), ('somewhat', 1), ('tempt', 1), ('bermuda', 1), ('plenty', 1), ('daylight', 1), ('epic', 1), ('sne', 1), ('learnt', 1), ('integrate', 1), ('device', 1), ('dog', 1), ('ltm', 1), ('trov', 1), ('reliably', 1), ('cancer', 1), ('mutation', 1), ('jackpot', 1), ('blew', 1), ('zmh', 1), ('wayyy', 1), ('slowmotion', 1), ('disruption', 1), ('bto', 1), ('intermediate', 1), ('closng', 1), ('shortterm', 1), ('omnivision', 1), ('xover', 1), ('expectation', 1), ('notice', 1), ('doomsaying', 1), ('disappear', 1), ('climb', 1), ('prob', 1), ('hznp', 1), ('amend', 1), ('bell', 1), ('easily', 1), ('safety', 1), ('equipment', 1), ('apart', 1), ('mention', 1), ('suitee', 1), ('capture', 1), ('impossible', 1), ('factual', 1), ('argument', 1), ('apparent', 1), ('buyout', 1), ('tplm', 1), ('petroleum', 1), ('expand', 1), ('caliber', 1), ('isrg', 1), ('pozn', 1), ('v', 1), ('est', 1), ('prior', 1), ('revise', 1), ('enters', 1), ('acquire', 1), ('yah', 1), ('viab', 1), ('sudden', 1), ('optimism', 1), ('constructive', 1), ('unt', 1), ('sdrl', 1), ('inside', 1), ('doubledigit', 1), ('hangin', 1), ('thread', 1), ('click', 1), ('away', 1), ('cliff', 1), ('process', 1), ('streamline', 1), ('structure', 1), ('especially', 1), ('divestment', 1), ('adx', 1), ('ad', 1), ('partnership', 1), ('talk', 1), ('nimble', 1), ('lei', 1), ('slide', 1), ('defect', 1), ('junior', 1), ('fee', 1), ('yelp', 1), ('scam', 1), ('innovate', 1), ('develo', 1), ('quiet', 1), ('nearterm', 1), ('accuses', 1), ('znga', 1), ('copyright', 1), ('joke', 1), ('buyerbeware', 1), ('azpn', 1), ('wfc', 1), ('underestimate', 1), ('express', 1), ('script', 1), ('loses', 1), ('smh', 1), ('sweeper', 1), ('hearing', 1), ('rumour', 1), ('announcement', 1), ('nlsn', 1), ('marketing', 1), ('release', 1), ('window', 1), ('xp', 1), ('popular', 1), ('reentry', 1), ('cite', 1), ('spending', 1), ('anywhere', 1), ('shoot', 1), ('moon', 1), ('pumper', 1), ('breakin', 1), ('dnkn', 1), ('eod', 1), ('explosive', 1), ('caught', 1), ('moneylosing', 1), ('junk', 1), ('honestly', 1), ('disappoint', 1), ('grab', 1), ('aside', 1), ('qcom', 1), ('kind', 1), ('marriott', 1), ('starwood', 1), ('portion', 1), ('lithium', 1), ('smack', 1), ('legitimate', 1), ('ignorant', 1), ('allow', 1), ('middle', 1), ('littlediscussed', 1), ('pro', 1), ('movement', 1), ('stuck', 1), ('mud', 1), ('warn', 1), ('upbeat', 1), ('raid', 1), ('steeper', 1), ('kneecap', 1), ('bexp', 1), ('become', 1), ('goto', 1), ('site', 1), ('category', 1), ('invests', 1), ('southeast', 1), ('asian', 1), ('lazada', 1), ('efut', 1), ('em', 1), ('attractive', 1), ('deliver', 1), ('spill', 1), ('lotto', 1), ('shot', 1), ('howing', 1), ('divergence', 1), ('fa', 1), ('woulda', 1), ('death', 1), ('horror', 1), ('event', 1), ('solarcity', 1), ('electricity', 1), ('power', 1), ('fleet', 1), ('kog', 1), ('shorters', 1), ('learn', 1), ('lesson', 1), ('mess', 1), ('lo', 1), ('skyrocket', 1), ('pm', 1), ('general', 1), ('receive', 1), ('fargo', 1), ('chill', 1), ('pill', 1), ('mile', 1), ('beyond', 1), ('experience', 1), ('rejection', 1), ('freewater', 1), ('famous', 1), ('lunch', 1), ('fade', 1), ('routine', 1), ('hand', 1), ('metal', 1), ('mr', 1), ('greed', 1), ('resolve', 1), ('trap', 1), ('whats', 1), ('marry', 1), ('christmas', 1), ('deep', 1), ('machinelearning', 1), ('ai', 1), ('territory', 1), ('supercomputer', 1), ('chk', 1), ('seller', 1), ('saw', 1), ('unless', 1), ('censor', 1), ('ross', 1), ('rbc', 1), ('wedbush', 1), ('pt', 1), ('lower', 1), ('sqns', 1), ('torture', 1), ('feb', 1), ('meeting', 1), ('sanofi', 1), ('often', 1), ('wrong', 1), ('compass', 1), ('bias', 1), ('lowquality', 1), ('ongoing', 1), ('please', 1), ('accept', 1), ('condolence', 1), ('buysupport', 1), ('gotta', 1), ('threaten', 1), ('descend', 1), ('breakdown', 1), ('surveillance', 1), ('camera', 1), ('infect', 1), ('malware', 1), ('brings', 1), ('upscale', 1), ('concept', 1), ('ownitdonttradeit', 1), ('skin', 1), ('gamepositive', 1), ('overweight', 1), ('callput', 1)]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "      <th>token_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$FB</td>\n",
       "      <td>719659409228451840</td>\n",
       "      <td>0.366</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[watching for bounce tomorrow]</td>\n",
       "      <td>[watch, for, bounce, tomorrow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$LUV</td>\n",
       "      <td>719904304207962112</td>\n",
       "      <td>0.638</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[record number of passengers served in 2015]</td>\n",
       "      <td>[record, number, of, passenger, serve, in]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$NFLX</td>\n",
       "      <td>5329774</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[out $NFLX -.35]</td>\n",
       "      <td>[out, nflx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$DIA</td>\n",
       "      <td>719891468173844480</td>\n",
       "      <td>0.460</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Looking for a strong bounce, Lunchtime rally ...</td>\n",
       "      <td>[look, for, strong, bounce, lunchtime, rally, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$PLUG</td>\n",
       "      <td>20091246</td>\n",
       "      <td>0.403</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Very intrigued with the technology and growth...</td>\n",
       "      <td>[very, intrigue, with, the, technology, and, g...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cashtag                  id  sentiment score      source  \\\n",
       "0     $FB  719659409228451840            0.366     twitter   \n",
       "1    $LUV  719904304207962112            0.638     twitter   \n",
       "2   $NFLX             5329774           -0.494  stocktwits   \n",
       "3    $DIA  719891468173844480            0.460     twitter   \n",
       "4   $PLUG            20091246            0.403  stocktwits   \n",
       "\n",
       "                                               spans  \\\n",
       "0                     [watching for bounce tomorrow]   \n",
       "1       [record number of passengers served in 2015]   \n",
       "2                                   [out $NFLX -.35]   \n",
       "3  [Looking for a strong bounce, Lunchtime rally ...   \n",
       "4  [Very intrigued with the technology and growth...   \n",
       "\n",
       "                                         token_spans  \n",
       "0                     [watch, for, bounce, tomorrow]  \n",
       "1         [record, number, of, passenger, serve, in]  \n",
       "2                                        [out, nflx]  \n",
       "3  [look, for, strong, bounce, lunchtime, rally, ...  \n",
       "4  [very, intrigue, with, the, technology, and, g...  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.build_wordlist()\n",
    "data.processed_data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1844"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow, labels = data.build_data_model()\n",
    "bow.head(5)\n",
    "len(bow.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "      <th>token_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$F</td>\n",
       "      <td>5540055</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Putting on a little $F short]</td>\n",
       "      <td>[put, on, little, short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>10752226</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short some]</td>\n",
       "      <td>[short, some]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$BAC</td>\n",
       "      <td>10920221</td>\n",
       "      <td>0.445</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[buying opportunity]</td>\n",
       "      <td>[buying, opportunity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$SHOR</td>\n",
       "      <td>12971398</td>\n",
       "      <td>0.661</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Scaling Up on Long Position]</td>\n",
       "      <td>[scale, up, on, long, position]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$JPM</td>\n",
       "      <td>16142438</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[its time to sell banks]</td>\n",
       "      <td>[it, time, to, sell, bank]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cashtag        id  sentiment score      source  \\\n",
       "0      $F   5540055           -0.454  stocktwits   \n",
       "1   $AAPL  10752226           -0.464  stocktwits   \n",
       "2    $BAC  10920221            0.445  stocktwits   \n",
       "3   $SHOR  12971398            0.661  stocktwits   \n",
       "4    $JPM  16142438           -0.763  stocktwits   \n",
       "\n",
       "                            spans                      token_spans  \n",
       "0  [Putting on a little $F short]         [put, on, little, short]  \n",
       "1                    [short some]                    [short, some]  \n",
       "2            [buying opportunity]            [buying, opportunity]  \n",
       "3   [Scaling Up on Long Position]  [scale, up, on, long, position]  \n",
       "4        [its time to sell banks]       [it, time, to, sell, bank]  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('on', 4), ('short', 4), ('put', 3), ('up', 3), ('long', 3), ('little', 2), ('buying', 2), ('position', 2), ('time', 2), ('to', 2), ('for', 2), ('opportunity', 1), ('scale', 1), ('sell', 1), ('bank', 1), ('enter', 1), ('picked', 1), ('accumulate', 1), ('far', 1), ('upside', 1), ('downside', 1), ('look', 1), ('strong', 1), ('bounce', 1), ('lunchtime', 1), ('rally', 1), ('come', 1), ('intrigue', 1), ('technology', 1), ('growth', 1), ('potential', 1), ('work', 1), ('big', 1), ('market', 1), ('loser', 1), ('goog', 1), ('googl', 1), ('would', 1), ('suck', 1), ('sbux', 1), ('dip', 1), ('below', 1), ('overbought', 1), ('dont', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.build_wordlist()\n",
    "len(data_test.wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_test= data_test.build_data_model()\n",
    "bow_test.head(5)\n",
    "len(bow_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>to</th>\n",
       "      <th>on</th>\n",
       "      <th>for</th>\n",
       "      <th>stock</th>\n",
       "      <th>up</th>\n",
       "      <th>short</th>\n",
       "      <th>look</th>\n",
       "      <th>buy</th>\n",
       "      <th>...</th>\n",
       "      <th>infect</th>\n",
       "      <th>malware</th>\n",
       "      <th>brings</th>\n",
       "      <th>upscale</th>\n",
       "      <th>concept</th>\n",
       "      <th>ownitdonttradeit</th>\n",
       "      <th>skin</th>\n",
       "      <th>gamepositive</th>\n",
       "      <th>overweight</th>\n",
       "      <th>callput</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>719659409228451840</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>719904304207962112</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5329774</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>719891468173844480</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20091246</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1844 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID  label  to  on  for  stock  up  short  look  buy  \\\n",
       "0  719659409228451840  0.366   0   0    1      0   0      0     0    0   \n",
       "1  719904304207962112  0.638   0   0    0      0   0      0     0    0   \n",
       "2             5329774 -0.494   0   0    0      0   0      0     0    0   \n",
       "3  719891468173844480  0.460   0   0    1      0   0      0     1    0   \n",
       "4            20091246  0.403   0   0    0      0   0      0     0    0   \n",
       "\n",
       "    ...     infect  malware  brings  upscale  concept  ownitdonttradeit  skin  \\\n",
       "0   ...          0        0       0        0        0                 0     0   \n",
       "1   ...          0        0       0        0        0                 0     0   \n",
       "2   ...          0        0       0        0        0                 0     0   \n",
       "3   ...          0        0       0        0        0                 0     0   \n",
       "4   ...          0        0       0        0        0                 0     0   \n",
       "\n",
       "   gamepositive  overweight  callput  \n",
       "0             0           0        0  \n",
       "1             0           0        0  \n",
       "2             0           0        0  \n",
       "3             0           0        0  \n",
       "4             0           0        0  \n",
       "\n",
       "[5 rows x 1844 columns]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>y</th>\n",
       "      <th>to</th>\n",
       "      <th>on</th>\n",
       "      <th>for</th>\n",
       "      <th>stock</th>\n",
       "      <th>up</th>\n",
       "      <th>short</th>\n",
       "      <th>look</th>\n",
       "      <th>buy</th>\n",
       "      <th>...</th>\n",
       "      <th>infect</th>\n",
       "      <th>malware</th>\n",
       "      <th>brings</th>\n",
       "      <th>upscale</th>\n",
       "      <th>concept</th>\n",
       "      <th>ownitdonttradeit</th>\n",
       "      <th>skin</th>\n",
       "      <th>gamepositive</th>\n",
       "      <th>overweight</th>\n",
       "      <th>callput</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>719659409228451840</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>719904304207962112</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5329774</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>719891468173844480</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20091246</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1844 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID      y  to  on  for  stock  up  short  look  buy  \\\n",
       "0  719659409228451840  0.366   0   0    1      0   0      0     0    0   \n",
       "1  719904304207962112  0.638   0   0    0      0   0      0     0    0   \n",
       "2             5329774 -0.494   0   0    0      0   0      0     0    0   \n",
       "3  719891468173844480  0.460   0   0    1      0   0      0     1    0   \n",
       "4            20091246  0.403   0   0    0      0   0      0     0    0   \n",
       "\n",
       "    ...     infect  malware  brings  upscale  concept  ownitdonttradeit  skin  \\\n",
       "0   ...          0        0       0        0        0                 0     0   \n",
       "1   ...          0        0       0        0        0                 0     0   \n",
       "2   ...          0        0       0        0        0                 0     0   \n",
       "3   ...          0        0       0        0        0                 0     0   \n",
       "4   ...          0        0       0        0        0                 0     0   \n",
       "\n",
       "   gamepositive  overweight  callput  \n",
       "0             0           0        0  \n",
       "1             0           0        0  \n",
       "2             0           0        0  \n",
       "3             0           0        0  \n",
       "4             0           0        0  \n",
       "\n",
       "[5 rows x 1844 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow=bow.rename(columns={\"label\":\"y\"})\n",
    "bow.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>on</th>\n",
       "      <th>short</th>\n",
       "      <th>put</th>\n",
       "      <th>up</th>\n",
       "      <th>long</th>\n",
       "      <th>little</th>\n",
       "      <th>buying</th>\n",
       "      <th>position</th>\n",
       "      <th>time</th>\n",
       "      <th>...</th>\n",
       "      <th>loser</th>\n",
       "      <th>goog</th>\n",
       "      <th>googl</th>\n",
       "      <th>would</th>\n",
       "      <th>suck</th>\n",
       "      <th>sbux</th>\n",
       "      <th>dip</th>\n",
       "      <th>below</th>\n",
       "      <th>overbought</th>\n",
       "      <th>dont</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5540055</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10752226</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10920221</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12971398</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16142438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  45 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  on  short  put  up  long  little  buying  position  time  ...   \\\n",
       "0   5540055   1      1    1   0     0       1       0         0     0  ...    \n",
       "1  10752226   0      1    0   0     0       0       0         0     0  ...    \n",
       "2  10920221   0      0    0   0     0       0       1         0     0  ...    \n",
       "3  12971398   1      0    0   1     1       0       0         1     0  ...    \n",
       "4  16142438   0      0    0   0     0       0       0         0     1  ...    \n",
       "\n",
       "   loser  goog  googl  would  suck  sbux  dip  below  overbought  dont  \n",
       "0      0     0      0      0     0     0    0      0           0     0  \n",
       "1      0     0      0      0     0     0    0      0           0     0  \n",
       "2      0     0      0      0     0     0    0      0           0     0  \n",
       "3      0     0      0      0     0     0    0      0           0     0  \n",
       "4      0     0      0      0     0     0    0      0           0     0  \n",
       "\n",
       "[5 rows x 45 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_test.head(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concat the data frames\n",
    "\n",
    "# This is for autocleaner to work properly\n",
    "\n",
    "# Since we have categorical variables we will\n",
    "# need our encoder to label them correctly\n",
    "# so we must use our encoder on the full\n",
    "# dataset to avoid having representation\n",
    "# errors.\n",
    "bow_clean = bow.append(bow_test)\n",
    "bow_clean = autoclean(bow_clean)\n",
    "#bow_clean = bow_clean.rename(columns = {'fit': 'fit_'})\n",
    "\n",
    "train, test = bow_clean[0:len(bow)], bow_clean[len(bow):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>aal</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aapls</th>\n",
       "      <th>absolute</th>\n",
       "      <th>abyss</th>\n",
       "      <th>accept</th>\n",
       "      <th>acct</th>\n",
       "      <th>accumulate</th>\n",
       "      <th>accumulation</th>\n",
       "      <th>...</th>\n",
       "      <th>yoku</th>\n",
       "      <th>zero</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>ziv</th>\n",
       "      <th>zmh</th>\n",
       "      <th>znga</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zuckerberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>719659409228451840</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>719904304207962112</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5329774</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>719891468173844480</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20091246</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1846 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID  aal  aapl  aapls  absolute  abyss  accept  acct  \\\n",
       "0  719659409228451840  0.0   0.0    0.0       0.0    0.0     0.0   0.0   \n",
       "1  719904304207962112  0.0   0.0    0.0       0.0    0.0     0.0   0.0   \n",
       "2             5329774  0.0   0.0    0.0       0.0    0.0     0.0   0.0   \n",
       "3  719891468173844480  0.0   0.0    0.0       0.0    0.0     0.0   0.0   \n",
       "4            20091246  0.0   0.0    0.0       0.0    0.0     0.0   0.0   \n",
       "\n",
       "   accumulate  accumulation     ...      yoku  zero  zigzag  ziv  zmh  znga  \\\n",
       "0           0           0.0     ...       0.0   0.0     0.0  0.0  0.0   0.0   \n",
       "1           0           0.0     ...       0.0   0.0     0.0  0.0  0.0   0.0   \n",
       "2           0           0.0     ...       0.0   0.0     0.0  0.0  0.0   0.0   \n",
       "3           0           0.0     ...       0.0   0.0     0.0  0.0  0.0   0.0   \n",
       "4           0           0.0     ...       0.0   0.0     0.0  0.0  0.0   0.0   \n",
       "\n",
       "   zombie  zone  zuck  zuckerberg  \n",
       "0     0.0   0.0   0.0         0.0  \n",
       "1     0.0   0.0   0.0         0.0  \n",
       "2     0.0   0.0   0.0         0.0  \n",
       "3     0.0   0.0   0.0         0.0  \n",
       "4     0.0   0.0   0.0         0.0  \n",
       "\n",
       "[5 rows x 1846 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>aal</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aapls</th>\n",
       "      <th>absolute</th>\n",
       "      <th>abyss</th>\n",
       "      <th>accept</th>\n",
       "      <th>acct</th>\n",
       "      <th>accumulate</th>\n",
       "      <th>accumulation</th>\n",
       "      <th>...</th>\n",
       "      <th>yoku</th>\n",
       "      <th>zero</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>ziv</th>\n",
       "      <th>zmh</th>\n",
       "      <th>znga</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zuckerberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5540055</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10752226</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10920221</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12971398</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16142438</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  1846 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         ID  aal  aapl  aapls  absolute  abyss  accept  acct  accumulate  \\\n",
       "0   5540055  0.0   0.0    0.0       0.0    0.0     0.0   0.0           0   \n",
       "1  10752226  0.0   0.0    0.0       0.0    0.0     0.0   0.0           0   \n",
       "2  10920221  0.0   0.0    0.0       0.0    0.0     0.0   0.0           0   \n",
       "3  12971398  0.0   0.0    0.0       0.0    0.0     0.0   0.0           0   \n",
       "4  16142438  0.0   0.0    0.0       0.0    0.0     0.0   0.0           0   \n",
       "\n",
       "   accumulation     ...      yoku  zero  zigzag  ziv  zmh  znga  zombie  zone  \\\n",
       "0           0.0     ...       0.0   0.0     0.0  0.0  0.0   0.0     0.0   0.0   \n",
       "1           0.0     ...       0.0   0.0     0.0  0.0  0.0   0.0     0.0   0.0   \n",
       "2           0.0     ...       0.0   0.0     0.0  0.0  0.0   0.0     0.0   0.0   \n",
       "3           0.0     ...       0.0   0.0     0.0  0.0  0.0   0.0     0.0   0.0   \n",
       "4           0.0     ...       0.0   0.0     0.0  0.0  0.0   0.0     0.0   0.0   \n",
       "\n",
       "   zuck  zuckerberg  \n",
       "0   0.0         0.0  \n",
       "1   0.0         0.0  \n",
       "2   0.0         0.0  \n",
       "3   0.0         0.0  \n",
       "4   0.0         0.0  \n",
       "\n",
       "[5 rows x 1846 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Organize our data for training\n",
    "X = train.drop({\"y\",\"ID\"}, axis=1)\n",
    "Y = train[\"y\"]\n",
    "#x_test = test.drop({\"y\",\"ID\"}, axis=1)\n",
    "#X, X_Val, Y, Y_Val = train_test_split(X, Y)\n",
    "#print(type(Y_Val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"X=\",len(X),\"Y=\", len(X_Val),\"X_val=\", len(Y),\"Y_val=\", len(Y_Val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Y_Val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# A parameter grid for XGBoost\\nparams = {'min_child_weight':[4,5], 'gamma':[i/10.0 for i in range(3,6)],  'subsample':[i/10.0 for i in range(6,11)],\\n'colsample_bytree':[i/10.0 for i in range(6,11)], 'max_depth': [2,3,4]}\\n# Initialize XGB and GridSearch\\nxgb = XGBRegressor(nthread=-1) \\n\\ngrid = GridSearchCV(xgb, params)\\ngrid.fit(X, Y)\\n\""
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "# A parameter grid for XGBoost\n",
    "params = {'min_child_weight':[4,5], 'gamma':[i/10.0 for i in range(3,6)],  'subsample':[i/10.0 for i in range(6,11)],\n",
    "'colsample_bytree':[i/10.0 for i in range(6,11)], 'max_depth': [2,3,4]}\n",
    "# Initialize XGB and GridSearch\n",
    "xgb = XGBRegressor(nthread=-1) \n",
    "\n",
    "grid = GridSearchCV(xgb, params)\n",
    "grid.fit(X, Y)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=0.7, gamma=0.3, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=4, min_child_weight=4, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=0.8)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import model_selection, preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#---------------------------XGBregressor---------------------------------------------------\n",
    "\n",
    "#Fitting XGB regressor with parameters obtained by Grid searchCV\n",
    "\"\"\"\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "param_test2 = {\n",
    " 'max_depth':[4,5,6],\n",
    " 'min_child_weight':[4,5,6]\n",
    "}\n",
    "param_test3 = {\n",
    "    'gamma':[i/10.0 for i in range(0,5)],\n",
    "    'n_estimators' : [int(x) for x in np.linspace(start = 200, stop = 1400, num = 7)]\n",
    "}\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "model = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, max_depth=4,\n",
    " min_child_weight=4,  subsample=0.8, colsample_bytree=1.0, nthread=4), \n",
    " param_grid = param_test3,scoring='neg_mean_squared_error',cv=5)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "\"\"\"\n",
    "#model = xgb.XGBRegressor(max_depth=4, min_child_weight= 4,gamma=0.3,subsample=0.8,colsample_bytree=1.0,n_estimators=1000)\n",
    "model=xgb.XGBRegressor(learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the r2 score\n",
    "#print(r2_score(Y_Val, grid.best_estimator_.predict(X_Val))) \n",
    "x_test = test.drop({\"y\",\"ID\"}, axis=1)\n",
    "y_test=test[\"y\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.454197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.254624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.140863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.527454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.233107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.350240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.244789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.487672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.429773</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.203284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.019161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>-0.268752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.012762</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.185847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>-0.496029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>-0.454197</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Prediction\n",
       "0    -0.454197\n",
       "1    -0.254624\n",
       "2     0.140863\n",
       "3     0.527454\n",
       "4    -0.233107\n",
       "5     0.350240\n",
       "6     0.244789\n",
       "7     0.487672\n",
       "8     0.429773\n",
       "9     0.203284\n",
       "10    0.019161\n",
       "11   -0.268752\n",
       "12    0.012762\n",
       "13    0.185847\n",
       "14   -0.496029\n",
       "15   -0.454197"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict \n",
    "output = model.predict(x_test)\n",
    "final_df = pd.DataFrame()\n",
    "#final_df[\"ID\"] = id_vals\n",
    "#final_df[\"cashtag\"]=row2\n",
    "\n",
    "#final_df[\"spans\"]=row\n",
    "final_df[\"Prediction\"] = output\n",
    "#final_df.to_csv(\"Output_1.csv\",sep=\",\")\n",
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the file\n",
    "#y_test = grid.best_estimator_.predict(x_test)\n",
    "#results_df = pd.DataFrame(data={'y':y_test}) \n",
    "#ids = test_df[\"ID\"]\n",
    "#ids=bow_test.index\n",
    "#joined = pd.DataFrame(ids).join(results_df)\n",
    "#joined.to_csv(\"mercedes.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error: 0.17285397295983146\n",
      "R2 score: -0.5217310213164306\n"
     ]
    }
   ],
   "source": [
    "#test model accuracy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "print(\"mean squared error:\" ,mean_squared_error(output, y_test))\n",
    "print(\"R2 score:\" ,r2_score(output,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_io.TextIOWrapper'>\n"
     ]
    }
   ],
   "source": [
    "#load file data into dictionnary \n",
    "with open('Microblog_Trialdata.json') as json_data:\n",
    "    print(type(json_data))\n",
    "    data_dict = json.load(json_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
