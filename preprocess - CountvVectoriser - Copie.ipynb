{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import sparse\n",
    "import time\n",
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize,blankline_tokenize\n",
    "from nltk import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import json\n",
    "import re as regex\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_Initialize():\n",
    "    data = []\n",
    "    processed_data = []\n",
    "    wordlist = []\n",
    "\n",
    "    featureList = []\n",
    "    fea_vect=[]\n",
    "    \n",
    "    data_model = None\n",
    "    data_labels = None\n",
    "    is_testing = False\n",
    "    \n",
    "    def initialize(self, csv_file, is_testing_set=False, from_cached=None):\n",
    "        if from_cached is not None:\n",
    "            #self.data_model = pd.read_csv(from_cached)\n",
    "            self.data_model = pd.read_json(from_cached)\n",
    "\n",
    "            return\n",
    "\n",
    "        self.is_testing = is_testing_set\n",
    "\n",
    "        if not is_testing_set:\n",
    "            #self.data = pd.read_csv(csv_file, header=0, names=[\"id\", \"emotion\", \"text\"])\n",
    "            self.data = pd.read_json(csv_file)\n",
    "\n",
    "            #self.data = self.data[self.data[\"emotion\"].isin([\"positive\", \"negative\", \"neutral\"])]\n",
    "        \n",
    "\n",
    "        self.processed_data = self.data\n",
    "        self.wordlist = []\n",
    "        self.data_model = None\n",
    "        self.data_labels = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def do_process(self):\n",
    "        start_time = time.time()\n",
    "        def stem_and_join(row,stemmer=nltk.PorterStemmer()):\n",
    "            row[\"spans\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"spans\"]))\n",
    "            return row\n",
    "    \n",
    "        def tokenize_grams(row):\n",
    "                \n",
    "                # turn a doc into clean tokens\n",
    "                def clean_doc(doc):\n",
    "                    # split into tokens by white space\n",
    "                    tokens = doc.split()\n",
    "                    # remove punctuation from each token\n",
    "                    table = str.maketrans('', '', string.punctuation)\n",
    "                    tokens = [w.translate(table) for w in tokens]\n",
    "                    # remove remaining tokens that are not alphabetic\n",
    "                    tokens = [word for word in tokens if word.isalpha()]\n",
    "                    # filter out stop words\n",
    "                    #stop_words = set(stopwords.words('english'))\n",
    "                    \n",
    "                    #tokens = [w for w in tokens if not w in stop_words]\n",
    "                    \n",
    "                    \n",
    "                    # filter out short tokens\n",
    "                    tokens = [word.lower() for word in tokens if len(word) > 1]\n",
    "                    return tokens\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "               \n",
    "                # Function to apply lemmatization to a list of words\n",
    "                def words_lemmatizer(words, encoding=\"utf8\"):\n",
    "                    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "                    lemma_words = []\n",
    "                    wl = WordNetLemmatizer()\n",
    "                    for word in words:\n",
    "                        pos = find_pos(word)\n",
    "                        lemma_words.append(wl.lemmatize(word, pos))\n",
    "                    return lemma_words\n",
    "                \n",
    "                # Function to find part of speech tag for a word\n",
    "                def find_pos(word):\n",
    "                    # Part of Speech constants\n",
    "                    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "   \n",
    "                    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "                    # Adjective tags - 'JJ', 'JJR', 'JJS'\n",
    "                    if pos.lower()[0] == 'j':\n",
    "                        return 'a'\n",
    "                    # Adverb tags - 'RB', 'RBR', 'RBS'\n",
    "                    elif pos.lower()[0] == 'r':\n",
    "                        return 'r'\n",
    "                    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "                    elif pos.lower()[0] == 'v':\n",
    "                        return 'v'\n",
    "                    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'\n",
    "                    else:\n",
    "                        return 'n'\n",
    "    \n",
    "            \n",
    "                #convert to string\n",
    "                idx=row[\"spans\"]\n",
    "                #ch=\"\".join(x for x in idx if x)\n",
    "                ch=' '.join(idx)\n",
    "               \n",
    "    \n",
    "                \n",
    "                \n",
    "                n_grams=clean_doc(ch)\n",
    "                tokens=words_lemmatizer(n_grams)\n",
    "\n",
    "                #print(\"nombre of tokens\",len(n_grams))\n",
    "\n",
    "                row[\"token_spans\"] = tokens\n",
    "    \n",
    "                return row\n",
    "        #self.processed_data = self.processed_data.apply(stem_and_join, axis=1)\n",
    "        self.processed_data = self.processed_data.apply(tokenize_grams, axis=1)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    \n",
    "\n",
    "    def build_wordlist(self, min_occurrences=0, max_occurences=500, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                       ):\n",
    "       \n",
    "        self.wordlist = []\n",
    "        whitelist=[\"to\",\"on\",\"for\",\"up\",\"below\",\"short\",\"long\"]\n",
    "        \n",
    "        #stopwords=[]\n",
    "       \n",
    "        words = Counter()\n",
    "      \n",
    "        for idx in self.processed_data.index:\n",
    "            words.update(self.processed_data.loc[idx, \"token_spans\"])\n",
    "\n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "        \n",
    "        \n",
    "        print(words.most_common())        \n",
    "\n",
    "        word_df = pd.DataFrame(data={\"word\": [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                     \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                               columns=[\"word\", \"occurrences\"])\n",
    "\n",
    "        word_df.to_csv(\"wordlist_copie.csv\", index_label=\"idx\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build_data_model(self):\n",
    "        label_column = []\n",
    "        Id_column=[\"ID\"]\n",
    "        label_column = [\"label\"]\n",
    "\n",
    "        columns = Id_column + label_column + list(\n",
    "            map(lambda w: w ,self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "\n",
    "            if True:\n",
    "                # add label\n",
    "                current_label = self.processed_data.loc[idx, \"sentiment score\"]\n",
    "                current_id = self.processed_data.loc[idx, \"id\"]\n",
    "                \n",
    "                labels.append(current_id)\n",
    "                labels.append(current_label)\n",
    "                \n",
    "                current_row.append(current_id)\n",
    "                current_row.append(current_label)\n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"token_spans\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        return self.data_model, self.data_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_Initialize_test(TwitterData_Initialize):\n",
    "\n",
    "    \n",
    "    \n",
    "    def do_process(self):\n",
    "        def stem_and_join(row,stemmer=nltk.PorterStemmer()):\n",
    "            \n",
    "            row[\"spans\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"spans\"]))\n",
    "            return row\n",
    "    \n",
    "        def tokenize_grams(row):\n",
    "                \n",
    "                # Function to remove stop words\n",
    "                def remove_stopwords(text, lang='english'):\n",
    "                    whitelist = [\"n't\",\"not\",\"below\"]    \n",
    "\n",
    "                    stop_words = set(stopwords.words('english'))\n",
    "                    word_tokens = word_tokenize(text)\n",
    "                    #filtered_sentence = [w for w in word_tokens if ((not w in stop_words) or (w in whitelist))]\n",
    "                    filtered_sentence = []\n",
    "                    for w in word_tokens:\n",
    "                        if ((w not in stop_words) or (w in whitelist)):\n",
    "                            filtered_sentence.append(w)\n",
    "                \n",
    "                    ch=\" \".join(filtered_sentence)\n",
    "\n",
    "                    return ch\n",
    "#40\n",
    "\n",
    "                def clean_doc(doc):\n",
    "                    # split into tokens by white space\n",
    "                    tokens = doc.split()\n",
    "                    # remove punctuation from each token\n",
    "                    table = str.maketrans('', '', string.punctuation)\n",
    "                    tokens = [w.translate(table) for w in tokens]\n",
    "                    # remove remaining tokens that are not alphabetic\n",
    "                    tokens = [word for word in tokens if word.isalpha()]\n",
    "                    # filter out stop words\n",
    "                    #stop_words = set(stopwords.words('english'))\n",
    "                    #tokens = [w for w in tokens if not w in stop_words]\n",
    "                    \n",
    "                    # filter out short tokens\n",
    "                    tokens = [word.lower() for word in tokens if len(word) > 1]\n",
    "                    return tokens\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                # Function to apply lemmatization to a list of words\n",
    "                def words_lemmatizer(words, encoding=\"utf8\"):\n",
    "                    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "                    lemma_words = []\n",
    "                    wl = WordNetLemmatizer()\n",
    "                    for word in words:\n",
    "                        pos = find_pos(word)\n",
    "                        lemma_words.append(wl.lemmatize(word, pos))\n",
    "                    return lemma_words\n",
    "\n",
    "                # Function to find part of speech tag for a word\n",
    "                def find_pos(word):\n",
    "                    # Part of Speech constants\n",
    "                    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "   \n",
    "                    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "                    # Adjective tags - 'JJ', 'JJR', 'JJS'\n",
    "                    if pos.lower()[0] == 'j':\n",
    "                        return 'a'\n",
    "                    # Adverb tags - 'RB', 'RBR', 'RBS'\n",
    "                    elif pos.lower()[0] == 'r':\n",
    "                        return 'r'\n",
    "                    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "                    elif pos.lower()[0] == 'v':\n",
    "                        return 'v'\n",
    "                    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'\n",
    "                    else:\n",
    "                        return 'n'\n",
    "    \n",
    "            \n",
    "                token=[]\n",
    "                idx=row[\"spans\"]\n",
    "                ch=' '.join(idx)\n",
    "\n",
    "\n",
    "    \n",
    "                #words_stemmer(ch, type=\"PorterStemmer\", lang=\"english\", encoding=\"utf8\")\n",
    "                #Convert to lower case\n",
    "                n_grams=clean_doc(ch)\n",
    "                tokens=words_lemmatizer(n_grams)\n",
    "\n",
    "                #print(\"list ngrams--------------\\n\",n_grams)\n",
    "                row[\"token_spans\"] = tokens\n",
    "    \n",
    "                return row\n",
    "        #self.processed_data = self.processed_data.apply(stem_and_join, axis=1)\n",
    "        self.processed_data = self.processed_data.apply(tokenize_grams, axis=1)\n",
    "        \n",
    "    def build_wordlist(self, min_occurrences=0, max_occurences=500, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                      ):\n",
    "        \n",
    "        whitelist=[\"to\",\"on\",\"for\",\"up\",\"below\",\"short\",\"long\"]\n",
    "        self.wordlist = []\n",
    "        #stopwords=[]\n",
    "        words = Counter()\n",
    "      \n",
    "        for idx in self.processed_data.index:\n",
    "            words.update(self.processed_data.loc[idx, \"token_spans\"])\n",
    "\n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "        \n",
    "        \n",
    "        print(words.most_common())        \n",
    "\n",
    "        word_df = pd.DataFrame(data={\"word\": [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                     \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                               columns=[\"word\", \"occurrences\"])\n",
    "\n",
    "        word_df.to_csv(\"wordlist_test.csv\", index_label=\"idx\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]\n",
    "        \n",
    "        \n",
    "    def build_data_model(self):\n",
    "        \n",
    "        label_id = [\"ID\"]\n",
    "\n",
    "        columns = label_id + list(\n",
    "            map(lambda w: w ,self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "            if True:\n",
    "                # add label\n",
    "                current_id = self.processed_data.loc[idx, \"id\"]\n",
    "                \n",
    "                labels.append(current_id)\n",
    "                \n",
    "                current_row.append(current_id)\n",
    "\n",
    "            \n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"token_spans\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        return self.data_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$FB</td>\n",
       "      <td>719659409228451840</td>\n",
       "      <td>0.366</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[watching for bounce tomorrow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$LUV</td>\n",
       "      <td>719904304207962112</td>\n",
       "      <td>0.638</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[record number of passengers served in 2015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$NFLX</td>\n",
       "      <td>5329774</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[out $NFLX -.35]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$DIA</td>\n",
       "      <td>719891468173844480</td>\n",
       "      <td>0.460</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Looking for a strong bounce, Lunchtime rally ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$PLUG</td>\n",
       "      <td>20091246</td>\n",
       "      <td>0.403</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Very intrigued with the technology and growth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$GMCR</td>\n",
       "      <td>5819749</td>\n",
       "      <td>0.000</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short worked, puts up]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$IBM</td>\n",
       "      <td>709741154393133056</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[overbought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$JOSB</td>\n",
       "      <td>17892972</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[absolute garbage still up, stores TOTALLY EMP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$CSTM</td>\n",
       "      <td>709834259687710720</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Biggest Market Losers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>$PYPL</td>\n",
       "      <td>708481442079068160</td>\n",
       "      <td>0.408</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Love this company long time.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>$GOOGL</td>\n",
       "      <td>31971935</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[$GOOG $GOOGL would suck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>$ENDP</td>\n",
       "      <td>710187873492983808</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[who won't pay anymore, REAL risk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>$XLI</td>\n",
       "      <td>13915103</td>\n",
       "      <td>0.025</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[No edge offered]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>$PCLN</td>\n",
       "      <td>10448993</td>\n",
       "      <td>0.486</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[runs into the 50sma on the acquisition news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>$AA</td>\n",
       "      <td>24886266</td>\n",
       "      <td>0.308</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[t can't go down]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>12793642</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[now seems like its helping the downtrend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>9408369</td>\n",
       "      <td>0.461</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[mastered their supply chain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>$GOLD</td>\n",
       "      <td>719909604654624768</td>\n",
       "      <td>0.408</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Most bullish stocks on Twitter during this dip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>$AMD</td>\n",
       "      <td>9674584</td>\n",
       "      <td>-0.699</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[big dumping, would not touch it for a while]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>$SPY</td>\n",
       "      <td>10041008</td>\n",
       "      <td>0.495</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[trade continuing very nicely from yesterday, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cashtag                  id  sentiment score      source  \\\n",
       "0      $FB  719659409228451840            0.366     twitter   \n",
       "1     $LUV  719904304207962112            0.638     twitter   \n",
       "2    $NFLX             5329774           -0.494  stocktwits   \n",
       "3     $DIA  719891468173844480            0.460     twitter   \n",
       "4    $PLUG            20091246            0.403  stocktwits   \n",
       "5    $GMCR             5819749            0.000  stocktwits   \n",
       "6     $IBM  709741154393133056           -0.296     twitter   \n",
       "7    $JOSB            17892972           -0.546  stocktwits   \n",
       "8    $CSTM  709834259687710720           -0.438     twitter   \n",
       "9    $PYPL  708481442079068160            0.408     twitter   \n",
       "10  $GOOGL            31971935           -0.398  stocktwits   \n",
       "11   $ENDP  710187873492983808           -0.349     twitter   \n",
       "12    $XLI            13915103            0.025  stocktwits   \n",
       "13   $PCLN            10448993            0.486  stocktwits   \n",
       "14     $AA            24886266            0.308  stocktwits   \n",
       "15   $AAPL            12793642           -0.372  stocktwits   \n",
       "16   $AAPL             9408369            0.461  stocktwits   \n",
       "17   $GOLD  719909604654624768            0.408     twitter   \n",
       "18    $AMD             9674584           -0.699  stocktwits   \n",
       "19    $SPY            10041008            0.495  stocktwits   \n",
       "\n",
       "                                                spans  \n",
       "0                      [watching for bounce tomorrow]  \n",
       "1        [record number of passengers served in 2015]  \n",
       "2                                    [out $NFLX -.35]  \n",
       "3   [Looking for a strong bounce, Lunchtime rally ...  \n",
       "4   [Very intrigued with the technology and growth...  \n",
       "5                             [short worked, puts up]  \n",
       "6                                        [overbought]  \n",
       "7   [absolute garbage still up, stores TOTALLY EMP...  \n",
       "8                             [Biggest Market Losers]  \n",
       "9                      [Love this company long time.]  \n",
       "10                          [$GOOG $GOOGL would suck]  \n",
       "11                 [who won't pay anymore, REAL risk]  \n",
       "12                                  [No edge offered]  \n",
       "13      [runs into the 50sma on the acquisition news]  \n",
       "14                                  [t can't go down]  \n",
       "15         [now seems like its helping the downtrend]  \n",
       "16                      [mastered their supply chain]  \n",
       "17   [Most bullish stocks on Twitter during this dip]  \n",
       "18      [big dumping, would not touch it for a while]  \n",
       "19  [trade continuing very nicely from yesterday, ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TwitterData_Initialize()\n",
    "data.initialize(\"Microblog_Trainingdata.json\")\n",
    "\n",
    "data.processed_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 323.9797320365906 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "      <th>token_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$FB</td>\n",
       "      <td>719659409228451840</td>\n",
       "      <td>0.366</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[watching for bounce tomorrow]</td>\n",
       "      <td>[watch, for, bounce, tomorrow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$LUV</td>\n",
       "      <td>719904304207962112</td>\n",
       "      <td>0.638</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[record number of passengers served in 2015]</td>\n",
       "      <td>[record, number, of, passenger, serve, in]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$NFLX</td>\n",
       "      <td>5329774</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[out $NFLX -.35]</td>\n",
       "      <td>[out, nflx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$DIA</td>\n",
       "      <td>719891468173844480</td>\n",
       "      <td>0.460</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Looking for a strong bounce, Lunchtime rally ...</td>\n",
       "      <td>[look, for, strong, bounce, lunchtime, rally, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$PLUG</td>\n",
       "      <td>20091246</td>\n",
       "      <td>0.403</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Very intrigued with the technology and growth...</td>\n",
       "      <td>[very, intrigue, with, the, technology, and, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$GMCR</td>\n",
       "      <td>5819749</td>\n",
       "      <td>0.000</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short worked, puts up]</td>\n",
       "      <td>[short, work, put, up]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$IBM</td>\n",
       "      <td>709741154393133056</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[overbought]</td>\n",
       "      <td>[overbought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$JOSB</td>\n",
       "      <td>17892972</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[absolute garbage still up, stores TOTALLY EMP...</td>\n",
       "      <td>[absolute, garbage, still, up, store, totally,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$CSTM</td>\n",
       "      <td>709834259687710720</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Biggest Market Losers]</td>\n",
       "      <td>[big, market, loser]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>$PYPL</td>\n",
       "      <td>708481442079068160</td>\n",
       "      <td>0.408</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Love this company long time.]</td>\n",
       "      <td>[love, this, company, long, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>$GOOGL</td>\n",
       "      <td>31971935</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[$GOOG $GOOGL would suck]</td>\n",
       "      <td>[goog, googl, would, suck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>$ENDP</td>\n",
       "      <td>710187873492983808</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[who won't pay anymore, REAL risk]</td>\n",
       "      <td>[who, wont, pay, anymore, real, risk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>$XLI</td>\n",
       "      <td>13915103</td>\n",
       "      <td>0.025</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[No edge offered]</td>\n",
       "      <td>[no, edge, offer]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>$PCLN</td>\n",
       "      <td>10448993</td>\n",
       "      <td>0.486</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[runs into the 50sma on the acquisition news]</td>\n",
       "      <td>[run, into, the, on, the, acquisition, news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>$AA</td>\n",
       "      <td>24886266</td>\n",
       "      <td>0.308</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[t can't go down]</td>\n",
       "      <td>[cant, go, down]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>12793642</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[now seems like its helping the downtrend]</td>\n",
       "      <td>[now, seem, like, it, help, the, downtrend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>9408369</td>\n",
       "      <td>0.461</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[mastered their supply chain]</td>\n",
       "      <td>[master, their, supply, chain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>$GOLD</td>\n",
       "      <td>719909604654624768</td>\n",
       "      <td>0.408</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Most bullish stocks on Twitter during this dip]</td>\n",
       "      <td>[most, bullish, stock, on, twitter, during, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>$AMD</td>\n",
       "      <td>9674584</td>\n",
       "      <td>-0.699</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[big dumping, would not touch it for a while]</td>\n",
       "      <td>[big, dump, would, not, touch, it, for, while]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>$SPY</td>\n",
       "      <td>10041008</td>\n",
       "      <td>0.495</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[trade continuing very nicely from yesterday, ...</td>\n",
       "      <td>[trade, continue, very, nicely, from, yesterda...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>$IDRA</td>\n",
       "      <td>20249627</td>\n",
       "      <td>0.306</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[in from 3.61]</td>\n",
       "      <td>[in, from]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>$ASMB</td>\n",
       "      <td>39047349</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Stochastic Overbought]</td>\n",
       "      <td>[stochastic, overbought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>$RHT</td>\n",
       "      <td>711907079296933888</td>\n",
       "      <td>0.336</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Unusual call buying]</td>\n",
       "      <td>[unusual, call, buying]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>$CHK</td>\n",
       "      <td>5364581</td>\n",
       "      <td>0.279</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[reserves are in decline]</td>\n",
       "      <td>[reserve, be, in, decline]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>$CELG</td>\n",
       "      <td>25816362</td>\n",
       "      <td>0.591</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[all on a longer-term, technical swing long ba...</td>\n",
       "      <td>[all, on, longerterm, technical, swing, long, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>$CBPO</td>\n",
       "      <td>708960045799759872</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Insiders Are Selling]</td>\n",
       "      <td>[insider, be, sell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>$TSLA</td>\n",
       "      <td>719521470175522816</td>\n",
       "      <td>-0.514</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[What goes up...]</td>\n",
       "      <td>[what, go, up]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>$TSLA</td>\n",
       "      <td>719572690453934080</td>\n",
       "      <td>-0.519</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[if $249.84 breaks we see $245 then $240]</td>\n",
       "      <td>[if, break, we, see, then]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>$BBRY</td>\n",
       "      <td>33458939</td>\n",
       "      <td>0.361</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[let's see a big bounce]</td>\n",
       "      <td>[let, see, big, bounce]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>$FB</td>\n",
       "      <td>719547428924481536</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[I don't think it will move $FB in the short t...</td>\n",
       "      <td>[dont, think, it, will, move, fb, in, the, sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1670</th>\n",
       "      <td>$HDGE</td>\n",
       "      <td>39047349</td>\n",
       "      <td>-0.385</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Stochastic Overbought]</td>\n",
       "      <td>[stochastic, overbought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1671</th>\n",
       "      <td>$XBI</td>\n",
       "      <td>6783339</td>\n",
       "      <td>0.281</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Good to load up Be confident]</td>\n",
       "      <td>[good, to, load, up, be, confident]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1672</th>\n",
       "      <td>$RIG</td>\n",
       "      <td>5679790</td>\n",
       "      <td>-0.720</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Worst performers today,  $RIG -13%]</td>\n",
       "      <td>[bad, performer, today, rig]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1673</th>\n",
       "      <td>$HPCQ</td>\n",
       "      <td>708960045799759872</td>\n",
       "      <td>-0.351</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Insiders Are Selling]</td>\n",
       "      <td>[insider, be, sell]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1674</th>\n",
       "      <td>$FB</td>\n",
       "      <td>719849703106551808</td>\n",
       "      <td>0.365</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Pie In The Sky]</td>\n",
       "      <td>[pie, in, the, sky]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1675</th>\n",
       "      <td>$IWM</td>\n",
       "      <td>22957112</td>\n",
       "      <td>-0.382</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Small caps threatening descending triangle br...</td>\n",
       "      <td>[small, cap, threaten, descend, triangle, brea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1676</th>\n",
       "      <td>$AMZN</td>\n",
       "      <td>719894972636512256</td>\n",
       "      <td>-0.186</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Amazon has been selling surveillance cameras ...</td>\n",
       "      <td>[amazon, have, be, sell, surveillance, camera,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1677</th>\n",
       "      <td>$HTZ</td>\n",
       "      <td>719507659741872128</td>\n",
       "      <td>-0.409</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[$HTZ, lower]</td>\n",
       "      <td>[htz, low]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1678</th>\n",
       "      <td>$SPY</td>\n",
       "      <td>5612699</td>\n",
       "      <td>0.412</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Don't hang your bull hat yet, Next week we wi...</td>\n",
       "      <td>[dont, hang, your, bull, hat, yet, next, week,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1679</th>\n",
       "      <td>$GOOGL</td>\n",
       "      <td>719560961879728128</td>\n",
       "      <td>0.465</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Alphabet Inc was just upgraded to buy]</td>\n",
       "      <td>[alphabet, inc, be, just, upgraded, to, buy]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1680</th>\n",
       "      <td>$MU</td>\n",
       "      <td>719585607215808512</td>\n",
       "      <td>0.362</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Today I bought more]</td>\n",
       "      <td>[today, bought, more]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1681</th>\n",
       "      <td>$SBUX</td>\n",
       "      <td>709056556466200576</td>\n",
       "      <td>0.220</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[brings upscale concept]</td>\n",
       "      <td>[brings, upscale, concept]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1682</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>710143603524427776</td>\n",
       "      <td>0.365</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[looking for a sharp move up in equities]</td>\n",
       "      <td>[look, for, sharp, move, up, in, equity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1683</th>\n",
       "      <td>$AMRN</td>\n",
       "      <td>11845293</td>\n",
       "      <td>-0.248</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[I'm betting over 52%]</td>\n",
       "      <td>[im, bet, over]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1684</th>\n",
       "      <td>$DUST</td>\n",
       "      <td>17553855</td>\n",
       "      <td>-0.152</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[careful can reverse fast]</td>\n",
       "      <td>[careful, can, reverse, fast]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1685</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>719886818800463872</td>\n",
       "      <td>0.435</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[#OwnItDon'tTradeIt]</td>\n",
       "      <td>[ownitdonttradeit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1686</th>\n",
       "      <td>$PSXP</td>\n",
       "      <td>719732320404709376</td>\n",
       "      <td>0.414</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[TOP 5 STOCK PICKS]</td>\n",
       "      <td>[top, stock, pick]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1687</th>\n",
       "      <td>$OPK</td>\n",
       "      <td>5792305</td>\n",
       "      <td>0.471</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Major skin in the game...positive.]</td>\n",
       "      <td>[major, skin, in, the, gamepositive]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1688</th>\n",
       "      <td>$TVIX</td>\n",
       "      <td>30158448</td>\n",
       "      <td>-0.589</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[100.0% increased bearish conversations]</td>\n",
       "      <td>[increase, bearish, conversation]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1689</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>719489192078610432</td>\n",
       "      <td>-0.513</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Another Sell Rating, Sell Rating]</td>\n",
       "      <td>[another, sell, rating, sell, rating]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1690</th>\n",
       "      <td>$FOLD</td>\n",
       "      <td>11118210</td>\n",
       "      <td>-0.581</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[sold today]</td>\n",
       "      <td>[sell, today]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1691</th>\n",
       "      <td>$FB</td>\n",
       "      <td>719536774821867520</td>\n",
       "      <td>0.087</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Buy Call!]</td>\n",
       "      <td>[buy, call]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1692</th>\n",
       "      <td>$TSLA</td>\n",
       "      <td>719535349379284992</td>\n",
       "      <td>0.230</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Long, buying point]</td>\n",
       "      <td>[long, buying, point]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1693</th>\n",
       "      <td>$ETN</td>\n",
       "      <td>15658194</td>\n",
       "      <td>0.813</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[UPGRADE today by MS to overweight, Excellent ...</td>\n",
       "      <td>[upgrade, today, by, m, to, overweight, excell...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1694</th>\n",
       "      <td>$OMER</td>\n",
       "      <td>22409313</td>\n",
       "      <td>0.380</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Buy the dip']</td>\n",
       "      <td>[buy, the, dip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1695</th>\n",
       "      <td>$RIMM</td>\n",
       "      <td>7442585</td>\n",
       "      <td>-0.126</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[So both call/put buyers are crushed]</td>\n",
       "      <td>[so, both, callput, buyer, be, crush]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>$XOM</td>\n",
       "      <td>5430926</td>\n",
       "      <td>0.295</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Buy stop above 80]</td>\n",
       "      <td>[buy, stop, above]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1697</th>\n",
       "      <td>$HOT</td>\n",
       "      <td>719547552874512384</td>\n",
       "      <td>0.405</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Airplane And Hospitality Industries Set Their...</td>\n",
       "      <td>[airplane, and, hospitality, industry, set, th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1698</th>\n",
       "      <td>$BBRY</td>\n",
       "      <td>18346099</td>\n",
       "      <td>0.296</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[nice bounce]</td>\n",
       "      <td>[nice, bounce]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1699</th>\n",
       "      <td>$AXP</td>\n",
       "      <td>709741154393133056</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[overbought]</td>\n",
       "      <td>[overbought]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1700 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     cashtag                  id  sentiment score      source  \\\n",
       "0        $FB  719659409228451840            0.366     twitter   \n",
       "1       $LUV  719904304207962112            0.638     twitter   \n",
       "2      $NFLX             5329774           -0.494  stocktwits   \n",
       "3       $DIA  719891468173844480            0.460     twitter   \n",
       "4      $PLUG            20091246            0.403  stocktwits   \n",
       "5      $GMCR             5819749            0.000  stocktwits   \n",
       "6       $IBM  709741154393133056           -0.296     twitter   \n",
       "7      $JOSB            17892972           -0.546  stocktwits   \n",
       "8      $CSTM  709834259687710720           -0.438     twitter   \n",
       "9      $PYPL  708481442079068160            0.408     twitter   \n",
       "10    $GOOGL            31971935           -0.398  stocktwits   \n",
       "11     $ENDP  710187873492983808           -0.349     twitter   \n",
       "12      $XLI            13915103            0.025  stocktwits   \n",
       "13     $PCLN            10448993            0.486  stocktwits   \n",
       "14       $AA            24886266            0.308  stocktwits   \n",
       "15     $AAPL            12793642           -0.372  stocktwits   \n",
       "16     $AAPL             9408369            0.461  stocktwits   \n",
       "17     $GOLD  719909604654624768            0.408     twitter   \n",
       "18      $AMD             9674584           -0.699  stocktwits   \n",
       "19      $SPY            10041008            0.495  stocktwits   \n",
       "20     $IDRA            20249627            0.306  stocktwits   \n",
       "21     $ASMB            39047349           -0.385  stocktwits   \n",
       "22      $RHT  711907079296933888            0.336     twitter   \n",
       "23      $CHK             5364581            0.279  stocktwits   \n",
       "24     $CELG            25816362            0.591  stocktwits   \n",
       "25     $CBPO  708960045799759872           -0.351     twitter   \n",
       "26     $TSLA  719521470175522816           -0.514     twitter   \n",
       "27     $TSLA  719572690453934080           -0.519     twitter   \n",
       "28     $BBRY            33458939            0.361  stocktwits   \n",
       "29       $FB  719547428924481536           -0.042     twitter   \n",
       "...      ...                 ...              ...         ...   \n",
       "1670   $HDGE            39047349           -0.385  stocktwits   \n",
       "1671    $XBI             6783339            0.281  stocktwits   \n",
       "1672    $RIG             5679790           -0.720  stocktwits   \n",
       "1673   $HPCQ  708960045799759872           -0.351     twitter   \n",
       "1674     $FB  719849703106551808            0.365     twitter   \n",
       "1675    $IWM            22957112           -0.382  stocktwits   \n",
       "1676   $AMZN  719894972636512256           -0.186     twitter   \n",
       "1677    $HTZ  719507659741872128           -0.409     twitter   \n",
       "1678    $SPY             5612699            0.412  stocktwits   \n",
       "1679  $GOOGL  719560961879728128            0.465     twitter   \n",
       "1680     $MU  719585607215808512            0.362     twitter   \n",
       "1681   $SBUX  709056556466200576            0.220     twitter   \n",
       "1682   $AAPL  710143603524427776            0.365     twitter   \n",
       "1683   $AMRN            11845293           -0.248  stocktwits   \n",
       "1684   $DUST            17553855           -0.152  stocktwits   \n",
       "1685   $AAPL  719886818800463872            0.435     twitter   \n",
       "1686   $PSXP  719732320404709376            0.414     twitter   \n",
       "1687    $OPK             5792305            0.471  stocktwits   \n",
       "1688   $TVIX            30158448           -0.589  stocktwits   \n",
       "1689   $AAPL  719489192078610432           -0.513     twitter   \n",
       "1690   $FOLD            11118210           -0.581  stocktwits   \n",
       "1691     $FB  719536774821867520            0.087     twitter   \n",
       "1692   $TSLA  719535349379284992            0.230     twitter   \n",
       "1693    $ETN            15658194            0.813  stocktwits   \n",
       "1694   $OMER            22409313            0.380  stocktwits   \n",
       "1695   $RIMM             7442585           -0.126  stocktwits   \n",
       "1696    $XOM             5430926            0.295  stocktwits   \n",
       "1697    $HOT  719547552874512384            0.405     twitter   \n",
       "1698   $BBRY            18346099            0.296  stocktwits   \n",
       "1699    $AXP  709741154393133056           -0.296     twitter   \n",
       "\n",
       "                                                  spans  \\\n",
       "0                        [watching for bounce tomorrow]   \n",
       "1          [record number of passengers served in 2015]   \n",
       "2                                      [out $NFLX -.35]   \n",
       "3     [Looking for a strong bounce, Lunchtime rally ...   \n",
       "4     [Very intrigued with the technology and growth...   \n",
       "5                               [short worked, puts up]   \n",
       "6                                          [overbought]   \n",
       "7     [absolute garbage still up, stores TOTALLY EMP...   \n",
       "8                               [Biggest Market Losers]   \n",
       "9                        [Love this company long time.]   \n",
       "10                            [$GOOG $GOOGL would suck]   \n",
       "11                   [who won't pay anymore, REAL risk]   \n",
       "12                                    [No edge offered]   \n",
       "13        [runs into the 50sma on the acquisition news]   \n",
       "14                                    [t can't go down]   \n",
       "15           [now seems like its helping the downtrend]   \n",
       "16                        [mastered their supply chain]   \n",
       "17     [Most bullish stocks on Twitter during this dip]   \n",
       "18        [big dumping, would not touch it for a while]   \n",
       "19    [trade continuing very nicely from yesterday, ...   \n",
       "20                                       [in from 3.61]   \n",
       "21                              [Stochastic Overbought]   \n",
       "22                                [Unusual call buying]   \n",
       "23                            [reserves are in decline]   \n",
       "24    [all on a longer-term, technical swing long ba...   \n",
       "25                               [Insiders Are Selling]   \n",
       "26                                    [What goes up...]   \n",
       "27            [if $249.84 breaks we see $245 then $240]   \n",
       "28                             [let's see a big bounce]   \n",
       "29    [I don't think it will move $FB in the short t...   \n",
       "...                                                 ...   \n",
       "1670                            [Stochastic Overbought]   \n",
       "1671                     [Good to load up Be confident]   \n",
       "1672               [Worst performers today,  $RIG -13%]   \n",
       "1673                             [Insiders Are Selling]   \n",
       "1674                                   [Pie In The Sky]   \n",
       "1675  [Small caps threatening descending triangle br...   \n",
       "1676  [Amazon has been selling surveillance cameras ...   \n",
       "1677                                      [$HTZ, lower]   \n",
       "1678  [Don't hang your bull hat yet, Next week we wi...   \n",
       "1679            [Alphabet Inc was just upgraded to buy]   \n",
       "1680                              [Today I bought more]   \n",
       "1681                           [brings upscale concept]   \n",
       "1682          [looking for a sharp move up in equities]   \n",
       "1683                             [I'm betting over 52%]   \n",
       "1684                         [careful can reverse fast]   \n",
       "1685                               [#OwnItDon'tTradeIt]   \n",
       "1686                                [TOP 5 STOCK PICKS]   \n",
       "1687               [Major skin in the game...positive.]   \n",
       "1688           [100.0% increased bearish conversations]   \n",
       "1689                 [Another Sell Rating, Sell Rating]   \n",
       "1690                                       [sold today]   \n",
       "1691                                        [Buy Call!]   \n",
       "1692                               [Long, buying point]   \n",
       "1693  [UPGRADE today by MS to overweight, Excellent ...   \n",
       "1694                                     [Buy the dip']   \n",
       "1695              [So both call/put buyers are crushed]   \n",
       "1696                                [Buy stop above 80]   \n",
       "1697  [Airplane And Hospitality Industries Set Their...   \n",
       "1698                                      [nice bounce]   \n",
       "1699                                       [overbought]   \n",
       "\n",
       "                                            token_spans  \n",
       "0                        [watch, for, bounce, tomorrow]  \n",
       "1            [record, number, of, passenger, serve, in]  \n",
       "2                                           [out, nflx]  \n",
       "3     [look, for, strong, bounce, lunchtime, rally, ...  \n",
       "4     [very, intrigue, with, the, technology, and, g...  \n",
       "5                                [short, work, put, up]  \n",
       "6                                          [overbought]  \n",
       "7     [absolute, garbage, still, up, store, totally,...  \n",
       "8                                  [big, market, loser]  \n",
       "9                     [love, this, company, long, time]  \n",
       "10                           [goog, googl, would, suck]  \n",
       "11                [who, wont, pay, anymore, real, risk]  \n",
       "12                                    [no, edge, offer]  \n",
       "13         [run, into, the, on, the, acquisition, news]  \n",
       "14                                     [cant, go, down]  \n",
       "15          [now, seem, like, it, help, the, downtrend]  \n",
       "16                       [master, their, supply, chain]  \n",
       "17    [most, bullish, stock, on, twitter, during, th...  \n",
       "18       [big, dump, would, not, touch, it, for, while]  \n",
       "19    [trade, continue, very, nicely, from, yesterda...  \n",
       "20                                           [in, from]  \n",
       "21                             [stochastic, overbought]  \n",
       "22                              [unusual, call, buying]  \n",
       "23                           [reserve, be, in, decline]  \n",
       "24    [all, on, longerterm, technical, swing, long, ...  \n",
       "25                                  [insider, be, sell]  \n",
       "26                                       [what, go, up]  \n",
       "27                           [if, break, we, see, then]  \n",
       "28                              [let, see, big, bounce]  \n",
       "29    [dont, think, it, will, move, fb, in, the, sho...  \n",
       "...                                                 ...  \n",
       "1670                           [stochastic, overbought]  \n",
       "1671                [good, to, load, up, be, confident]  \n",
       "1672                       [bad, performer, today, rig]  \n",
       "1673                                [insider, be, sell]  \n",
       "1674                                [pie, in, the, sky]  \n",
       "1675  [small, cap, threaten, descend, triangle, brea...  \n",
       "1676  [amazon, have, be, sell, surveillance, camera,...  \n",
       "1677                                         [htz, low]  \n",
       "1678  [dont, hang, your, bull, hat, yet, next, week,...  \n",
       "1679       [alphabet, inc, be, just, upgraded, to, buy]  \n",
       "1680                              [today, bought, more]  \n",
       "1681                         [brings, upscale, concept]  \n",
       "1682           [look, for, sharp, move, up, in, equity]  \n",
       "1683                                    [im, bet, over]  \n",
       "1684                      [careful, can, reverse, fast]  \n",
       "1685                                 [ownitdonttradeit]  \n",
       "1686                                 [top, stock, pick]  \n",
       "1687               [major, skin, in, the, gamepositive]  \n",
       "1688                  [increase, bearish, conversation]  \n",
       "1689              [another, sell, rating, sell, rating]  \n",
       "1690                                      [sell, today]  \n",
       "1691                                        [buy, call]  \n",
       "1692                              [long, buying, point]  \n",
       "1693  [upgrade, today, by, m, to, overweight, excell...  \n",
       "1694                                    [buy, the, dip]  \n",
       "1695              [so, both, callput, buyer, be, crush]  \n",
       "1696                                 [buy, stop, above]  \n",
       "1697  [airplane, and, hospitality, industry, set, th...  \n",
       "1698                                     [nice, bounce]  \n",
       "1699                                       [overbought]  \n",
       "\n",
       "[1700 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.do_process()\n",
    "data.processed_data.head()\n",
    "data.processed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def get_tweets(data):\n",
    "    start_time = time.time()\n",
    "\n",
    "    tweets=list(data.processed_data.token_spans)\n",
    "    \n",
    "    list_tweet=[]\n",
    "    for i,chaine in enumerate(tweets):\n",
    "        ch=\" \".join(chaine)\n",
    "        list_tweet.append(ch)\n",
    "    print(type(list_tweet))\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    return list_tweet\n",
    "\n",
    "def get_count_vector(corpus):\n",
    "    vect = CountVectorizer()#appel à l'objet counvectorizer\n",
    "    vect.fit(corpus)#construction du vecteur de mots\n",
    "    #print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "    #print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\n",
    "    vect.get_stop_words()\n",
    "    X = vect.transform(corpus)\n",
    "    #print(\"bag_of_words: {}\".format(repr(X)))\n",
    "    #print(\"Dense representation of bag_of_words:\\n{}\".format(\n",
    "    #X.toarray()))\n",
    "    return vect,X\n",
    "   \n",
    "    \n",
    "#sparse_matrix = sparse.csr_matrix(bag_of_words)\n",
    "#print(\"\\nSciPy sparse CSR matrix:\\n{}\".format(sparse_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.366\n",
       "1       0.638\n",
       "2      -0.494\n",
       "3       0.460\n",
       "4       0.403\n",
       "5       0.000\n",
       "6      -0.296\n",
       "7      -0.546\n",
       "8      -0.438\n",
       "9       0.408\n",
       "10     -0.398\n",
       "11     -0.349\n",
       "12      0.025\n",
       "13      0.486\n",
       "14      0.308\n",
       "15     -0.372\n",
       "16      0.461\n",
       "17      0.408\n",
       "18     -0.699\n",
       "19      0.495\n",
       "20      0.306\n",
       "21     -0.385\n",
       "22      0.336\n",
       "23      0.279\n",
       "24      0.591\n",
       "25     -0.351\n",
       "26     -0.514\n",
       "27     -0.519\n",
       "28      0.361\n",
       "29     -0.042\n",
       "        ...  \n",
       "1670   -0.385\n",
       "1671    0.281\n",
       "1672   -0.720\n",
       "1673   -0.351\n",
       "1674    0.365\n",
       "1675   -0.382\n",
       "1676   -0.186\n",
       "1677   -0.409\n",
       "1678    0.412\n",
       "1679    0.465\n",
       "1680    0.362\n",
       "1681    0.220\n",
       "1682    0.365\n",
       "1683   -0.248\n",
       "1684   -0.152\n",
       "1685    0.435\n",
       "1686    0.414\n",
       "1687    0.471\n",
       "1688   -0.589\n",
       "1689   -0.513\n",
       "1690   -0.581\n",
       "1691    0.087\n",
       "1692    0.230\n",
       "1693    0.813\n",
       "1694    0.380\n",
       "1695   -0.126\n",
       "1696    0.295\n",
       "1697    0.405\n",
       "1698    0.296\n",
       "1699   -0.296\n",
       "Name: sentiment score, Length: 1700, dtype: float64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cols = ['sentiment score']  + [col for col in data.processed_data if col != 'sentiment score']\n",
    "data.processed_data=data.processed_data[cols]\n",
    "data.processed_data.iloc[:,0:]\n",
    "data.processed_data.iloc[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "--- 0.0009953975677490234 seconds ---\n"
     ]
    }
   ],
   "source": [
    "\"\"\"train_data, test_data, y_train, y_test = train_test_split(data.processed_data.iloc[:,1:], data.processed_data.iloc[:,0],\n",
    "                                                   train_size=0.7)\n",
    "corpus=get_tweets(train_data)\n",
    "len(corpus)\"\"\"\n",
    "corpus=get_tweets(data)\n",
    "with open(\"tweets.txt\",\"w\") as fichier:\n",
    "    fichier.writelines(\"\\n\".join(corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1921\n",
      "  (0, 203)\t1\n",
      "  (0, 653)\t1\n",
      "  (0, 1718)\t1\n",
      "  (0, 1832)\t1\n",
      "be 289\n",
      "the 249\n",
      "to 235\n",
      "in 151\n",
      "on 133\n",
      "for 128\n",
      "stock 126\n",
      "of 110\n",
      "up 109\n",
      "short 95\n",
      "look 83\n",
      "and 80\n",
      "it 76\n",
      "buy 76\n",
      "long 74\n",
      "this 73\n",
      "today 68\n",
      "sell 66\n",
      "high 66\n",
      "call 59\n"
     ]
    }
   ],
   "source": [
    "def get_top_n_words(corpus, n=None):\n",
    "    \"\"\"\n",
    "    List the top n words in a vocabulary according to occurrence in a text corpus.\n",
    "    \n",
    "    get_top_n_words([\"I love Python\", \"Python is a language programming\", \"Hello world\", \"I love the world\"]) -> \n",
    "    [('python', 2),\n",
    "     ('world', 2),\n",
    "     ('love', 2),\n",
    "     ('hello', 1),\n",
    "     ('is', 1),\n",
    "     ('programming', 1),\n",
    "     ('the', 1),\n",
    "     ('language', 1)]\n",
    "    \"\"\"\n",
    "    vec = CountVectorizer().fit(corpus)\n",
    "    bag_of_words = vec.transform(corpus)\n",
    "    print(len(vec.get_feature_names()))\n",
    "    print(bag_of_words[0])\n",
    "\n",
    "    sum_words = bag_of_words.sum(axis=0) \n",
    "    words_freq = [(word, sum_words[0, idx]) for word, idx in     vec.vocabulary_.items()]\n",
    "    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "    return words_freq[:n]\n",
    "\n",
    "\n",
    "\n",
    "common_words = get_top_n_words(corpus, 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "--- 0.0009984970092773438 seconds ---\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "corpus=get_tweets(data)\n",
    "lst=get_top_n_words(corpus)\n",
    "with open(\"top_words.csv\", \"w\",newline='') as fichier:\n",
    "    c = csv.writer(fichier,delimiter=\",\")\n",
    "\n",
    "    for l in lst:\n",
    "        c.writerow(l)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = TwitterData_Initialize_test()\n",
    "data_test.initialize(\"Microblog_Trialdata.json\")\n",
    "\n",
    "data_test.processed_data.head()\n",
    "data_test.do_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_vec,X=get_count_vector(corpus)\n",
    "labels=data.processed_data[\"sentiment score\"].values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "words=list(train_vec.get_feature_names())\n",
    "X_train=pd.DataFrame(X.toarray(),columns=words)\n",
    "y_train=pd.DataFrame(labels,columns=['label'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=0.8, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=5, min_child_weight=1, missing=None, n_estimators=1000,\n",
      "       n_jobs=1, nthread=4, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=27, silent=True,\n",
      "       subsample=0.8)\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "from sklearn import model_selection, preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "#---------------------------XGBregressor---------------------------------------------------\n",
    "\n",
    "#Fitting XGB regressor with parameters obtained by Grid searchCV\n",
    "\"\"\"\n",
    "param_test1 = {\n",
    " 'max_depth':range(3,10,2),\n",
    " 'min_child_weight':range(1,6,2)\n",
    "}\n",
    "param_test2 = {\n",
    " 'max_depth':[4,5,6],\n",
    " 'min_child_weight':[4,5,6]\n",
    "}\n",
    "param_test3 = {\n",
    "    'gamma':[i/10.0 for i in range(0,5)],\n",
    "    'n_estimators' : [int(x) for x in np.linspace(start = 200, stop = 1400, num = 7)]\n",
    "}\n",
    "\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "model = GridSearchCV(estimator = XGBRegressor( learning_rate =0.1, max_depth=4,\n",
    " min_child_weight=4,  subsample=0.8, colsample_bytree=1.0, nthread=4), \n",
    " param_grid = param_test3,scoring='neg_mean_squared_error',cv=5)\n",
    "model.fit(X_train,y_train)\n",
    "\n",
    "\"\"\"\n",
    "#model = xgb.XGBRegressor(max_depth=4, min_child_weight= 4,gamma=0.3,subsample=0.8,colsample_bytree=1.0,n_estimators=1000)\n",
    "XGBR=xgb.XGBRegressor(learning_rate =0.1,\n",
    " n_estimators=1000,\n",
    " max_depth=5,\n",
    " min_child_weight=1,\n",
    " gamma=0,\n",
    " subsample=0.8,\n",
    " colsample_bytree=0.8,\n",
    " nthread=4,\n",
    " scale_pos_weight=1,\n",
    " seed=27)\n",
    "XGBR.fit(X_train,y_train)\n",
    "\n",
    "print (XGBR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "--- 0.0 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['put on little short',\n",
       " 'short some',\n",
       " 'buying opportunity',\n",
       " 'scale up on long position',\n",
       " 'it time to sell bank',\n",
       " 'enter long',\n",
       " 'picked some up',\n",
       " 'time to accumulate for long position far more upside than downside',\n",
       " 'look for strong bounce lunchtime rally come',\n",
       " 'very intrigue with the technology and growth potential',\n",
       " 'work put up',\n",
       " 'big market loser',\n",
       " 'goog googl would suck',\n",
       " 'buying sbux on dip',\n",
       " 'be short below and be overbought',\n",
       " 'dont put on down little short']"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_corpus=get_tweets(data_test)\n",
    "test_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "on 4\n",
      "short 4\n",
      "put 3\n",
      "up 3\n",
      "long 3\n",
      "little 2\n",
      "some 2\n",
      "buying 2\n",
      "position 2\n",
      "time 2\n",
      "to 2\n",
      "for 2\n",
      "and 2\n",
      "be 2\n",
      "opportunity 1\n",
      "scale 1\n",
      "it 1\n",
      "sell 1\n",
      "bank 1\n",
      "enter 1\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "common_words = get_top_n_words(test_corpus, 20)\n",
    "for word, freq in common_words:\n",
    "    print(word, freq)\n",
    "print(len(test_corpus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_vec,X_=get_count_vector(test_corpus)\n",
    "labels_=data_test.processed_data[\"sentiment score\"].values\n",
    "test_words=list(test_vec.get_feature_names())\n",
    "X_test=pd.DataFrame(X_.toarray(),columns=test_words)\n",
    "y_test=pd.DataFrame(labels_,columns=['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "bow_columns=list(X_train.columns)\n",
    "for word in bow_columns:\n",
    "    current_row=[]\n",
    "    if (not word  in test_words):\n",
    "        # add label\n",
    "        for idx in X_test.index:\n",
    "            current_row.append(0)\n",
    "        X_test[word]=current_row\n",
    "X_test=X_test[X_train.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>old_pred</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Putting on a little $F short]</td>\n",
       "      <td>[-0.454]</td>\n",
       "      <td>-0.774885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[short some]</td>\n",
       "      <td>[-0.464]</td>\n",
       "      <td>-0.335771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[buying opportunity]</td>\n",
       "      <td>[0.445]</td>\n",
       "      <td>0.415726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Scaling Up on Long Position]</td>\n",
       "      <td>[0.661]</td>\n",
       "      <td>0.692813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[its time to sell banks]</td>\n",
       "      <td>[-0.763]</td>\n",
       "      <td>-0.521347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Entering long]</td>\n",
       "      <td>[0.627]</td>\n",
       "      <td>0.470386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[picked some up]</td>\n",
       "      <td>[0.653]</td>\n",
       "      <td>0.397556</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[time to accumulate for a long position, far m...</td>\n",
       "      <td>[0.668]</td>\n",
       "      <td>0.617554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Looking for a strong bounce, Lunchtime rally ...</td>\n",
       "      <td>[0.46]</td>\n",
       "      <td>0.493107</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Very intrigued with the technology and growth...</td>\n",
       "      <td>[0.403]</td>\n",
       "      <td>0.393143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[short2 48 + - ***worked, puts up]</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>0.080027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[Biggest Market Losers]</td>\n",
       "      <td>[-0.438]</td>\n",
       "      <td>-0.423175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[$GOOG $GOOGL would suck]</td>\n",
       "      <td>[-0.398]</td>\n",
       "      <td>-0.294987</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[Buying $SBUX on dip]</td>\n",
       "      <td>[0.483]</td>\n",
       "      <td>0.351095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[is a short below 740, and is overbought]</td>\n",
       "      <td>[-0.48]</td>\n",
       "      <td>-0.955162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[don't Putting on a down little $F short]</td>\n",
       "      <td>[-0.454]</td>\n",
       "      <td>-0.690074</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  old_pred  Prediction\n",
       "0                      [Putting on a little $F short]  [-0.454]   -0.774885\n",
       "1                                        [short some]  [-0.464]   -0.335771\n",
       "2                                [buying opportunity]   [0.445]    0.415726\n",
       "3                       [Scaling Up on Long Position]   [0.661]    0.692813\n",
       "4                            [its time to sell banks]  [-0.763]   -0.521347\n",
       "5                                     [Entering long]   [0.627]    0.470386\n",
       "6                                    [picked some up]   [0.653]    0.397556\n",
       "7   [time to accumulate for a long position, far m...   [0.668]    0.617554\n",
       "8   [Looking for a strong bounce, Lunchtime rally ...    [0.46]    0.493107\n",
       "9   [Very intrigued with the technology and growth...   [0.403]    0.393143\n",
       "10                 [short2 48 + - ***worked, puts up]     [0.0]    0.080027\n",
       "11                            [Biggest Market Losers]  [-0.438]   -0.423175\n",
       "12                          [$GOOG $GOOGL would suck]  [-0.398]   -0.294987\n",
       "13                              [Buying $SBUX on dip]   [0.483]    0.351095\n",
       "14          [is a short below 740, and is overbought]   [-0.48]   -0.955162\n",
       "15          [don't Putting on a down little $F short]  [-0.454]   -0.690074"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict \n",
    "#output=grid.best_estimator_.predict(X_test)\n",
    "tweet_row=list(data_test.processed_data[\"spans\"])\n",
    "#old_pred=list(data_test.processed_data[\"sentiment score\"])\n",
    "old_pred=list(y_test.values)\n",
    "print(len(old_pred))\n",
    "output = XGBR.predict(X_test)\n",
    "#final_df = pd.DataFrame(tweet_row, columns=labels)\n",
    "final_df = pd.DataFrame()\n",
    "final_df[\"tweet\"] = tweet_row\n",
    "\n",
    "final_df[\"old_pred\"] = old_pred\n",
    "\n",
    "final_df[\"Prediction\"] = output\n",
    "final_df.to_csv(\"Output_1.csv\",sep=\",\")\n",
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error: 0.03683362613575526\n",
      "R2 score: 0.8713257051094979\n"
     ]
    }
   ],
   "source": [
    "#test model accuracy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics importean_squared_error\n",
    "\n",
    "ss\n",
    "print(\"mean squared error:\" ,mean_squared_error(output, y_test))\n",
    "print(\"R2 score:\" ,r2_score(output,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
