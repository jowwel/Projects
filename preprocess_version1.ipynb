{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class '_io.TextIOWrapper'>\n",
      "<class 'list'>\n",
      "feature List= ['on dip', 'and be', 'time to', 'rally come', 'far more', 'on a', 'Lunchtime rally', 'Long Position', 'little F', 'the technology', 'it time', 'technology and', 'put up', 'SBUX on', 'Entering long', 'intrigue with', 'GOOGL would', 'picked some', 'Very intrigue', 'Buying SBUX', 'Scaling Up', 'sell bank', 'Market Losers', 'and growth', 'on Long', 'Up on', 'than downside', 'would suck', 'growth potential', 'more upside', 'upside than', 'to sell', 'some up', 'Biggest Market', 'F short', 'be overbought', 'buying opportunity', 'a little', 'Putting on', 'short some', 'with the', 'GOOG GOOGL']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize,blankline_tokenize\n",
    "from nltk import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import json\n",
    "import re as regex\n",
    "\n",
    "#remove numbers\n",
    "def remove_numbers(text):\n",
    "    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "#20 Function to remove whitespace\n",
    "def remove_whitespace(text):\n",
    "    return \" \".join(text.split())\n",
    "\n",
    "# Function to remove punctuations\n",
    "def remove_punctuations(text):\n",
    "    words = nltk.word_tokenize(text)\n",
    "    punt_removed = [w for w in words if w.lower() not in string.punctuation]\n",
    "    return \" \".join(punt_removed)\n",
    "#30\n",
    "\n",
    "# Function to remove stop words\n",
    "def remove_stopwords(text, lang='english'):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "    filtered_sentence = []\n",
    "\n",
    "    for w in word_tokens:\n",
    "        if w not in stop_words:\n",
    "            filtered_sentence.append(w)\n",
    "    return filtered_sentence\n",
    "\n",
    "#40\n",
    "#function for stemming\n",
    "def words_stemmer(words, type=\"PorterStemmer\", lang=\"english\", encoding=\"utf8\"):\n",
    "    supported_stemmers = [\"PorterStemmer\",\"LancasterStemmer\",\"SnowballStemmer\"]\n",
    "    if type is False or type not in supported_stemmers:\n",
    "        return words\n",
    "    else:\n",
    "        stem_words = []\n",
    "        if type == \"PorterStemmer\":\n",
    "            stemmer = PorterStemmer()\n",
    "#50\n",
    "            for word in words:\n",
    "                stem_words.append(stemmer.stem(word))\n",
    "        if type == \"LancasterStemmer\":\n",
    "            stemmer = LancasterStemmer()\n",
    "            for word in words:\n",
    "                stem_words.append(stemmer.stem(word))\n",
    "        if type == \"SnowballStemmer\":\n",
    "            stemmer = SnowballStemmer(lang)\n",
    "            for word in words:\n",
    "#60\n",
    "                stem_words.append(stemmer.stem(word))\n",
    "    return \" \".join(stem_words)\n",
    "\n",
    "\n",
    "\n",
    "# Function to apply lemmatization to a list of words\n",
    "def words_lemmatizer(text, encoding=\"utf8\"):\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    words = nltk.word_tokenize(text)\n",
    "    lemma_words = []\n",
    "    wl = WordNetLemmatizer()\n",
    "    for word in words:\n",
    "        pos = find_pos(word)\n",
    "        lemma_words.append(wl.lemmatize(word, pos))\n",
    "    return \" \".join(lemma_words)\n",
    "\n",
    "# Function to find part of speech tag for a word\n",
    "def find_pos(word):\n",
    "    # Part of Speech constants\n",
    "    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "    # You can learn more about these at http://wordnet.princeton.edu/\n",
    "    #wordnet/man/wndb.5WN.html#sect3\n",
    "    # You can learn more about all the penn tree tags at https://www.ling.\n",
    "    #upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html\n",
    "    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "    # Adjective tags - 'JJ', 'JJR', 'JJS'\n",
    "    if pos.lower()[0] == 'j':\n",
    "        return 'a'\n",
    "    # Adverb tags - 'RB', 'RBR', 'RBS'\n",
    "    elif pos.lower()[0] == 'r':\n",
    "        return 'r'\n",
    "    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "    elif pos.lower()[0] == 'v':\n",
    "        return 'v'\n",
    "    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'\n",
    "    else:\n",
    "        return 'n'\n",
    "\n",
    "# Function to extract n-grams from text\n",
    "def get_ngrams(text, n):\n",
    "\n",
    "    n_grams = ngrams(nltk.word_tokenize(text), n)\n",
    "    return [ ' '.join(grams) for grams in n_grams]\n",
    "\n",
    "#load file data into dictionnary \n",
    "with open('Microblog_Trialdata.json') as json_data:\n",
    "    print(type(json_data))\n",
    "    data_dict = json.load(json_data)\n",
    "    title0 = data_dict[0]['spans']\n",
    "    print(type(title0))\n",
    "    \n",
    "#print ('Removed numbers:---------------\\n ', remove_numbers(text))\n",
    "#print ('Removed whitespace:--------------- \\n ', remove_whitespace(text))\n",
    "#print ('remove_punctuations---------------- \\n', remove_punctuations(text))\n",
    "#print ('remove_stopwords----------- \\n',remove_stopwords(text))\n",
    "featureList = []\n",
    "fea_vect=[]\n",
    "for data in data_dict:\n",
    "    title0 = data['spans']\n",
    "    score=data['sentiment score']\n",
    "    #convert to string\n",
    "    ch=\"\"\n",
    "    for w in title0 :\n",
    "        ch=\"\".join(w)\n",
    "    #Convert to lower case\n",
    "    txt_num=remove_numbers(ch)\n",
    "    txt_ws=remove_whitespace(txt_num)\n",
    "    txt_pun=remove_punctuations(txt_ws)\n",
    "    txt_final=remove_stopwords(txt_pun)\n",
    "    #print ('texte finale --------------\\n',txt_final)\n",
    "    texte=words_lemmatizer(txt_pun)\n",
    "    n_grams=get_ngrams(texte, 2)\n",
    "    featureList.extend(n_grams)\n",
    "    featureList = list(set(featureList))\n",
    "\n",
    "    #print(Counter(word_tokenize(texte)))\n",
    "    fea_vect.append((n_grams, score));\n",
    "    \n",
    "#fea_vect\n",
    "print(\"feature List=\",featureList)\n",
    "with open('ngrams.csv', 'w',newline='') as csvfile:\n",
    "    filewriter = csv.writer(csvfile,delimiter=' ', quotechar='|')\n",
    "    for row in fea_vect:\n",
    "        filewriter.writerow(row[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(['Putting on', 'on a', 'a little', 'little F', 'F short'], -0.454),\n",
       " (['short some'], -0.464),\n",
       " (['buying opportunity'], 0.445),\n",
       " (['Scaling Up', 'Up on', 'on Long', 'Long Position'], 0.661),\n",
       " (['it time', 'time to', 'to sell', 'sell bank'], -0.763),\n",
       " (['Entering long'], 0.627),\n",
       " (['picked some', 'some up'], 0.653),\n",
       " (['far more', 'more upside', 'upside than', 'than downside'], 0.668),\n",
       " (['Lunchtime rally', 'rally come'], 0.46),\n",
       " (['Very intrigue',\n",
       "   'intrigue with',\n",
       "   'with the',\n",
       "   'the technology',\n",
       "   'technology and',\n",
       "   'and growth',\n",
       "   'growth potential'],\n",
       "  0.403),\n",
       " (['put up'], 0.0),\n",
       " (['Biggest Market', 'Market Losers'], -0.438),\n",
       " (['GOOG GOOGL', 'GOOGL would', 'would suck'], -0.398),\n",
       " (['Buying SBUX', 'SBUX on', 'on dip'], 0.483),\n",
       " (['and be', 'be overbought'], -0.48),\n",
       " (['Putting on', 'on a', 'a little', 'little F', 'F short'], -0.454)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fea_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('on dip', 1),\n",
       " ('and be', 1),\n",
       " ('time to', 1),\n",
       " ('rally come', 1),\n",
       " ('far more', 1),\n",
       " ('on a', 1),\n",
       " ('Lunchtime rally', 1),\n",
       " ('Long Position', 1),\n",
       " ('little F', 1),\n",
       " ('the technology', 1),\n",
       " ('it time', 1),\n",
       " ('technology and', 1),\n",
       " ('put up', 1),\n",
       " ('SBUX on', 1),\n",
       " ('Entering long', 1),\n",
       " ('intrigue with', 1),\n",
       " ('GOOGL would', 1),\n",
       " ('picked some', 1),\n",
       " ('Very intrigue', 1),\n",
       " ('Buying SBUX', 1),\n",
       " ('Scaling Up', 1),\n",
       " ('sell bank', 1),\n",
       " ('Market Losers', 1),\n",
       " ('and growth', 1),\n",
       " ('on Long', 1),\n",
       " ('Up on', 1),\n",
       " ('than downside', 1),\n",
       " ('would suck', 1),\n",
       " ('growth potential', 1),\n",
       " ('more upside', 1),\n",
       " ('upside than', 1),\n",
       " ('to sell', 1),\n",
       " ('some up', 1),\n",
       " ('Biggest Market', 1),\n",
       " ('F short', 1),\n",
       " ('be overbought', 1),\n",
       " ('buying opportunity', 1),\n",
       " ('a little', 1),\n",
       " ('Putting on', 1),\n",
       " ('short some', 1),\n",
       " ('with the', 1),\n",
       " ('GOOG GOOGL', 1)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words=Counter(featureList)\n",
    "\n",
    "words.most_common()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_wordlist(data, min_occurrences=3, max_occurences=500,whitelist=None):\n",
    "        wordlist = []\n",
    "        import os\n",
    "        if os.path.isfile(\"wordlist_me.csv\"):\n",
    "            word_df = pd.read_csv(\"wordlist_me.csv\")\n",
    "            word_df = word_df[word_df[\"occurrences\"] > min_occurrences]\n",
    "            wordlist = list(word_df.loc[:, \"word\"])\n",
    "            return\n",
    "\n",
    "        words = Counter()\n",
    "        \n",
    "\n",
    "        word_df = pd.DataFrame(data={\"word\": [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                     \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                               columns=[\"word\", \"occurrences\"])\n",
    "\n",
    "        word_df.to_csv(\"wordlist_me.csv\", index_label=\"id\")\n",
    "        wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$F</td>\n",
       "      <td>5540055</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Putting on a little $F short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>10752226</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short some]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$BAC</td>\n",
       "      <td>10920221</td>\n",
       "      <td>0.445</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[buying opportunity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$SHOR</td>\n",
       "      <td>12971398</td>\n",
       "      <td>0.661</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Scaling Up on Long Position]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$JPM</td>\n",
       "      <td>16142438</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[its time to sell banks]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$LMT</td>\n",
       "      <td>14073133</td>\n",
       "      <td>0.627</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Entering long]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$DNN</td>\n",
       "      <td>18479024</td>\n",
       "      <td>0.653</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[picked some up]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>0.668</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[time to accumulate for a long position, far m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>0.460</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Looking for a strong bounce, Lunchtime rally ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>0.403</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Very intrigued with the technology and growth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>0.000</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short2 48 + - ***worked, puts up]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Biggest Market Losers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[$GOOG $GOOGL would suck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>$SBUX</td>\n",
       "      <td>719890387314335744</td>\n",
       "      <td>0.483</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Buying $SBUX on dip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>$GOOGL</td>\n",
       "      <td>708668814427348992</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[is a short below 740, and is overbought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>$F</td>\n",
       "      <td>5540055</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Putting on a little $F short]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cashtag                  id  sentiment score      source  \\\n",
       "0       $F             5540055           -0.454  stocktwits   \n",
       "1    $AAPL            10752226           -0.464  stocktwits   \n",
       "2     $BAC            10920221            0.445  stocktwits   \n",
       "3    $SHOR            12971398            0.661  stocktwits   \n",
       "4     $JPM            16142438           -0.763  stocktwits   \n",
       "5     $LMT            14073133            0.627  stocktwits   \n",
       "6     $DNN            18479024            0.653  stocktwits   \n",
       "7     $CRK            34147106            0.668  stocktwits   \n",
       "8     $CRK            34147106            0.460  stocktwits   \n",
       "9     $CRK            34147106            0.403  stocktwits   \n",
       "10    $CRK            34147106            0.000  stocktwits   \n",
       "11    $CRK            34147106           -0.438  stocktwits   \n",
       "12    $CRK            34147106           -0.398  stocktwits   \n",
       "13   $SBUX  719890387314335744            0.483     twitter   \n",
       "14  $GOOGL  708668814427348992           -0.480     twitter   \n",
       "15      $F             5540055           -0.454  stocktwits   \n",
       "\n",
       "                                                spans  \n",
       "0                      [Putting on a little $F short]  \n",
       "1                                        [short some]  \n",
       "2                                [buying opportunity]  \n",
       "3                       [Scaling Up on Long Position]  \n",
       "4                            [its time to sell banks]  \n",
       "5                                     [Entering long]  \n",
       "6                                    [picked some up]  \n",
       "7   [time to accumulate for a long position, far m...  \n",
       "8   [Looking for a strong bounce, Lunchtime rally ...  \n",
       "9   [Very intrigued with the technology and growth...  \n",
       "10                 [short2 48 + - ***worked, puts up]  \n",
       "11                            [Biggest Market Losers]  \n",
       "12                          [$GOOG $GOOGL would suck]  \n",
       "13                              [Buying $SBUX on dip]  \n",
       "14          [is a short below 740, and is overbought]  \n",
       "15                     [Putting on a little $F short]  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json('Microblog_Trialdata.json')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', \"'s\", 'words', ',', 'showing', 'off', 'the', 'stop', 'words', 'filtration', '.']\n",
      "This sample sentence 's words , showing stop words filtration .\n"
     ]
    }
   ],
   "source": [
    "example_sent = \"This is a sample sentence's words, showing off the stop words filtration.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "word_tokens = word_tokenize(example_sent)\n",
    "\n",
    "filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "\n",
    "filtered_sentence = []\n",
    "\n",
    "for w in word_tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_sentence.append(w)\n",
    "ch=\" \".join(filtered_sentence)\n",
    "\n",
    "print(word_tokens)\n",
    "print(ch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
