{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize,blankline_tokenize\n",
    "from nltk import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import json\n",
    "import re as regex\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "import xgboost as xgb\n",
    "from sklearn import model_selection, preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_Initialize():\n",
    "    data = []\n",
    "    processed_data = []\n",
    "    wordlist = []\n",
    "\n",
    "    featureList = []\n",
    "    fea_vect=[]\n",
    "    whitelist = [\"n't\",\"not\",\"don't\",\"up\",\"down\",\"below\"]    \n",
    "    \n",
    "    data_model = None\n",
    "    data_labels = None\n",
    "    is_testing = False\n",
    "    \n",
    "    def initialize(self, csv_file, is_testing_set=False, from_cached=None):\n",
    "        if from_cached is not None:\n",
    "            #self.data_model = pd.read_csv(from_cached)\n",
    "            self.data_model = pd.read_json(from_cached)\n",
    "\n",
    "            return\n",
    "\n",
    "        self.is_testing = is_testing_set\n",
    "\n",
    "        if not is_testing_set:\n",
    "            #self.data = pd.read_csv(csv_file, header=0, names=[\"id\", \"emotion\", \"text\"])\n",
    "            self.data = pd.read_json(csv_file)\n",
    "\n",
    "            #self.data = self.data[self.data[\"emotion\"].isin([\"positive\", \"negative\", \"neutral\"])]\n",
    "        \n",
    "\n",
    "        self.processed_data = self.data\n",
    "        self.wordlist = []\n",
    "        self.data_model = None\n",
    "        self.data_labels = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def do_process(self):\n",
    "        import time\n",
    "        start_time = time.time()\n",
    "        def stem_and_join(row,stemmer=nltk.PorterStemmer()):\n",
    "            row[\"spans\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"spans\"]))\n",
    "            return row\n",
    "    \n",
    "        def tokenize_grams(row):\n",
    "                #remove numbers\n",
    "                def remove_numbers(text):\n",
    "                    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "                #20 Function to remove whitespace\n",
    "                def remove_whitespace(text):\n",
    "                    return \" \".join(text.split())\n",
    "\n",
    "                # Function to remove punctuations\n",
    "                def remove_punctuations(text):\n",
    "                    words = nltk.word_tokenize(text)\n",
    "                    punt_removed = [w for w in words if w.lower() not in string.punctuation]\n",
    "                    return \" \".join(punt_removed)\n",
    "#30\n",
    "\n",
    "                # Function to remove stop words\n",
    "                def remove_stopwords(text, lang='english'):\n",
    "                    whitelist = [\"n't\",\"not\",\"don't\",\"up\",\"down\",\"below\"]    \n",
    "\n",
    "                    stop_words = set(stopwords.words('english'))\n",
    "                    word_tokens = word_tokenize(text)\n",
    "                    #filtered_sentence = [w for w in word_tokens if not w in stop_words]\n",
    "                    filtered_sentence = []\n",
    "                    for w in word_tokens:\n",
    "                        if w not in stop_words or w in whitelist:\n",
    "                            filtered_sentence.append(w)\n",
    "                \n",
    "                    ch=\" \".join(filtered_sentence)\n",
    "\n",
    "                    return ch\n",
    "#40\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Function to extract n-grams from text\n",
    "                def get_ngrams(text, n):\n",
    "\n",
    "                    n_grams = ngrams(nltk.word_tokenize(text), n)\n",
    "                    list_grams=[ ' '.join(grams) for grams in n_grams]\n",
    "                    return list_grams\n",
    "\n",
    "\n",
    "                # Function to apply lemmatization to a list of words\n",
    "                def words_lemmatizer(text, encoding=\"utf8\"):\n",
    "                    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "                    words = nltk.word_tokenize(text)\n",
    "                    lemma_words = []\n",
    "                    wl = WordNetLemmatizer()\n",
    "                    for word in words:\n",
    "                        pos = find_pos(word)\n",
    "                        lemma_words.append(wl.lemmatize(word, pos))\n",
    "                    return \" \".join(lemma_words)\n",
    "\n",
    "                # Function to find part of speech tag for a word\n",
    "                def find_pos(word):\n",
    "                    # Part of Speech constants\n",
    "                    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "   \n",
    "                    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "                    # Adjective tags - 'JJ', 'JJR', 'JJS'\n",
    "                    if pos.lower()[0] == 'j':\n",
    "                        return 'a'\n",
    "                    # Adverb tags - 'RB', 'RBR', 'RBS'\n",
    "                    elif pos.lower()[0] == 'r':\n",
    "                        return 'r'\n",
    "                    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "                    elif pos.lower()[0] == 'v':\n",
    "                        return 'v'\n",
    "                    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'\n",
    "                    else:\n",
    "                        return 'n'\n",
    "    \n",
    "            \n",
    "            \n",
    "                idx=row[\"spans\"]\n",
    "                #convert to string\n",
    "                #ch=\"\"\n",
    "                #for w in idx :\n",
    "                ch=\" \".join(idx)\n",
    "    \n",
    "                #words_stemmer(ch, type=\"PorterStemmer\", lang=\"english\", encoding=\"utf8\")\n",
    "                #Convert to lower case\n",
    "                txt_num=remove_numbers(ch)\n",
    "    \n",
    "                txt_ws=remove_whitespace(txt_num)\n",
    "    \n",
    "                txt_pun=remove_punctuations(txt_ws)\n",
    "        \n",
    "                txt_final=remove_stopwords(txt_pun)\n",
    "\n",
    "                texte=words_lemmatizer(txt_final)\n",
    "\n",
    "                n_grams=get_ngrams(texte, 1)\n",
    "                row[\"token_spans\"] = n_grams\n",
    "    \n",
    "                return row\n",
    "        self.processed_data = self.processed_data.apply(stem_and_join, axis=1)\n",
    "        self.processed_data = self.processed_data.apply(tokenize_grams, axis=1)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    def build_wordlist(self, min_occurrences=0, max_occurences=500, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                       whitelist=None):\n",
    "        self.wordlist = []\n",
    "        whitelist = self.whitelist if whitelist is None else whitelist\n",
    "        \n",
    "\n",
    "        words = Counter()\n",
    "\n",
    "        for idx in self.processed_data.index:\n",
    "            words.update(self.processed_data.loc[idx, \"token_spans\"])\n",
    "\n",
    "        \n",
    "                \n",
    "\n",
    "        word_df = pd.DataFrame(data={\"word\": [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                     \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                               columns=[\"word\", \"occurrences\"])\n",
    "\n",
    "        word_df.to_csv(\"wordlist_copie.csv\", index_label=\"idx\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]\n",
    "        \n",
    "        \n",
    "    def build_data_model(self):\n",
    "        label_column = []\n",
    "        \n",
    "        label_column = [\"label\"]\n",
    "\n",
    "        columns = label_column + list(\n",
    "            map(lambda w: w + \"_bow\",self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "\n",
    "            if True:\n",
    "                # add label\n",
    "                current_label = self.processed_data.loc[idx, \"sentiment score\"]\n",
    "                labels.append(current_label)\n",
    "                current_row.append(current_label)\n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"token_spans\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        return self.data_model, self.data_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_Initialize_test(TwitterData_Initialize):\n",
    "\n",
    "    \n",
    "    \n",
    "    def do_process(self):\n",
    "        def stem_and_join(row,stemmer=nltk.PorterStemmer()):\n",
    "            row[\"spans\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"spans\"]))\n",
    "            return row\n",
    "    \n",
    "        def tokenize_grams(row):\n",
    "                #remove numbers\n",
    "                def remove_numbers(text):\n",
    "                    return re.sub(r'\\d+', '', text)\n",
    "\n",
    "                def to_lower(text):\n",
    "                    return text.lower()\n",
    "                \n",
    "                #20 Function to remove whitespace\n",
    "                def remove_whitespace(text):\n",
    "                    return \" \".join(text.split())\n",
    "\n",
    "                # Function to remove punctuations\n",
    "                def remove_punctuations(text):\n",
    "                    text=re.sub(r'[?|*|.|!|+|-]',r'',text)\n",
    "\n",
    "                    words = nltk.word_tokenize(text)\n",
    "                    punt_removed = [w for w in words if w.lower() not in string.punctuation]\n",
    "                    return \" \".join(punt_removed)\n",
    "#30\n",
    "\n",
    "                # Function to remove stop words\n",
    "                def remove_stopwords(text, lang='english'):\n",
    "                    whitelist = [\"n't\",\"not\",\"below\"]    \n",
    "\n",
    "                    stop_words = set(stopwords.words('english'))\n",
    "                    word_tokens = word_tokenize(text)\n",
    "                    #filtered_sentence = [w for w in word_tokens if ((not w in stop_words) or (w in whitelist))]\n",
    "                    filtered_sentence = []\n",
    "                    for w in word_tokens:\n",
    "                        if ((w not in stop_words) or (w in whitelist)):\n",
    "                            filtered_sentence.append(w)\n",
    "                \n",
    "                    ch=\" \".join(filtered_sentence)\n",
    "\n",
    "                    return ch\n",
    "#40\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "                # Function to extract n-grams from text\n",
    "                def get_ngrams(text, n):\n",
    "\n",
    "                    n_grams = ngrams(nltk.word_tokenize(text), n)\n",
    "                    list_grams=[ ' '.join(grams) for grams in n_grams]\n",
    "                    return list_grams\n",
    "\n",
    "\n",
    "                # Function to apply lemmatization to a list of words\n",
    "                def words_lemmatizer(text, encoding=\"utf8\"):\n",
    "                    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "                    words = nltk.word_tokenize(text)\n",
    "                    lemma_words = []\n",
    "                    wl = WordNetLemmatizer()\n",
    "                    for word in words:\n",
    "                        pos = find_pos(word)\n",
    "                        lemma_words.append(wl.lemmatize(word, pos))\n",
    "                    return \" \".join(lemma_words)\n",
    "\n",
    "                # Function to find part of speech tag for a word\n",
    "                def find_pos(word):\n",
    "                    # Part of Speech constants\n",
    "                    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "   \n",
    "                    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "                    # Adjective tags - 'JJ', 'JJR', 'JJS'\n",
    "                    if pos.lower()[0] == 'j':\n",
    "                        return 'a'\n",
    "                    # Adverb tags - 'RB', 'RBR', 'RBS'\n",
    "                    elif pos.lower()[0] == 'r':\n",
    "                        return 'r'\n",
    "                    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "                    elif pos.lower()[0] == 'v':\n",
    "                        return 'v'\n",
    "                    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'\n",
    "                    else:\n",
    "                        return 'n'\n",
    "    \n",
    "            \n",
    "            \n",
    "                idx=row[\"spans\"]\n",
    "                ch=''.join(idx)\n",
    "\n",
    "\n",
    "    \n",
    "                #words_stemmer(ch, type=\"PorterStemmer\", lang=\"english\", encoding=\"utf8\")\n",
    "                #Convert to lower case\n",
    "                txt_num=remove_numbers(ch)\n",
    "                \n",
    "                txt_low=to_lower(txt_num)\n",
    "                \n",
    "                txt_ws=remove_whitespace(txt_low)\n",
    "    \n",
    "                txt_pun=remove_punctuations(txt_ws)\n",
    "        \n",
    "                txt_final=remove_stopwords(txt_pun)\n",
    "\n",
    "                texte=words_lemmatizer(txt_final)\n",
    "\n",
    "                n_grams=get_ngrams(texte, 1)\n",
    "                print(\"list ngrams--------------\\n\",n_grams)\n",
    "\n",
    "                row[\"token_spans\"] = n_grams\n",
    "    \n",
    "                return row\n",
    "        #self.processed_data = self.processed_data.apply(stem_and_join, axis=1)\n",
    "        self.processed_data = self.processed_data.apply(tokenize_grams, axis=1)\n",
    "        \n",
    "    def build_wordlist(self, min_occurrences=0, max_occurences=500, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                      ):\n",
    "        self.wordlist = []\n",
    "        \n",
    "\n",
    "        words = Counter()\n",
    "\n",
    "        for idx in self.processed_data.index:\n",
    "            words.update(self.processed_data.loc[idx, \"token_spans\"])\n",
    "\n",
    "        \n",
    "                \n",
    "\n",
    "        word_df = pd.DataFrame(data={\"word\": [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                     \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                               columns=[\"word\", \"occurrences\"])\n",
    "\n",
    "        word_df.to_csv(\"wordlist_test.csv\", index_label=\"idx\")\n",
    "        self.wordlist = [k for k, v in words.most_common() if min_occurrences < v < max_occurences]\n",
    "        \n",
    "        \n",
    "    def build_data_model(self):\n",
    "        \n",
    "\n",
    "        columns = list(\n",
    "            map(lambda w: w + \"_bow\",self.wordlist))\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "\n",
    "            \n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"token_spans\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        return self.data_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$F</td>\n",
       "      <td>5540055</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Putting on a little $F short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>10752226</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short some]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$BAC</td>\n",
       "      <td>10920221</td>\n",
       "      <td>0.445</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[buying opportunity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$SHOR</td>\n",
       "      <td>12971398</td>\n",
       "      <td>0.661</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Scaling Up on Long Position]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$JPM</td>\n",
       "      <td>16142438</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[its time to sell banks]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cashtag        id  sentiment score      source  \\\n",
       "0      $F   5540055           -0.454  stocktwits   \n",
       "1   $AAPL  10752226           -0.464  stocktwits   \n",
       "2    $BAC  10920221            0.445  stocktwits   \n",
       "3   $SHOR  12971398            0.661  stocktwits   \n",
       "4    $JPM  16142438           -0.763  stocktwits   \n",
       "\n",
       "                            spans  \n",
       "0  [Putting on a little $F short]  \n",
       "1                    [short some]  \n",
       "2            [buying opportunity]  \n",
       "3   [Scaling Up on Long Position]  \n",
       "4        [its time to sell banks]  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TwitterData_Initialize()\n",
    "data.initialize(\"Microblog_Trialdata.json\")\n",
    "\n",
    "data.processed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$SBUX</td>\n",
       "      <td>719911771625140200</td>\n",
       "      <td>twitter</td>\n",
       "      <td>will be a solid long term investmen Income</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>711948847971307500</td>\n",
       "      <td>twitter</td>\n",
       "      <td>Excited;$;big +value;$;Hoping little</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$QUNR</td>\n",
       "      <td>719917003033681900</td>\n",
       "      <td>twitter</td>\n",
       "      <td>China's Tourist Bonanza Isn't Going Anywhere</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$ESI</td>\n",
       "      <td>39047884</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>0.15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$TSLA</td>\n",
       "      <td>719590251891658700</td>\n",
       "      <td>twitter</td>\n",
       "      <td>not guaranteed</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cashtag                  id      source  \\\n",
       "0   $SBUX  719911771625140200     twitter   \n",
       "1   $AAPL  711948847971307500     twitter   \n",
       "2   $QUNR  719917003033681900     twitter   \n",
       "3    $ESI            39047884  stocktwits   \n",
       "4   $TSLA  719590251891658700     twitter   \n",
       "\n",
       "                                          spans  \n",
       "0    will be a solid long term investmen Income  \n",
       "1          Excited;$;big +value;$;Hoping little  \n",
       "2  China's Tourist Bonanza Isn't Going Anywhere  \n",
       "3                                          0.15  \n",
       "4                                not guaranteed  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test = TwitterData_Initialize_test()\n",
    "data_test.initialize(\"Microblog_trialtest.json\")\n",
    "\n",
    "data_test.processed_data.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 57\n",
      "Vocabulary content:\n",
      " {'putting': 36, 'on': 29, 'little': 21, 'short': 41, 'some': 43, 'buying': 8, 'opportunity': 30, 'scaling': 39, 'up': 51, 'long': 22, 'position': 33, 'its': 20, 'time': 49, 'to': 50, 'sell': 40, 'banks': 4, 'entering': 13, 'picked': 32, 'accumulate': 2, 'for': 14, 'positionfar': 34, 'more': 26, 'upside': 52, 'than': 47, 'downside': 12, 'looking': 23, 'strong': 44, 'bouncelunchtime': 7, 'rally': 37, 'coming': 9, 'very': 53, 'intrigued': 18, 'with': 54, 'the': 48, 'technology': 46, 'and': 3, 'growth': 17, 'potential': 35, 'short2': 42, '48': 0, 'workedputs': 55, 'biggest': 6, 'market': 25, 'losers': 24, 'goog': 15, 'googl': 16, 'would': 56, 'suck': 45, 'sbux': 38, 'dip': 11, 'is': 19, 'below': 5, '740and': 1, 'overbought': 31, 'not': 28, 'non': 27, 'didn': 10}\n",
      "bag_of_words: <16x57 sparse matrix of type '<class 'numpy.int64'>'\n",
      "\twith 74 stored elements in Compressed Sparse Row format>\n",
      "Dense representation of bag_of_words:\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 1 0 0 0 1 0 0\n",
      "  0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 1 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0\n",
      "  0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 1 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 1 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0 1 1 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 1 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0\n",
      "  0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 1 2 1 0 0 0 0 0 0\n",
      "  1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "--- 0.007017850875854492 seconds ---\n"
     ]
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "start_time = time.time()\n",
    "tweets=list(data.processed_data.spans)\n",
    "tweets\n",
    "list_tweet=[]\n",
    "for i,chaine in enumerate(tweets):\n",
    "    ch=\"\".join(chaine)\n",
    "    list_tweet.append(ch)\n",
    "list_tweet\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vect = CountVectorizer()\n",
    "vect.fit(list_tweet)\n",
    "print(\"Vocabulary size: {}\".format(len(vect.vocabulary_)))\n",
    "print(\"Vocabulary content:\\n {}\".format(vect.vocabulary_))\n",
    "X = vect.transform(list_tweet)\n",
    "print(\"bag_of_words: {}\".format(repr(X)))\n",
    "print(\"Dense representation of bag_of_words:\\n{}\".format(\n",
    "X.toarray()))\n",
    "print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "#sparse_matrix = sparse.csr_matrix(bag_of_words)\n",
    "#print(\"\\nSciPy sparse CSR matrix:\\n{}\".format(sparse_matrix))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$F</td>\n",
       "      <td>5540055</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Putting on a little $F short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>10752226</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short some]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$BAC</td>\n",
       "      <td>10920221</td>\n",
       "      <td>0.445</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[buying opportunity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$SHOR</td>\n",
       "      <td>12971398</td>\n",
       "      <td>0.661</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Scaling Up on Long Position]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$JPM</td>\n",
       "      <td>16142438</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[its time to sell banks]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$LMT</td>\n",
       "      <td>14073133</td>\n",
       "      <td>0.627</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Entering long]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$DNN</td>\n",
       "      <td>18479024</td>\n",
       "      <td>0.653</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[picked some up]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>0.668</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[time to accumulate for a long position, far m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>0.460</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Looking for a strong bounce, Lunchtime rally ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>0.403</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Very intrigued with the technology and growth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>0.000</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short2 48 + - ***worked, puts up]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Biggest Market Losers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[$GOOG $GOOGL would suck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>$SBUX</td>\n",
       "      <td>719890387314335744</td>\n",
       "      <td>0.483</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Buying $SBUX on dip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>$GOOGL</td>\n",
       "      <td>708668814427348992</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[is a short below 740, and is overbought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>$F</td>\n",
       "      <td>5540055</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[not Putting on a non little didn't $F short not]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cashtag                  id  sentiment score      source  \\\n",
       "0       $F             5540055           -0.454  stocktwits   \n",
       "1    $AAPL            10752226           -0.464  stocktwits   \n",
       "2     $BAC            10920221            0.445  stocktwits   \n",
       "3    $SHOR            12971398            0.661  stocktwits   \n",
       "4     $JPM            16142438           -0.763  stocktwits   \n",
       "5     $LMT            14073133            0.627  stocktwits   \n",
       "6     $DNN            18479024            0.653  stocktwits   \n",
       "7     $CRK            34147106            0.668  stocktwits   \n",
       "8     $CRK            34147106            0.460  stocktwits   \n",
       "9     $CRK            34147106            0.403  stocktwits   \n",
       "10    $CRK            34147106            0.000  stocktwits   \n",
       "11    $CRK            34147106           -0.438  stocktwits   \n",
       "12    $CRK            34147106           -0.398  stocktwits   \n",
       "13   $SBUX  719890387314335744            0.483     twitter   \n",
       "14  $GOOGL  708668814427348992           -0.480     twitter   \n",
       "15      $F             5540055           -0.454  stocktwits   \n",
       "\n",
       "                                                spans  \n",
       "0                      [Putting on a little $F short]  \n",
       "1                                        [short some]  \n",
       "2                                [buying opportunity]  \n",
       "3                       [Scaling Up on Long Position]  \n",
       "4                            [its time to sell banks]  \n",
       "5                                     [Entering long]  \n",
       "6                                    [picked some up]  \n",
       "7   [time to accumulate for a long position, far m...  \n",
       "8   [Looking for a strong bounce, Lunchtime rally ...  \n",
       "9   [Very intrigued with the technology and growth...  \n",
       "10                 [short2 48 + - ***worked, puts up]  \n",
       "11                            [Biggest Market Losers]  \n",
       "12                          [$GOOG $GOOGL would suck]  \n",
       "13                              [Buying $SBUX on dip]  \n",
       "14          [is a short below 740, and is overbought]  \n",
       "15  [not Putting on a non little didn't $F short not]  "
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.processed_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.251349449157715 seconds ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.do_process()\n",
    "data.processed_data.head(20)\n",
    "data.build_wordlist()\n",
    "data.wordlist\n",
    "bow, labels = data.build_data_model()\n",
    "len(bow.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y=data.processed_data[\"sentiment score\"].values\n",
    "Y = bow.label.as_matrix()\n",
    "X_drop = bow.drop([\"label\"],axis=1)\n",
    "#train_x = train_x.drop([\"ID\"],axis=1)\n",
    "#train_x = train_x.values\n",
    "X=X_drop.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y,\n",
    "                                                    train_size=0.7\n",
    "                                                    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "ename": "JoblibAttributeError",
     "evalue": "JoblibAttributeError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\runpy.py in _run_code(code=<code object <module> at 0x0000020ACEF64E40, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\G...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x0000020ACEF64E40, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\G...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n     17 \n     18 \n     19 \n     20 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         self.io_loop = ioloop.IOLoop.current()\n    477         try:\n--> 478             self.io_loop.start()\n        self.io_loop.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    479         except KeyboardInterrupt:\n    480             pass\n    481 \n    482 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': '# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 5, 3, 15, 8, 39, 239612, tzinfo=tzutc()), 'msg_id': 'a48f4eec55d04e4689926ff2a1eb1ac5', 'msg_type': 'execute_request', 'session': 'd4858a1ad46447ad81f8da6d31e2e658', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': 'a48f4eec55d04e4689926ff2a1eb1ac5', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'd4858a1ad46447ad81f8da6d31e2e658']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': '# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 5, 3, 15, 8, 39, 239612, tzinfo=tzutc()), 'msg_id': 'a48f4eec55d04e4689926ff2a1eb1ac5', 'msg_type': 'execute_request', 'session': 'd4858a1ad46447ad81f8da6d31e2e658', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': 'a48f4eec55d04e4689926ff2a1eb1ac5', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'd4858a1ad46447ad81f8da6d31e2e658'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': '# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 5, 3, 15, 8, 39, 239612, tzinfo=tzutc()), 'msg_id': 'a48f4eec55d04e4689926ff2a1eb1ac5', 'msg_type': 'execute_request', 'session': 'd4858a1ad46447ad81f8da6d31e2e658', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': 'a48f4eec55d04e4689926ff2a1eb1ac5', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = '# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n', store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>], cell_name='<ipython-input-102-b14d86ea2afd>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 20ad96cd828, executio..._before_exec=None error_in_exec=None result=None>)\n   2845 \n   2846         try:\n   2847             for i, node in enumerate(to_run_exec):\n   2848                 mod = ast.Module([node])\n   2849                 code = compiler(mod, cell_name, \"exec\")\n-> 2850                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x0000020AE3DA6E40, file \"<ipython-input-102-b14d86ea2afd>\", line 7>\n        result = <ExecutionResult object at 20ad96cd828, executio..._before_exec=None error_in_exec=None result=None>\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x0000020AE3DA6E40, file \"<ipython-input-102-b14d86ea2afd>\", line 7>, result=<ExecutionResult object at 20ad96cd828, executio..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x0000020AE3DA6E40, file \"<ipython-input-102-b14d86ea2afd>\", line 7>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'Counter': <class 'collections.Counter'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', 'class TwitterData_Initialize_test(TwitterData_In..., columns=columns)\\n        return self.data_model', 'data = TwitterData_Initialize()\\ndata.initialize(...blog_Trialdata.json\")\\n\\ndata.processed_data.head()', 'data_test = TwitterData_Initialize_test()\\ndata_t...trialtest.json\")\\n\\ndata_test.processed_data.head()', 'from scipy import sparse\\nstart_time = time.time(...Py sparse CSR matrix:\\\\n{}\".format(sparse_matrix))', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', 'class TwitterData_Initialize_test(TwitterData_In..., columns=columns)\\n        return self.data_model', 'data = TwitterData_Initialize()\\ndata.initialize(...blog_Trialdata.json\")\\n\\ndata.processed_data.head()', 'data_test = TwitterData_Initialize_test()\\ndata_t...trialtest.json\")\\n\\ndata_test.processed_data.head()', 'from scipy import sparse\\nstart_time = time.time(...Py sparse CSR matrix:\\\\n{}\".format(sparse_matrix))', \"stop_words = set(stopwords.words('english'))\", 'stop_words', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', 'class TwitterData_Initialize_test(TwitterData_In..., columns=columns)\\n        return self.data_model', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', ...], 'LabelEncoder': <class 'sklearn.preprocessing.label.LabelEncoder'>, 'LancasterStemmer': <class 'nltk.stem.lancaster.LancasterStemmer'>, 'Out': {4:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , 5:   cashtag                  id      source  \\\n0  ...4                                not guaranteed  , 10:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , 11:   cashtag                  id      source  \\\n0  ...4                                not guaranteed  , 14: {'a', 'about', 'above', 'after', 'again', 'against', ...}, 21:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , 22:   cashtag                  id      source  \\\n0  ...4                                not guaranteed  , 23:    cashtag                  id  sentiment score ...ot Putting on a non little didn't $F short not]  , 25:    cashtag                  id  sentiment score ...              [put, non, little, n't, f, short]  , 28:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , ...}, 'PorterStemmer': <class 'nltk.stem.porter.PorterStemmer'>, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'SnowballStemmer': <class 'nltk.stem.snowball.SnowballStemmer'>, ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'Counter': <class 'collections.Counter'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', 'class TwitterData_Initialize_test(TwitterData_In..., columns=columns)\\n        return self.data_model', 'data = TwitterData_Initialize()\\ndata.initialize(...blog_Trialdata.json\")\\n\\ndata.processed_data.head()', 'data_test = TwitterData_Initialize_test()\\ndata_t...trialtest.json\")\\n\\ndata_test.processed_data.head()', 'from scipy import sparse\\nstart_time = time.time(...Py sparse CSR matrix:\\\\n{}\".format(sparse_matrix))', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', 'class TwitterData_Initialize_test(TwitterData_In..., columns=columns)\\n        return self.data_model', 'data = TwitterData_Initialize()\\ndata.initialize(...blog_Trialdata.json\")\\n\\ndata.processed_data.head()', 'data_test = TwitterData_Initialize_test()\\ndata_t...trialtest.json\")\\n\\ndata_test.processed_data.head()', 'from scipy import sparse\\nstart_time = time.time(...Py sparse CSR matrix:\\\\n{}\".format(sparse_matrix))', \"stop_words = set(stopwords.words('english'))\", 'stop_words', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', 'class TwitterData_Initialize_test(TwitterData_In..., columns=columns)\\n        return self.data_model', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', ...], 'LabelEncoder': <class 'sklearn.preprocessing.label.LabelEncoder'>, 'LancasterStemmer': <class 'nltk.stem.lancaster.LancasterStemmer'>, 'Out': {4:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , 5:   cashtag                  id      source  \\\n0  ...4                                not guaranteed  , 10:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , 11:   cashtag                  id      source  \\\n0  ...4                                not guaranteed  , 14: {'a', 'about', 'above', 'after', 'again', 'against', ...}, 21:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , 22:   cashtag                  id      source  \\\n0  ...4                                not guaranteed  , 23:    cashtag                  id  sentiment score ...ot Putting on a non little didn't $F short not]  , 25:    cashtag                  id  sentiment score ...              [put, non, little, n't, f, short]  , 28:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , ...}, 'PorterStemmer': <class 'nltk.stem.porter.PorterStemmer'>, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'SnowballStemmer': <class 'nltk.stem.snowball.SnowballStemmer'>, ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\Globalnet\\Desktop\\moi\\<ipython-input-102-b14d86ea2afd> in <module>()\n      2 model = XGBRegressor()\n      3 subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n      4 param_grid = dict(subsample=subsample)\n      5 kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=7)\n      6 grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1)\n----> 7 grid_result = grid_search.fit(X_train, y_train)\n      8 # summarize results\n      9 \n     10 \n     11 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...e=True,\n       scoring='neg_log_loss', verbose=0), X=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), y=array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), groups=None)\n    940 \n    941         groups : array-like, with shape (n_samples,), optional\n    942             Group labels for the samples used while splitting the dataset into\n    943             train/test set.\n    944         \"\"\"\n--> 945         return self._fit(X, y, groups, ParameterGrid(self.param_grid))\n        self._fit = <bound method BaseSearchCV._fit of GridSearchCV(...=True,\n       scoring='neg_log_loss', verbose=0)>\n        X = array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64)\n        y = array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483])\n        groups = None\n        self.param_grid = {'subsample': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]}\n    946 \n    947 \n    948 class RandomizedSearchCV(BaseSearchCV):\n    949     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py in _fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...e=True,\n       scoring='neg_log_loss', verbose=0), X=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), y=array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), groups=None, parameter_iterable=<sklearn.model_selection._search.ParameterGrid object>)\n    559                                   fit_params=self.fit_params,\n    560                                   return_train_score=self.return_train_score,\n    561                                   return_n_test_samples=True,\n    562                                   return_times=True, return_parameters=True,\n    563                                   error_score=self.error_score)\n--> 564           for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.model_selection._search.ParameterGrid object>\n    565           for train, test in cv_iter)\n    566 \n    567         # if one choose to see train score, \"out\" will contain train score info\n    568         if self.return_train_score:\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV._fit.<locals>.<genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nAttributeError                                     Thu May  3 16:08:40 2018\nPID: 2464             Python 3.6.4: C:\\Users\\Globalnet\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), make_scorer(log_loss, greater_is_better=False, needs_proba=True), array([ 4,  5,  6,  7,  8,  9, 10]), array([0, 1, 2, 3]), 0, {'subsample': 0.1}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), make_scorer(log_loss, greater_is_better=False, needs_proba=True), array([ 4,  5,  6,  7,  8,  9, 10]), array([0, 1, 2, 3]), 0, {'subsample': 0.1})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), X=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), y=array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), scorer=make_scorer(log_loss, greater_is_better=False, needs_proba=True), train=array([ 4,  5,  6,  7,  8,  9, 10]), test=array([0, 1, 2, 3]), verbose=0, parameters={'subsample': 0.1}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    255                              \" numeric value. (Hint: if using 'raise', please\"\n    256                              \" make sure that it has been spelled correctly.)\")\n    257 \n    258     else:\n    259         fit_time = time.time() - start_time\n--> 260         test_score = _score(estimator, X_test, y_test, scorer)\n        test_score = undefined\n        estimator = XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1)\n        X_test = array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64)\n        y_test = array([ 0.445,  0.403, -0.48 , -0.763])\n        scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n    261         score_time = time.time() - start_time - fit_time\n    262         if return_train_score:\n    263             train_score = _score(estimator, X_train, y_train, scorer)\n    264 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _score(estimator=XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), X_test=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64), y_test=array([ 0.445,  0.403, -0.48 , -0.763]), scorer=make_scorer(log_loss, greater_is_better=False, needs_proba=True))\n    283 def _score(estimator, X_test, y_test, scorer):\n    284     \"\"\"Compute the score of an estimator on a given test set.\"\"\"\n    285     if y_test is None:\n    286         score = scorer(estimator, X_test)\n    287     else:\n--> 288         score = scorer(estimator, X_test, y_test)\n        score = undefined\n        scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n        estimator = XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1)\n        X_test = array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64)\n        y_test = array([ 0.445,  0.403, -0.48 , -0.763])\n    289     if hasattr(score, 'item'):\n    290         try:\n    291             # e.g. unwrap memmapped scalars\n    292             score = score.item()\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py in __call__(self=make_scorer(log_loss, greater_is_better=False, needs_proba=True), clf=XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), X=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64), y=array([ 0.445,  0.403, -0.48 , -0.763]), sample_weight=None)\n    123         score : float\n    124             Score function applied to prediction of estimator on X.\n    125         \"\"\"\n    126         super(_ProbaScorer, self).__call__(clf, X, y,\n    127                                            sample_weight=sample_weight)\n--> 128         y_pred = clf.predict_proba(X)\n        y_pred = undefined\n        clf.predict_proba = undefined\n        X = array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64)\n    129         if sample_weight is not None:\n    130             return self._sign * self._score_func(y, y_pred,\n    131                                                  sample_weight=sample_weight,\n    132                                                  **self._kwargs)\n\nAttributeError: 'XGBRegressor' object has no attribute 'predict_proba'\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 344, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 260, in _fit_and_score\n    test_score = _score(estimator, X_test, y_test, scorer)\n  File \"C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 288, in _score\n    score = scorer(estimator, X_test, y_test)\n  File \"C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\", line 128, in __call__\n    y_pred = clf.predict_proba(X)\nAttributeError: 'XGBRegressor' object has no attribute 'predict_proba'\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\Globalnet\\Anaconda3\\lib\\multiprocessing\\pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 353, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nAttributeError                                     Thu May  3 16:08:40 2018\nPID: 2464             Python 3.6.4: C:\\Users\\Globalnet\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), make_scorer(log_loss, greater_is_better=False, needs_proba=True), array([ 4,  5,  6,  7,  8,  9, 10]), array([0, 1, 2, 3]), 0, {'subsample': 0.1}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), make_scorer(log_loss, greater_is_better=False, needs_proba=True), array([ 4,  5,  6,  7,  8,  9, 10]), array([0, 1, 2, 3]), 0, {'subsample': 0.1})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), X=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), y=array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), scorer=make_scorer(log_loss, greater_is_better=False, needs_proba=True), train=array([ 4,  5,  6,  7,  8,  9, 10]), test=array([0, 1, 2, 3]), verbose=0, parameters={'subsample': 0.1}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    255                              \" numeric value. (Hint: if using 'raise', please\"\n    256                              \" make sure that it has been spelled correctly.)\")\n    257 \n    258     else:\n    259         fit_time = time.time() - start_time\n--> 260         test_score = _score(estimator, X_test, y_test, scorer)\n        test_score = undefined\n        estimator = XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1)\n        X_test = array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64)\n        y_test = array([ 0.445,  0.403, -0.48 , -0.763])\n        scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n    261         score_time = time.time() - start_time - fit_time\n    262         if return_train_score:\n    263             train_score = _score(estimator, X_train, y_train, scorer)\n    264 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _score(estimator=XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), X_test=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64), y_test=array([ 0.445,  0.403, -0.48 , -0.763]), scorer=make_scorer(log_loss, greater_is_better=False, needs_proba=True))\n    283 def _score(estimator, X_test, y_test, scorer):\n    284     \"\"\"Compute the score of an estimator on a given test set.\"\"\"\n    285     if y_test is None:\n    286         score = scorer(estimator, X_test)\n    287     else:\n--> 288         score = scorer(estimator, X_test, y_test)\n        score = undefined\n        scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n        estimator = XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1)\n        X_test = array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64)\n        y_test = array([ 0.445,  0.403, -0.48 , -0.763])\n    289     if hasattr(score, 'item'):\n    290         try:\n    291             # e.g. unwrap memmapped scalars\n    292             score = score.item()\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py in __call__(self=make_scorer(log_loss, greater_is_better=False, needs_proba=True), clf=XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), X=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64), y=array([ 0.445,  0.403, -0.48 , -0.763]), sample_weight=None)\n    123         score : float\n    124             Score function applied to prediction of estimator on X.\n    125         \"\"\"\n    126         super(_ProbaScorer, self).__call__(clf, X, y,\n    127                                            sample_weight=sample_weight)\n--> 128         y_pred = clf.predict_proba(X)\n        y_pred = undefined\n        clf.predict_proba = undefined\n        X = array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64)\n    129         if sample_weight is not None:\n    130             return self._sign * self._score_func(y, y_pred,\n    131                                                  sample_weight=sample_weight,\n    132                                                  **self._kwargs)\n\nAttributeError: 'XGBRegressor' object has no attribute 'predict_proba'\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    681\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;34m'timeout'\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgetfullargspec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 682\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    683\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    643\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 644\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    645\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nAttributeError                                     Thu May  3 16:08:40 2018\nPID: 2464             Python 3.6.4: C:\\Users\\Globalnet\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), make_scorer(log_loss, greater_is_better=False, needs_proba=True), array([ 4,  5,  6,  7,  8,  9, 10]), array([0, 1, 2, 3]), 0, {'subsample': 0.1}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), make_scorer(log_loss, greater_is_better=False, needs_proba=True), array([ 4,  5,  6,  7,  8,  9, 10]), array([0, 1, 2, 3]), 0, {'subsample': 0.1})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), X=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), y=array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), scorer=make_scorer(log_loss, greater_is_better=False, needs_proba=True), train=array([ 4,  5,  6,  7,  8,  9, 10]), test=array([0, 1, 2, 3]), verbose=0, parameters={'subsample': 0.1}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    255                              \" numeric value. (Hint: if using 'raise', please\"\n    256                              \" make sure that it has been spelled correctly.)\")\n    257 \n    258     else:\n    259         fit_time = time.time() - start_time\n--> 260         test_score = _score(estimator, X_test, y_test, scorer)\n        test_score = undefined\n        estimator = XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1)\n        X_test = array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64)\n        y_test = array([ 0.445,  0.403, -0.48 , -0.763])\n        scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n    261         score_time = time.time() - start_time - fit_time\n    262         if return_train_score:\n    263             train_score = _score(estimator, X_train, y_train, scorer)\n    264 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _score(estimator=XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), X_test=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64), y_test=array([ 0.445,  0.403, -0.48 , -0.763]), scorer=make_scorer(log_loss, greater_is_better=False, needs_proba=True))\n    283 def _score(estimator, X_test, y_test, scorer):\n    284     \"\"\"Compute the score of an estimator on a given test set.\"\"\"\n    285     if y_test is None:\n    286         score = scorer(estimator, X_test)\n    287     else:\n--> 288         score = scorer(estimator, X_test, y_test)\n        score = undefined\n        scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n        estimator = XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1)\n        X_test = array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64)\n        y_test = array([ 0.445,  0.403, -0.48 , -0.763])\n    289     if hasattr(score, 'item'):\n    290         try:\n    291             # e.g. unwrap memmapped scalars\n    292             score = score.item()\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py in __call__(self=make_scorer(log_loss, greater_is_better=False, needs_proba=True), clf=XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), X=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64), y=array([ 0.445,  0.403, -0.48 , -0.763]), sample_weight=None)\n    123         score : float\n    124             Score function applied to prediction of estimator on X.\n    125         \"\"\"\n    126         super(_ProbaScorer, self).__call__(clf, X, y,\n    127                                            sample_weight=sample_weight)\n--> 128         y_pred = clf.predict_proba(X)\n        y_pred = undefined\n        clf.predict_proba = undefined\n        X = array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64)\n    129         if sample_weight is not None:\n    130             return self._sign * self._score_func(y, y_pred,\n    131                                                  sample_weight=sample_weight,\n    132                                                  **self._kwargs)\n\nAttributeError: 'XGBRegressor' object has no attribute 'predict_proba'\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJoblibAttributeError\u001b[0m                      Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-102-b14d86ea2afd>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mkfold\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStratifiedKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"neg_log_loss\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mgrid_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;31m# summarize results\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups)\u001b[0m\n\u001b[0;32m    943\u001b[0m             \u001b[0mtrain\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mtest\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m         \"\"\"\n\u001b[1;32m--> 945\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    946\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, groups, parameter_iterable)\u001b[0m\n\u001b[0;32m    562\u001b[0m                                   \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_parameters\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    563\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m--> 564\u001b[1;33m           \u001b[1;32mfor\u001b[0m \u001b[0mparameters\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mparameter_iterable\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    565\u001b[0m           for train, test in cv_iter)\n\u001b[0;32m    566\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    766\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 768\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    769\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    770\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    717\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m                     \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 719\u001b[1;33m                 \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    720\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    721\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJoblibAttributeError\u001b[0m: JoblibAttributeError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\runpy.py in _run_code(code=<code object <module> at 0x0000020ACEF64E40, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\G...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x0000020ACEF64E40, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...nda3\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\G...a3\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n     17 \n     18 \n     19 \n     20 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    473         if self.poller is not None:\n    474             self.poller.start()\n    475         self.kernel.start()\n    476         self.io_loop = ioloop.IOLoop.current()\n    477         try:\n--> 478             self.io_loop.start()\n        self.io_loop.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    479         except KeyboardInterrupt:\n    480             pass\n    481 \n    482 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    172             )\n    173         return loop\n    174     \n    175     def start(self):\n    176         try:\n--> 177             super(ZMQIOLoop, self).start()\n        self.start = <bound method ZMQIOLoop.start of <zmq.eventloop.ioloop.ZMQIOLoop object>>\n    178         except ZMQError as e:\n    179             if e.errno == ETERM:\n    180                 # quietly return on ETERM\n    181                 pass\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\tornado\\ioloop.py in start(self=<zmq.eventloop.ioloop.ZMQIOLoop object>)\n    883                 self._events.update(event_pairs)\n    884                 while self._events:\n    885                     fd, events = self._events.popitem()\n    886                     try:\n    887                         fd_obj, handler_func = self._handlers[fd]\n--> 888                         handler_func(fd_obj, events)\n        handler_func = <function wrap.<locals>.null_wrapper>\n        fd_obj = <zmq.sugar.socket.Socket object>\n        events = 1\n    889                     except (OSError, IOError) as e:\n    890                         if errno_from_exception(e) == errno.EPIPE:\n    891                             # Happens when the client closes the connection\n    892                             pass\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<zmq.sugar.socket.Socket object>, 1), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = (<zmq.sugar.socket.Socket object>, 1)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_events(self=<zmq.eventloop.zmqstream.ZMQStream object>, fd=<zmq.sugar.socket.Socket object>, events=1)\n    435             # dispatch events:\n    436             if events & IOLoop.ERROR:\n    437                 gen_log.error(\"got POLLERR event on ZMQStream, which doesn't make sense\")\n    438                 return\n    439             if events & IOLoop.READ:\n--> 440                 self._handle_recv()\n        self._handle_recv = <bound method ZMQStream._handle_recv of <zmq.eventloop.zmqstream.ZMQStream object>>\n    441                 if not self.socket:\n    442                     return\n    443             if events & IOLoop.WRITE:\n    444                 self._handle_send()\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _handle_recv(self=<zmq.eventloop.zmqstream.ZMQStream object>)\n    467                 gen_log.error(\"RECV Error: %s\"%zmq.strerror(e.errno))\n    468         else:\n    469             if self._recv_callback:\n    470                 callback = self._recv_callback\n    471                 # self._recv_callback = None\n--> 472                 self._run_callback(callback, msg)\n        self._run_callback = <bound method ZMQStream._run_callback of <zmq.eventloop.zmqstream.ZMQStream object>>\n        callback = <function wrap.<locals>.null_wrapper>\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    473                 \n    474         # self.update_state()\n    475         \n    476 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\zmq\\eventloop\\zmqstream.py in _run_callback(self=<zmq.eventloop.zmqstream.ZMQStream object>, callback=<function wrap.<locals>.null_wrapper>, *args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    409         close our socket.\"\"\"\n    410         try:\n    411             # Use a NullContext to ensure that all StackContexts are run\n    412             # inside our blanket exception handler rather than outside.\n    413             with stack_context.NullContext():\n--> 414                 callback(*args, **kwargs)\n        callback = <function wrap.<locals>.null_wrapper>\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    415         except:\n    416             gen_log.error(\"Uncaught exception, closing connection.\",\n    417                           exc_info=True)\n    418             # Close the socket on an uncaught exception from a user callback\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],), **kwargs={})\n    272         # Fast path when there are no active contexts.\n    273         def null_wrapper(*args, **kwargs):\n    274             try:\n    275                 current_state = _state.contexts\n    276                 _state.contexts = cap_contexts[0]\n--> 277                 return fn(*args, **kwargs)\n        args = ([<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>],)\n        kwargs = {}\n    278             finally:\n    279                 _state.contexts = current_state\n    280         null_wrapper._wrapped = True\n    281         return null_wrapper\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatcher(msg=[<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    278         if self.control_stream:\n    279             self.control_stream.on_recv(self.dispatch_control, copy=False)\n    280 \n    281         def make_dispatcher(stream):\n    282             def dispatcher(msg):\n--> 283                 return self.dispatch_shell(stream, msg)\n        msg = [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]\n    284             return dispatcher\n    285 \n    286         for s in self.shell_streams:\n    287             s.on_recv(make_dispatcher(s), copy=False)\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': '# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 5, 3, 15, 8, 39, 239612, tzinfo=tzutc()), 'msg_id': 'a48f4eec55d04e4689926ff2a1eb1ac5', 'msg_type': 'execute_request', 'session': 'd4858a1ad46447ad81f8da6d31e2e658', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': 'a48f4eec55d04e4689926ff2a1eb1ac5', 'msg_type': 'execute_request', 'parent_header': {}})\n    228             self.log.warn(\"Unknown message type: %r\", msg_type)\n    229         else:\n    230             self.log.debug(\"%s: %s\", msg_type, msg)\n    231             self.pre_handler_hook()\n    232             try:\n--> 233                 handler(stream, idents, msg)\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'd4858a1ad46447ad81f8da6d31e2e658']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': '# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 5, 3, 15, 8, 39, 239612, tzinfo=tzutc()), 'msg_id': 'a48f4eec55d04e4689926ff2a1eb1ac5', 'msg_type': 'execute_request', 'session': 'd4858a1ad46447ad81f8da6d31e2e658', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': 'a48f4eec55d04e4689926ff2a1eb1ac5', 'msg_type': 'execute_request', 'parent_header': {}}\n    234             except Exception:\n    235                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    236             finally:\n    237                 self.post_handler_hook()\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'd4858a1ad46447ad81f8da6d31e2e658'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': '# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n', 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2018, 5, 3, 15, 8, 39, 239612, tzinfo=tzutc()), 'msg_id': 'a48f4eec55d04e4689926ff2a1eb1ac5', 'msg_type': 'execute_request', 'session': 'd4858a1ad46447ad81f8da6d31e2e658', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': 'a48f4eec55d04e4689926ff2a1eb1ac5', 'msg_type': 'execute_request', 'parent_header': {}})\n    394         if not silent:\n    395             self.execution_count += 1\n    396             self._publish_execute_input(code, parent, self.execution_count)\n    397 \n    398         reply_content = self.do_execute(code, silent, store_history,\n--> 399                                         user_expressions, allow_stdin)\n        user_expressions = {}\n        allow_stdin = True\n    400 \n    401         # Flush output before sending the reply.\n    402         sys.stdout.flush()\n    403         sys.stderr.flush()\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code='# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n', silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    203 \n    204         self._forward_input(allow_stdin)\n    205 \n    206         reply_content = {}\n    207         try:\n--> 208             res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        shell.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = '# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n'\n        store_history = True\n        silent = False\n    209         finally:\n    210             self._restore_input()\n    211 \n    212         if res.error_before_exec is not None:\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=('# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n',), **kwargs={'silent': False, 'store_history': True})\n    532             )\n    533         self.payload_manager.write_payload(payload)\n    534 \n    535     def run_cell(self, *args, **kwargs):\n    536         self._last_traceback = None\n--> 537         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = ('# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n',)\n        kwargs = {'silent': False, 'store_history': True}\n    538 \n    539     def _showtraceback(self, etype, evalue, stb):\n    540         # try to preserve ordering of tracebacks and print statements\n    541         sys.stdout.flush()\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell='# grid search\\nmodel = XGBRegressor()\\nsubsample =...search.fit(X_train, y_train)\\n# summarize results\\n', store_history=True, silent=False, shell_futures=True)\n   2723                 self.displayhook.exec_result = result\n   2724 \n   2725                 # Execute the user code\n   2726                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   2727                 has_raised = self.run_ast_nodes(code_ast.body, cell_name,\n-> 2728                    interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   2729                 \n   2730                 self.last_execution_succeeded = not has_raised\n   2731                 self.last_execution_result = result\n   2732 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>], cell_name='<ipython-input-102-b14d86ea2afd>', interactivity='none', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 20ad96cd828, executio..._before_exec=None error_in_exec=None result=None>)\n   2845 \n   2846         try:\n   2847             for i, node in enumerate(to_run_exec):\n   2848                 mod = ast.Module([node])\n   2849                 code = compiler(mod, cell_name, \"exec\")\n-> 2850                 if self.run_code(code, result):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x0000020AE3DA6E40, file \"<ipython-input-102-b14d86ea2afd>\", line 7>\n        result = <ExecutionResult object at 20ad96cd828, executio..._before_exec=None error_in_exec=None result=None>\n   2851                     return True\n   2852 \n   2853             for i, node in enumerate(to_run_interactive):\n   2854                 mod = ast.Interactive([node])\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x0000020AE3DA6E40, file \"<ipython-input-102-b14d86ea2afd>\", line 7>, result=<ExecutionResult object at 20ad96cd828, executio..._before_exec=None error_in_exec=None result=None>)\n   2905         outflag = True  # happens in more places, so it's easier as default\n   2906         try:\n   2907             try:\n   2908                 self.hooks.pre_run_code_hook()\n   2909                 #rprint('Running code', repr(code_obj)) # dbg\n-> 2910                 exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x0000020AE3DA6E40, file \"<ipython-input-102-b14d86ea2afd>\", line 7>\n        self.user_global_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'Counter': <class 'collections.Counter'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', 'class TwitterData_Initialize_test(TwitterData_In..., columns=columns)\\n        return self.data_model', 'data = TwitterData_Initialize()\\ndata.initialize(...blog_Trialdata.json\")\\n\\ndata.processed_data.head()', 'data_test = TwitterData_Initialize_test()\\ndata_t...trialtest.json\")\\n\\ndata_test.processed_data.head()', 'from scipy import sparse\\nstart_time = time.time(...Py sparse CSR matrix:\\\\n{}\".format(sparse_matrix))', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', 'class TwitterData_Initialize_test(TwitterData_In..., columns=columns)\\n        return self.data_model', 'data = TwitterData_Initialize()\\ndata.initialize(...blog_Trialdata.json\")\\n\\ndata.processed_data.head()', 'data_test = TwitterData_Initialize_test()\\ndata_t...trialtest.json\")\\n\\ndata_test.processed_data.head()', 'from scipy import sparse\\nstart_time = time.time(...Py sparse CSR matrix:\\\\n{}\".format(sparse_matrix))', \"stop_words = set(stopwords.words('english'))\", 'stop_words', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', 'class TwitterData_Initialize_test(TwitterData_In..., columns=columns)\\n        return self.data_model', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', ...], 'LabelEncoder': <class 'sklearn.preprocessing.label.LabelEncoder'>, 'LancasterStemmer': <class 'nltk.stem.lancaster.LancasterStemmer'>, 'Out': {4:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , 5:   cashtag                  id      source  \\\n0  ...4                                not guaranteed  , 10:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , 11:   cashtag                  id      source  \\\n0  ...4                                not guaranteed  , 14: {'a', 'about', 'above', 'after', 'again', 'against', ...}, 21:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , 22:   cashtag                  id      source  \\\n0  ...4                                not guaranteed  , 23:    cashtag                  id  sentiment score ...ot Putting on a non little didn't $F short not]  , 25:    cashtag                  id  sentiment score ...              [put, non, little, n't, f, short]  , 28:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , ...}, 'PorterStemmer': <class 'nltk.stem.porter.PorterStemmer'>, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'SnowballStemmer': <class 'nltk.stem.snowball.SnowballStemmer'>, ...}\n        self.user_ns = {'CountVectorizer': <class 'sklearn.feature_extraction.text.CountVectorizer'>, 'Counter': <class 'collections.Counter'>, 'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', 'class TwitterData_Initialize_test(TwitterData_In..., columns=columns)\\n        return self.data_model', 'data = TwitterData_Initialize()\\ndata.initialize(...blog_Trialdata.json\")\\n\\ndata.processed_data.head()', 'data_test = TwitterData_Initialize_test()\\ndata_t...trialtest.json\")\\n\\ndata_test.processed_data.head()', 'from scipy import sparse\\nstart_time = time.time(...Py sparse CSR matrix:\\\\n{}\".format(sparse_matrix))', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', 'class TwitterData_Initialize_test(TwitterData_In..., columns=columns)\\n        return self.data_model', 'data = TwitterData_Initialize()\\ndata.initialize(...blog_Trialdata.json\")\\n\\ndata.processed_data.head()', 'data_test = TwitterData_Initialize_test()\\ndata_t...trialtest.json\")\\n\\ndata_test.processed_data.head()', 'from scipy import sparse\\nstart_time = time.time(...Py sparse CSR matrix:\\\\n{}\".format(sparse_matrix))', \"stop_words = set(stopwords.words('english'))\", 'stop_words', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', 'class TwitterData_Initialize_test(TwitterData_In..., columns=columns)\\n        return self.data_model', 'import nltk\\nfrom collections import Counter\\nimpo...cross_val_score, GridSearchCV, RandomizedSearchCV', 'class TwitterData_Initialize():\\n    data = []\\n  ...\\n        return self.data_model, self.data_labels', ...], 'LabelEncoder': <class 'sklearn.preprocessing.label.LabelEncoder'>, 'LancasterStemmer': <class 'nltk.stem.lancaster.LancasterStemmer'>, 'Out': {4:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , 5:   cashtag                  id      source  \\\n0  ...4                                not guaranteed  , 10:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , 11:   cashtag                  id      source  \\\n0  ...4                                not guaranteed  , 14: {'a', 'about', 'above', 'after', 'again', 'against', ...}, 21:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , 22:   cashtag                  id      source  \\\n0  ...4                                not guaranteed  , 23:    cashtag                  id  sentiment score ...ot Putting on a non little didn't $F short not]  , 25:    cashtag                  id  sentiment score ...              [put, non, little, n't, f, short]  , 28:   cashtag        id  sentiment score      source...g Position]  \n4        [its time to sell banks]  , ...}, 'PorterStemmer': <class 'nltk.stem.porter.PorterStemmer'>, 'RandomizedSearchCV': <class 'sklearn.model_selection._search.RandomizedSearchCV'>, 'SnowballStemmer': <class 'nltk.stem.snowball.SnowballStemmer'>, ...}\n   2911             finally:\n   2912                 # Reset our crash handler in place\n   2913                 sys.excepthook = old_excepthook\n   2914         except SystemExit as e:\n\n...........................................................................\nC:\\Users\\Globalnet\\Desktop\\moi\\<ipython-input-102-b14d86ea2afd> in <module>()\n      2 model = XGBRegressor()\n      3 subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n      4 param_grid = dict(subsample=subsample)\n      5 kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=7)\n      6 grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1)\n----> 7 grid_result = grid_search.fit(X_train, y_train)\n      8 # summarize results\n      9 \n     10 \n     11 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...e=True,\n       scoring='neg_log_loss', verbose=0), X=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), y=array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), groups=None)\n    940 \n    941         groups : array-like, with shape (n_samples,), optional\n    942             Group labels for the samples used while splitting the dataset into\n    943             train/test set.\n    944         \"\"\"\n--> 945         return self._fit(X, y, groups, ParameterGrid(self.param_grid))\n        self._fit = <bound method BaseSearchCV._fit of GridSearchCV(...=True,\n       scoring='neg_log_loss', verbose=0)>\n        X = array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64)\n        y = array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483])\n        groups = None\n        self.param_grid = {'subsample': [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]}\n    946 \n    947 \n    948 class RandomizedSearchCV(BaseSearchCV):\n    949     \"\"\"Randomized search on hyper parameters.\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py in _fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...e=True,\n       scoring='neg_log_loss', verbose=0), X=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), y=array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), groups=None, parameter_iterable=<sklearn.model_selection._search.ParameterGrid object>)\n    559                                   fit_params=self.fit_params,\n    560                                   return_train_score=self.return_train_score,\n    561                                   return_n_test_samples=True,\n    562                                   return_times=True, return_parameters=True,\n    563                                   error_score=self.error_score)\n--> 564           for parameters in parameter_iterable\n        parameters = undefined\n        parameter_iterable = <sklearn.model_selection._search.ParameterGrid object>\n    565           for train, test in cv_iter)\n    566 \n    567         # if one choose to see train score, \"out\" will contain train score info\n    568         if self.return_train_score:\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV._fit.<locals>.<genexpr>>)\n    763             if pre_dispatch == \"all\" or n_jobs == 1:\n    764                 # The iterable was consumed all at once by the above for loop.\n    765                 # No need to wait for async callbacks to trigger to\n    766                 # consumption.\n    767                 self._iterating = False\n--> 768             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    769             # Make sure that we get a last message telling us we are done\n    770             elapsed_time = time.time() - self._start_time\n    771             self._print('Done %3i out of %3i | elapsed: %s finished',\n    772                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nAttributeError                                     Thu May  3 16:08:40 2018\nPID: 2464             Python 3.6.4: C:\\Users\\Globalnet\\Anaconda3\\python.exe\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), make_scorer(log_loss, greater_is_better=False, needs_proba=True), array([ 4,  5,  6,  7,  8,  9, 10]), array([0, 1, 2, 3]), 0, {'subsample': 0.1}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), make_scorer(log_loss, greater_is_better=False, needs_proba=True), array([ 4,  5,  6,  7,  8,  9, 10]), array([0, 1, 2, 3]), 0, {'subsample': 0.1})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': True, 'return_times': True, 'return_train_score': True}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), X=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 1, 1, 0, 0, 0,\n        0]], dtype=int64), y=array([ 0.445,  0.403, -0.48 , -0.763,  0.653, -..., -0.454, -0.464,\n        0.46 ,  0.   ,  0.483]), scorer=make_scorer(log_loss, greater_is_better=False, needs_proba=True), train=array([ 4,  5,  6,  7,  8,  9, 10]), test=array([0, 1, 2, 3]), verbose=0, parameters={'subsample': 0.1}, fit_params={}, return_train_score=True, return_parameters=True, return_n_test_samples=True, return_times=True, error_score='raise')\n    255                              \" numeric value. (Hint: if using 'raise', please\"\n    256                              \" make sure that it has been spelled correctly.)\")\n    257 \n    258     else:\n    259         fit_time = time.time() - start_time\n--> 260         test_score = _score(estimator, X_test, y_test, scorer)\n        test_score = undefined\n        estimator = XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1)\n        X_test = array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64)\n        y_test = array([ 0.445,  0.403, -0.48 , -0.763])\n        scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n    261         score_time = time.time() - start_time - fit_time\n    262         if return_train_score:\n    263             train_score = _score(estimator, X_train, y_train, scorer)\n    264 \n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _score(estimator=XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), X_test=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64), y_test=array([ 0.445,  0.403, -0.48 , -0.763]), scorer=make_scorer(log_loss, greater_is_better=False, needs_proba=True))\n    283 def _score(estimator, X_test, y_test, scorer):\n    284     \"\"\"Compute the score of an estimator on a given test set.\"\"\"\n    285     if y_test is None:\n    286         score = scorer(estimator, X_test)\n    287     else:\n--> 288         score = scorer(estimator, X_test, y_test)\n        score = undefined\n        scorer = make_scorer(log_loss, greater_is_better=False, needs_proba=True)\n        estimator = XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1)\n        X_test = array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64)\n        y_test = array([ 0.445,  0.403, -0.48 , -0.763])\n    289     if hasattr(score, 'item'):\n    290         try:\n    291             # e.g. unwrap memmapped scalars\n    292             score = score.item()\n\n...........................................................................\nC:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py in __call__(self=make_scorer(log_loss, greater_is_better=False, needs_proba=True), clf=XGBRegressor(base_score=0.5, booster='gbtree', c...=1, seed=None,\n       silent=True, subsample=0.1), X=array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64), y=array([ 0.445,  0.403, -0.48 , -0.763]), sample_weight=None)\n    123         score : float\n    124             Score function applied to prediction of estimator on X.\n    125         \"\"\"\n    126         super(_ProbaScorer, self).__call__(clf, X, y,\n    127                                            sample_weight=sample_weight)\n--> 128         y_pred = clf.predict_proba(X)\n        y_pred = undefined\n        clf.predict_proba = undefined\n        X = array([[0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0...0, 0, 0, 0, 0, 0, 0, 0,\n        0]], dtype=int64)\n    129         if sample_weight is not None:\n    130             return self._sign * self._score_func(y, y_pred,\n    131                                                  sample_weight=sample_weight,\n    132                                                  **self._kwargs)\n\nAttributeError: 'XGBRegressor' object has no attribute 'predict_proba'\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "# grid search\n",
    "model = XGBRegressor()\n",
    "subsample = [0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 1.0]\n",
    "param_grid = dict(subsample=subsample)\n",
    "kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=7)\n",
    "grid_search = GridSearchCV(model, param_grid, scoring=\"neg_log_loss\", n_jobs=-1)\n",
    "grid_result = grid_search.fit(X_train, y_train)\n",
    "# summarize results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
      "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
      "       n_jobs=1, nthread=None, objective='reg:linear', random_state=0,\n",
      "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
      "       silent=True, subsample=1)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Fitting XGB regressor \n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "print (model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.132158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.153285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.179562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.219891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.018351</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.133477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.133477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.133477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.133477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.116063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.133477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.191882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.133477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.240012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.081196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.232836</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.133477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.133477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.133477</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.167556</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Prediction\n",
       "0     0.132158\n",
       "1     0.153285\n",
       "2     0.179562\n",
       "3    -0.219891\n",
       "4     0.018351\n",
       "5     0.133477\n",
       "6     0.133477\n",
       "7     0.133477\n",
       "8     0.133477\n",
       "9     0.116063\n",
       "10    0.133477\n",
       "11    0.191882\n",
       "12    0.133477\n",
       "13    0.240012\n",
       "14    0.081196\n",
       "15    0.232836\n",
       "16    0.133477\n",
       "17    0.133477\n",
       "18    0.133477\n",
       "19    0.167556"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict \n",
    "output = model.predict(X_test)\n",
    "final_df = pd.DataFrame()\n",
    "#final_df[\"ID\"] = id_vals\n",
    "#final_df[\"cashtag\"]=row2\n",
    "\n",
    "#final_df[\"spans\"]=row\n",
    "final_df[\"Prediction\"] = output\n",
    "#final_df.to_csv(\"Output_1.csv\",sep=\",\")\n",
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.10838428827316153\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "acc=mean_squared_error(output, y_test)\n",
    "\n",
    "print(\"Accuracy:\" ,acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 2.50732421875 seconds ---\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "      <th>token_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$F</td>\n",
       "      <td>5540055</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[putting on a little $f short]</td>\n",
       "      <td>[put, little, f, short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>10752226</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short som]</td>\n",
       "      <td>[short, som]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$BAC</td>\n",
       "      <td>10920221</td>\n",
       "      <td>0.445</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[buying opportun]</td>\n",
       "      <td>[buying, opportun]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$SHOR</td>\n",
       "      <td>12971398</td>\n",
       "      <td>0.661</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[scaling up on long posit]</td>\n",
       "      <td>[scale, up, long, posit]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$JPM</td>\n",
       "      <td>16142438</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[its time to sell bank]</td>\n",
       "      <td>[time, sell, bank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$LMT</td>\n",
       "      <td>14073133</td>\n",
       "      <td>0.627</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[entering long]</td>\n",
       "      <td>[enter, long]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$DNN</td>\n",
       "      <td>18479024</td>\n",
       "      <td>0.653</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[picked some up]</td>\n",
       "      <td>[picked, up]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>0.668</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[time to accumulate for a long posit, far more...</td>\n",
       "      <td>[time, accumulate, long, posit, far, upside, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>0.460</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[looking for a strong bounc, lunchtime rally com]</td>\n",
       "      <td>[look, strong, bounc, lunchtime, rally, com]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>0.403</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[very intrigued with the technology and growth...</td>\n",
       "      <td>[intrigue, technology, growth, potenti]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>0.000</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short2 48 + - ***work, puts up]</td>\n",
       "      <td>[short, ***work, put, up]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[biggest market los]</td>\n",
       "      <td>[big, market, los]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[$goog $googl would suck]</td>\n",
       "      <td>[goog, googl, would, suck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>$SBUX</td>\n",
       "      <td>719890387314335744</td>\n",
       "      <td>0.483</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[buying $sbux on dip]</td>\n",
       "      <td>[buying, sbux, dip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>$GOOGL</td>\n",
       "      <td>708668814427348992</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[is a short below 740, and is overbought]</td>\n",
       "      <td>[short, below, overbought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>$F</td>\n",
       "      <td>5540055</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[not putting on a non little didn't $f short not]</td>\n",
       "      <td>[not, put, non, little, n't, f, short, not]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cashtag                  id  sentiment score      source  \\\n",
       "0       $F             5540055           -0.454  stocktwits   \n",
       "1    $AAPL            10752226           -0.464  stocktwits   \n",
       "2     $BAC            10920221            0.445  stocktwits   \n",
       "3    $SHOR            12971398            0.661  stocktwits   \n",
       "4     $JPM            16142438           -0.763  stocktwits   \n",
       "5     $LMT            14073133            0.627  stocktwits   \n",
       "6     $DNN            18479024            0.653  stocktwits   \n",
       "7     $CRK            34147106            0.668  stocktwits   \n",
       "8     $CRK            34147106            0.460  stocktwits   \n",
       "9     $CRK            34147106            0.403  stocktwits   \n",
       "10    $CRK            34147106            0.000  stocktwits   \n",
       "11    $CRK            34147106           -0.438  stocktwits   \n",
       "12    $CRK            34147106           -0.398  stocktwits   \n",
       "13   $SBUX  719890387314335744            0.483     twitter   \n",
       "14  $GOOGL  708668814427348992           -0.480     twitter   \n",
       "15      $F             5540055           -0.454  stocktwits   \n",
       "\n",
       "                                                spans  \\\n",
       "0                      [putting on a little $f short]   \n",
       "1                                         [short som]   \n",
       "2                                   [buying opportun]   \n",
       "3                          [scaling up on long posit]   \n",
       "4                             [its time to sell bank]   \n",
       "5                                     [entering long]   \n",
       "6                                    [picked some up]   \n",
       "7   [time to accumulate for a long posit, far more...   \n",
       "8   [looking for a strong bounc, lunchtime rally com]   \n",
       "9   [very intrigued with the technology and growth...   \n",
       "10                   [short2 48 + - ***work, puts up]   \n",
       "11                               [biggest market los]   \n",
       "12                          [$goog $googl would suck]   \n",
       "13                              [buying $sbux on dip]   \n",
       "14          [is a short below 740, and is overbought]   \n",
       "15  [not putting on a non little didn't $f short not]   \n",
       "\n",
       "                                          token_spans  \n",
       "0                             [put, little, f, short]  \n",
       "1                                        [short, som]  \n",
       "2                                  [buying, opportun]  \n",
       "3                            [scale, up, long, posit]  \n",
       "4                                  [time, sell, bank]  \n",
       "5                                       [enter, long]  \n",
       "6                                        [picked, up]  \n",
       "7   [time, accumulate, long, posit, far, upside, d...  \n",
       "8        [look, strong, bounc, lunchtime, rally, com]  \n",
       "9             [intrigue, technology, growth, potenti]  \n",
       "10                          [short, ***work, put, up]  \n",
       "11                                 [big, market, los]  \n",
       "12                         [goog, googl, would, suck]  \n",
       "13                                [buying, sbux, dip]  \n",
       "14                         [short, below, overbought]  \n",
       "15        [not, put, non, little, n't, f, short, not]  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['short',\n",
       " 'put',\n",
       " 'up',\n",
       " 'long',\n",
       " 'little',\n",
       " 'f',\n",
       " 'buying',\n",
       " 'posit',\n",
       " 'time',\n",
       " 'not',\n",
       " 'som',\n",
       " 'opportun',\n",
       " 'scale',\n",
       " 'sell',\n",
       " 'bank',\n",
       " 'enter',\n",
       " 'picked',\n",
       " 'accumulate',\n",
       " 'far',\n",
       " 'upside',\n",
       " 'downsid',\n",
       " 'look',\n",
       " 'strong',\n",
       " 'bounc',\n",
       " 'lunchtime',\n",
       " 'rally',\n",
       " 'com',\n",
       " 'intrigue',\n",
       " 'technology',\n",
       " 'growth',\n",
       " 'potenti',\n",
       " '***work',\n",
       " 'big',\n",
       " 'market',\n",
       " 'los',\n",
       " 'goog',\n",
       " 'googl',\n",
       " 'would',\n",
       " 'suck',\n",
       " 'sbux',\n",
       " 'dip',\n",
       " 'below',\n",
       " 'overbought',\n",
       " 'non',\n",
       " \"n't\"]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1770"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "42.14956432827035"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time.clock()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a',\n",
       " 'about',\n",
       " 'above',\n",
       " 'after',\n",
       " 'again',\n",
       " 'against',\n",
       " 'ain',\n",
       " 'all',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'any',\n",
       " 'are',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'because',\n",
       " 'been',\n",
       " 'before',\n",
       " 'being',\n",
       " 'below',\n",
       " 'between',\n",
       " 'both',\n",
       " 'but',\n",
       " 'by',\n",
       " 'can',\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'd',\n",
       " 'did',\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'do',\n",
       " 'does',\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'doing',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'down',\n",
       " 'during',\n",
       " 'each',\n",
       " 'few',\n",
       " 'for',\n",
       " 'from',\n",
       " 'further',\n",
       " 'had',\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'has',\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'have',\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'having',\n",
       " 'he',\n",
       " 'her',\n",
       " 'here',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'him',\n",
       " 'himself',\n",
       " 'his',\n",
       " 'how',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'into',\n",
       " 'is',\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'just',\n",
       " 'll',\n",
       " 'm',\n",
       " 'ma',\n",
       " 'me',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'more',\n",
       " 'most',\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'my',\n",
       " 'myself',\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'now',\n",
       " 'o',\n",
       " 'of',\n",
       " 'off',\n",
       " 'on',\n",
       " 'once',\n",
       " 'only',\n",
       " 'or',\n",
       " 'other',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'out',\n",
       " 'over',\n",
       " 'own',\n",
       " 're',\n",
       " 's',\n",
       " 'same',\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'so',\n",
       " 'some',\n",
       " 'such',\n",
       " 't',\n",
       " 'than',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'the',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'then',\n",
       " 'there',\n",
       " 'these',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'through',\n",
       " 'to',\n",
       " 'too',\n",
       " 'under',\n",
       " 'until',\n",
       " 'up',\n",
       " 've',\n",
       " 'very',\n",
       " 'was',\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'we',\n",
       " 'were',\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'what',\n",
       " 'when',\n",
       " 'where',\n",
       " 'which',\n",
       " 'while',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'why',\n",
       " 'will',\n",
       " 'with',\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\",\n",
       " 'y',\n",
       " 'you',\n",
       " \"you'd\",\n",
       " \"you'll\",\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves'}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
