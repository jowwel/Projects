{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import time\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize,blankline_tokenize\n",
    "from nltk import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import json\n",
    "import re as regex\n",
    "import xgboost as xgb\n",
    "from sklearn import model_selection, preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_Initialize():\n",
    "    data = []\n",
    "    processed_data = []\n",
    "    wordlist = []\n",
    "\n",
    "    featureList = []\n",
    "    fea_vect=[]\n",
    "    \n",
    "    data_model = None\n",
    "    data_labels = None\n",
    "    is_testing = False\n",
    "    \n",
    "    def initialize(self, csv_file, is_testing_set=False, from_cached=None):\n",
    "        if from_cached is not None:\n",
    "            #self.data_model = pd.read_csv(from_cached)\n",
    "            self.data_model = pd.read_json(from_cached)\n",
    "\n",
    "            return\n",
    "\n",
    "        self.is_testing = is_testing_set\n",
    "\n",
    "        if not is_testing_set:\n",
    "            #self.data = pd.read_csv(csv_file, header=0, names=[\"id\", \"emotion\", \"text\"])\n",
    "            self.data = pd.read_json(csv_file)\n",
    "\n",
    "            #self.data = self.data[self.data[\"emotion\"].isin([\"positive\", \"negative\", \"neutral\"])]\n",
    "        \n",
    "\n",
    "        self.processed_data = self.data\n",
    "        self.wordlist = []\n",
    "        self.data_model = None\n",
    "        self.data_labels = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    \n",
    "    def do_process(self):\n",
    "        start_time = time.time()\n",
    "        def stem_and_join(row,stemmer=nltk.PorterStemmer()):\n",
    "            row[\"spans\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"spans\"]))\n",
    "            return row\n",
    "    \n",
    "        def tokenize_grams(row):\n",
    "                \n",
    "                # turn a doc into clean tokens\n",
    "                def clean_doc(doc):\n",
    "                    # split into tokens by white space\n",
    "                    tokens = doc.split()\n",
    "                    # remove punctuation from each token\n",
    "                    table = str.maketrans('', '', string.punctuation)\n",
    "                    tokens = [w.translate(table) for w in tokens]\n",
    "                    # remove remaining tokens that are not alphabetic\n",
    "                    tokens = [word for word in tokens if word.isalpha()]\n",
    "                    # filter out stop words\n",
    "                    #stop_words = set(stopwords.words('english'))\n",
    "                    \n",
    "                    #tokens = [w for w in tokens if not w in stop_words]\n",
    "                    \n",
    "                    \n",
    "                    # filter out short tokens\n",
    "                    tokens = [word.lower() for word in tokens if len(word) > 2]\n",
    "                    return tokens\n",
    "                \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "               \n",
    "                # Function to apply lemmatization to a list of words\n",
    "                def words_lemmatizer(words, encoding=\"utf8\"):\n",
    "                    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "                    lemma_words = []\n",
    "                    wl = WordNetLemmatizer()\n",
    "                    for word in words:\n",
    "                        pos = find_pos(word)\n",
    "                        lemma_words.append(wl.lemmatize(word, pos))\n",
    "                    return lemma_words\n",
    "                \n",
    "                # Function to find part of speech tag for a word\n",
    "                def find_pos(word):\n",
    "                    # Part of Speech constants\n",
    "                    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "   \n",
    "                    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "                    # Adjective tags - 'JJ', 'JJR', 'JJS'\n",
    "                    if pos.lower()[0] == 'j':\n",
    "                        return 'a'\n",
    "                    # Adverb tags - 'RB', 'RBR', 'RBS'\n",
    "                    elif pos.lower()[0] == 'r':\n",
    "                        return 'r'\n",
    "                    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "                    elif pos.lower()[0] == 'v':\n",
    "                        return 'v'\n",
    "                    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'\n",
    "                    else:\n",
    "                        return 'n'\n",
    "                    \n",
    "                    \n",
    "                def get_ngrams(words,n):\n",
    "                    n_grams = ngrams(words, n)\n",
    "                    \n",
    "                    list_grams=[ ' '.join(grams) for grams in n_grams]\n",
    "                    return list_grams\n",
    "    \n",
    "            \n",
    "                #convert to string\n",
    "                idx=row[\"spans\"]\n",
    "                #ch=\"\".join(x for x in idx if x)\n",
    "                ch=' '.join(idx)\n",
    "               \n",
    "    \n",
    "                \n",
    "                \n",
    "                n_grams=clean_doc(ch)\n",
    "                #tokens=words_lemmatizer(n_grams)\n",
    "                #print(\"nombre of tokens\",len(n_grams))\n",
    "\n",
    "                row[\"token_spans\"] = n_grams\n",
    "    \n",
    "                return row\n",
    "        #self.processed_data = self.processed_data.apply(stem_and_join, axis=1)\n",
    "        self.processed_data = self.processed_data.apply(tokenize_grams, axis=1)\n",
    "        print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "\n",
    "    \n",
    "\n",
    "    def build_wordlist(self, min_occurrences=0, max_occurences=500, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                       ):\n",
    "       \n",
    "        self.wordlist = []\n",
    "        whitelist=[\"to\",\"on\",\"for\",\"up\",\"below\",\"short\",\"long\"]\n",
    "        \n",
    "        #stopwords=[]\n",
    "       \n",
    "        words = Counter()\n",
    "        \n",
    "        for idx in self.processed_data.index:\n",
    "            words.update(self.processed_data.loc[idx, \"token_spans\"])\n",
    "\n",
    "       \n",
    "            \n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "                \n",
    "                \n",
    "        \n",
    "                \n",
    "        \n",
    "        \n",
    "        print(words.most_common())        \n",
    "        \n",
    "        word_df = pd.DataFrame(data={\"word\": [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                     \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                               columns=[\"word\", \"occurrences\"])\n",
    "\n",
    "        word_df.to_csv(\"wordlist_copie.csv\", index_label=\"idx\")\n",
    "        self.wordlist = [k for k, v in (words.most_common()) if min_occurrences < v < max_occurences]\n",
    "        \n",
    "        \n",
    "        \n",
    "    def build_data_model(self):\n",
    "        label_column = []\n",
    "        Id_column=[\"ID\"]\n",
    "        label_column = [\"label\"]\n",
    "\n",
    "        columns = Id_column + label_column + list(\n",
    "            map(lambda w: w ,self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "\n",
    "            if True:\n",
    "                # add label\n",
    "                current_label = self.processed_data.loc[idx, \"sentiment score\"]\n",
    "                current_id = self.processed_data.loc[idx, \"id\"]\n",
    "                \n",
    "                labels.append(current_id)\n",
    "                labels.append(current_label)\n",
    "                \n",
    "                current_row.append(current_id)\n",
    "                current_row.append(current_label)\n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"token_spans\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        self.data_labels = pd.Series(labels)\n",
    "        return self.data_model, self.data_labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterData_Initialize_test(TwitterData_Initialize):\n",
    "\n",
    "    \n",
    "    \n",
    "    def do_process(self):\n",
    "        def stem_and_join(row,stemmer=nltk.PorterStemmer()):\n",
    "            \n",
    "            row[\"spans\"] = list(map(lambda str: stemmer.stem(str.lower()), row[\"spans\"]))\n",
    "            return row\n",
    "    \n",
    "        def tokenize_grams(row):\n",
    "                \n",
    "                # Function to remove stop words\n",
    "                def remove_stopwords(text, lang='english'):\n",
    "                    whitelist = [\"n't\",\"not\",\"below\"]    \n",
    "\n",
    "                    stop_words = set(stopwords.words('english'))\n",
    "                    word_tokens = word_tokenize(text)\n",
    "                    #filtered_sentence = [w for w in word_tokens if ((not w in stop_words) or (w in whitelist))]\n",
    "                    filtered_sentence = []\n",
    "                    for w in word_tokens:\n",
    "                        if ((w not in stop_words) or (w in whitelist)):\n",
    "                            filtered_sentence.append(w)\n",
    "                \n",
    "                    ch=\" \".join(filtered_sentence)\n",
    "\n",
    "                    return ch\n",
    "#40\n",
    "\n",
    "                def clean_doc(doc):\n",
    "                    # split into tokens by white space\n",
    "                    tokens = doc.split()\n",
    "                    # remove punctuation from each token\n",
    "                    table = str.maketrans('', '', string.punctuation)\n",
    "                    tokens = [w.translate(table) for w in tokens]\n",
    "                    # remove remaining tokens that are not alphabetic\n",
    "                    tokens = [word for word in tokens if word.isalpha()]\n",
    "                    # filter out stop words\n",
    "                    #stop_words = set(stopwords.words('english'))\n",
    "                    #tokens = [w for w in tokens if not w in stop_words]\n",
    "                    \n",
    "                    # filter out short tokens\n",
    "                    tokens = [word.lower() for word in tokens if len(word) > 2]\n",
    "                    return tokens\n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "                # Function to apply lemmatization to a list of words\n",
    "                def words_lemmatizer(words, encoding=\"utf8\"):\n",
    "                    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "                    lemma_words = []\n",
    "                    wl = WordNetLemmatizer()\n",
    "                    for word in words:\n",
    "                        pos = find_pos(word)\n",
    "                        lemma_words.append(wl.lemmatize(word, pos))\n",
    "                    return lemma_words\n",
    "\n",
    "                # Function to find part of speech tag for a word\n",
    "                def find_pos(word):\n",
    "                    # Part of Speech constants\n",
    "                    # ADJ, ADJ_SAT, ADV, NOUN, VERB = 'a', 's', 'r', 'n', 'v'\n",
    "   \n",
    "                    pos = nltk.pos_tag(nltk.word_tokenize(word))[0][1]\n",
    "                    # Adjective tags - 'JJ', 'JJR', 'JJS'\n",
    "                    if pos.lower()[0] == 'j':\n",
    "                        return 'a'\n",
    "                    # Adverb tags - 'RB', 'RBR', 'RBS'\n",
    "                    elif pos.lower()[0] == 'r':\n",
    "                        return 'r'\n",
    "                    # Verb tags - 'VB', 'VBD', 'VBG', 'VBN', 'VBP', 'VBZ'\n",
    "                    elif pos.lower()[0] == 'v':\n",
    "                        return 'v'\n",
    "                    # Noun tags - 'NN', 'NNS', 'NNP', 'NNPS'\n",
    "                    else:\n",
    "                        return 'n'\n",
    "                    \n",
    "                def get_ngrams(words,n):\n",
    "                    n_grams = ngrams(words, n)\n",
    "                    \n",
    "                    list_grams=[ ' '.join(grams) for grams in n_grams]\n",
    "                    return list_grams\n",
    "    \n",
    "            \n",
    "                token=[]\n",
    "                idx=row[\"spans\"]\n",
    "                ch=' '.join(idx)\n",
    "\n",
    "\n",
    "    \n",
    "                #words_stemmer(ch, type=\"PorterStemmer\", lang=\"english\", encoding=\"utf8\")\n",
    "                #Convert to lower case\n",
    "                n_grams=clean_doc(ch)\n",
    "                #tokens=words_lemmatizer(n_grams)\n",
    "\n",
    "                #print(\"list ngrams--------------\\n\",n_grams)\n",
    "                row[\"token_spans\"] = n_grams\n",
    "    \n",
    "                return row\n",
    "        #self.processed_data = self.processed_data.apply(stem_and_join, axis=1)\n",
    "        self.processed_data = self.processed_data.apply(tokenize_grams, axis=1)\n",
    "        \n",
    "    def build_wordlist(self, min_occurrences=0, max_occurences=500, stopwords=nltk.corpus.stopwords.words(\"english\"),\n",
    "                      ):\n",
    "        \n",
    "        whitelist=[\"to\",\"on\",\"for\",\"up\",\"below\",\"short\",\"long\"]\n",
    "        self.wordlist = []\n",
    "        #stopwords=[]\n",
    "        \n",
    "        \n",
    "        words = Counter()\n",
    "        \n",
    "        \n",
    "        for idx in self.processed_data.index:\n",
    "            words.update(self.processed_data.loc[idx, \"token_spans\"])\n",
    "\n",
    "        \n",
    "            \n",
    "        for idx, stop_word in enumerate(stopwords):\n",
    "            if stop_word not in whitelist:\n",
    "                del words[stop_word]\n",
    "                \n",
    "                \n",
    "        \n",
    "                \n",
    "        \n",
    "        \n",
    "        print(words.most_common())        \n",
    "\n",
    "        word_df = pd.DataFrame(data={\"word\": [k for k, v in words.most_common() if min_occurrences < v < max_occurences],\n",
    "                                     \"occurrences\": [v for k, v in words.most_common() if min_occurrences < v < max_occurences]},\n",
    "                               columns=[\"word\", \"occurrences\"])\n",
    "\n",
    "        word_df.to_csv(\"wordlist_test.csv\", index_label=\"idx\")\n",
    "        self.wordlist = [k for k, v in (words.most_common()) if min_occurrences < v < max_occurences]\n",
    "        \n",
    "        \n",
    "    def build_data_model(self):\n",
    "        \n",
    "        label_id = [\"ID\"]\n",
    "\n",
    "        columns = label_id + list(\n",
    "            map(lambda w: w ,self.wordlist))\n",
    "        labels = []\n",
    "        rows = []\n",
    "        for idx in self.processed_data.index:\n",
    "            current_row = []\n",
    "            if True:\n",
    "                # add label\n",
    "                current_id = self.processed_data.loc[idx, \"id\"]\n",
    "                \n",
    "                labels.append(current_id)\n",
    "                \n",
    "                current_row.append(current_id)\n",
    "\n",
    "            \n",
    "\n",
    "            # add bag-of-words\n",
    "            tokens = set(self.processed_data.loc[idx, \"token_spans\"])\n",
    "            for _, word in enumerate(self.wordlist):\n",
    "                current_row.append(1 if word in tokens else 0)\n",
    "\n",
    "            rows.append(current_row)\n",
    "\n",
    "        self.data_model = pd.DataFrame(rows, columns=columns)\n",
    "        return self.data_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$FB</td>\n",
       "      <td>719659409228451840</td>\n",
       "      <td>0.366</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[watching for bounce tomorrow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$LUV</td>\n",
       "      <td>719904304207962112</td>\n",
       "      <td>0.638</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[record number of passengers served in 2015]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$NFLX</td>\n",
       "      <td>5329774</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[out $NFLX -.35]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$DIA</td>\n",
       "      <td>719891468173844480</td>\n",
       "      <td>0.460</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Looking for a strong bounce, Lunchtime rally ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$PLUG</td>\n",
       "      <td>20091246</td>\n",
       "      <td>0.403</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Very intrigued with the technology and growth...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$GMCR</td>\n",
       "      <td>5819749</td>\n",
       "      <td>0.000</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short worked, puts up]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$IBM</td>\n",
       "      <td>709741154393133056</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[overbought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$JOSB</td>\n",
       "      <td>17892972</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[absolute garbage still up, stores TOTALLY EMP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$CSTM</td>\n",
       "      <td>709834259687710720</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Biggest Market Losers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>$PYPL</td>\n",
       "      <td>708481442079068160</td>\n",
       "      <td>0.408</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Love this company long time.]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>$GOOGL</td>\n",
       "      <td>31971935</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[$GOOG $GOOGL would suck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>$ENDP</td>\n",
       "      <td>710187873492983808</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[who won't pay anymore, REAL risk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>$XLI</td>\n",
       "      <td>13915103</td>\n",
       "      <td>0.025</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[No edge offered]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>$PCLN</td>\n",
       "      <td>10448993</td>\n",
       "      <td>0.486</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[runs into the 50sma on the acquisition news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>$AA</td>\n",
       "      <td>24886266</td>\n",
       "      <td>0.308</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[t can't go down]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>12793642</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[now seems like its helping the downtrend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>9408369</td>\n",
       "      <td>0.461</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[mastered their supply chain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>$GOLD</td>\n",
       "      <td>719909604654624768</td>\n",
       "      <td>0.408</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Most bullish stocks on Twitter during this dip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>$AMD</td>\n",
       "      <td>9674584</td>\n",
       "      <td>-0.699</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[big dumping, would not touch it for a while]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>$SPY</td>\n",
       "      <td>10041008</td>\n",
       "      <td>0.495</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[trade continuing very nicely from yesterday, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cashtag                  id  sentiment score      source  \\\n",
       "0      $FB  719659409228451840            0.366     twitter   \n",
       "1     $LUV  719904304207962112            0.638     twitter   \n",
       "2    $NFLX             5329774           -0.494  stocktwits   \n",
       "3     $DIA  719891468173844480            0.460     twitter   \n",
       "4    $PLUG            20091246            0.403  stocktwits   \n",
       "5    $GMCR             5819749            0.000  stocktwits   \n",
       "6     $IBM  709741154393133056           -0.296     twitter   \n",
       "7    $JOSB            17892972           -0.546  stocktwits   \n",
       "8    $CSTM  709834259687710720           -0.438     twitter   \n",
       "9    $PYPL  708481442079068160            0.408     twitter   \n",
       "10  $GOOGL            31971935           -0.398  stocktwits   \n",
       "11   $ENDP  710187873492983808           -0.349     twitter   \n",
       "12    $XLI            13915103            0.025  stocktwits   \n",
       "13   $PCLN            10448993            0.486  stocktwits   \n",
       "14     $AA            24886266            0.308  stocktwits   \n",
       "15   $AAPL            12793642           -0.372  stocktwits   \n",
       "16   $AAPL             9408369            0.461  stocktwits   \n",
       "17   $GOLD  719909604654624768            0.408     twitter   \n",
       "18    $AMD             9674584           -0.699  stocktwits   \n",
       "19    $SPY            10041008            0.495  stocktwits   \n",
       "\n",
       "                                                spans  \n",
       "0                      [watching for bounce tomorrow]  \n",
       "1        [record number of passengers served in 2015]  \n",
       "2                                    [out $NFLX -.35]  \n",
       "3   [Looking for a strong bounce, Lunchtime rally ...  \n",
       "4   [Very intrigued with the technology and growth...  \n",
       "5                             [short worked, puts up]  \n",
       "6                                        [overbought]  \n",
       "7   [absolute garbage still up, stores TOTALLY EMP...  \n",
       "8                             [Biggest Market Losers]  \n",
       "9                      [Love this company long time.]  \n",
       "10                          [$GOOG $GOOGL would suck]  \n",
       "11                 [who won't pay anymore, REAL risk]  \n",
       "12                                  [No edge offered]  \n",
       "13      [runs into the 50sma on the acquisition news]  \n",
       "14                                  [t can't go down]  \n",
       "15         [now seems like its helping the downtrend]  \n",
       "16                      [mastered their supply chain]  \n",
       "17   [Most bullish stocks on Twitter during this dip]  \n",
       "18      [big dumping, would not touch it for a while]  \n",
       "19  [trade continuing very nicely from yesterday, ...  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = TwitterData_Initialize()\n",
    "data.initialize(\"Microblog_Trainingdata.json\")\n",
    "\n",
    "data.processed_data.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = TwitterData_Initialize_test()\n",
    "data_test.initialize(\"Microblog_Trialdata.json\")\n",
    "\n",
    "data_test.processed_data.head()\n",
    "test_data=data_test.processed_data.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1.165207862854004 seconds ---\n"
     ]
    }
   ],
   "source": [
    "data.do_process()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8314\n"
     ]
    }
   ],
   "source": [
    "data.processed_data.head(20)\n",
    "tokens=data.processed_data[\"token_spans\"]\n",
    "taille=0\n",
    "for token in tokens:\n",
    "    taille+=len(token)\n",
    "print(taille)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "      <th>token_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$FB</td>\n",
       "      <td>719659409228451840</td>\n",
       "      <td>0.366</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[watching for bounce tomorrow]</td>\n",
       "      <td>[watching, for, bounce, tomorrow]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$LUV</td>\n",
       "      <td>719904304207962112</td>\n",
       "      <td>0.638</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[record number of passengers served in 2015]</td>\n",
       "      <td>[record, number, passengers, served]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$NFLX</td>\n",
       "      <td>5329774</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[out $NFLX -.35]</td>\n",
       "      <td>[out, nflx]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$DIA</td>\n",
       "      <td>719891468173844480</td>\n",
       "      <td>0.460</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Looking for a strong bounce, Lunchtime rally ...</td>\n",
       "      <td>[looking, for, strong, bounce, lunchtime, rall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$PLUG</td>\n",
       "      <td>20091246</td>\n",
       "      <td>0.403</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Very intrigued with the technology and growth...</td>\n",
       "      <td>[very, intrigued, with, the, technology, and, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$GMCR</td>\n",
       "      <td>5819749</td>\n",
       "      <td>0.000</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short worked, puts up]</td>\n",
       "      <td>[short, worked, puts]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$IBM</td>\n",
       "      <td>709741154393133056</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[overbought]</td>\n",
       "      <td>[overbought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$JOSB</td>\n",
       "      <td>17892972</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[absolute garbage still up, stores TOTALLY EMP...</td>\n",
       "      <td>[absolute, garbage, still, stores, totally, em...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$CSTM</td>\n",
       "      <td>709834259687710720</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Biggest Market Losers]</td>\n",
       "      <td>[biggest, market, losers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>$PYPL</td>\n",
       "      <td>708481442079068160</td>\n",
       "      <td>0.408</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Love this company long time.]</td>\n",
       "      <td>[love, this, company, long, time]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>$GOOGL</td>\n",
       "      <td>31971935</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[$GOOG $GOOGL would suck]</td>\n",
       "      <td>[goog, googl, would, suck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>$ENDP</td>\n",
       "      <td>710187873492983808</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[who won't pay anymore, REAL risk]</td>\n",
       "      <td>[who, wont, pay, anymore, real, risk]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>$XLI</td>\n",
       "      <td>13915103</td>\n",
       "      <td>0.025</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[No edge offered]</td>\n",
       "      <td>[edge, offered]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>$PCLN</td>\n",
       "      <td>10448993</td>\n",
       "      <td>0.486</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[runs into the 50sma on the acquisition news]</td>\n",
       "      <td>[runs, into, the, the, acquisition, news]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>$AA</td>\n",
       "      <td>24886266</td>\n",
       "      <td>0.308</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[t can't go down]</td>\n",
       "      <td>[cant, down]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>12793642</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[now seems like its helping the downtrend]</td>\n",
       "      <td>[now, seems, like, its, helping, the, downtrend]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>9408369</td>\n",
       "      <td>0.461</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[mastered their supply chain]</td>\n",
       "      <td>[mastered, their, supply, chain]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>$GOLD</td>\n",
       "      <td>719909604654624768</td>\n",
       "      <td>0.408</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Most bullish stocks on Twitter during this dip]</td>\n",
       "      <td>[most, bullish, stocks, twitter, during, this,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>$AMD</td>\n",
       "      <td>9674584</td>\n",
       "      <td>-0.699</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[big dumping, would not touch it for a while]</td>\n",
       "      <td>[big, dumping, would, not, touch, for, while]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>$SPY</td>\n",
       "      <td>10041008</td>\n",
       "      <td>0.495</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[trade continuing very nicely from yesterday, ...</td>\n",
       "      <td>[trade, continuing, very, nicely, from, yester...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cashtag                  id  sentiment score      source  \\\n",
       "0      $FB  719659409228451840            0.366     twitter   \n",
       "1     $LUV  719904304207962112            0.638     twitter   \n",
       "2    $NFLX             5329774           -0.494  stocktwits   \n",
       "3     $DIA  719891468173844480            0.460     twitter   \n",
       "4    $PLUG            20091246            0.403  stocktwits   \n",
       "5    $GMCR             5819749            0.000  stocktwits   \n",
       "6     $IBM  709741154393133056           -0.296     twitter   \n",
       "7    $JOSB            17892972           -0.546  stocktwits   \n",
       "8    $CSTM  709834259687710720           -0.438     twitter   \n",
       "9    $PYPL  708481442079068160            0.408     twitter   \n",
       "10  $GOOGL            31971935           -0.398  stocktwits   \n",
       "11   $ENDP  710187873492983808           -0.349     twitter   \n",
       "12    $XLI            13915103            0.025  stocktwits   \n",
       "13   $PCLN            10448993            0.486  stocktwits   \n",
       "14     $AA            24886266            0.308  stocktwits   \n",
       "15   $AAPL            12793642           -0.372  stocktwits   \n",
       "16   $AAPL             9408369            0.461  stocktwits   \n",
       "17   $GOLD  719909604654624768            0.408     twitter   \n",
       "18    $AMD             9674584           -0.699  stocktwits   \n",
       "19    $SPY            10041008            0.495  stocktwits   \n",
       "\n",
       "                                                spans  \\\n",
       "0                      [watching for bounce tomorrow]   \n",
       "1        [record number of passengers served in 2015]   \n",
       "2                                    [out $NFLX -.35]   \n",
       "3   [Looking for a strong bounce, Lunchtime rally ...   \n",
       "4   [Very intrigued with the technology and growth...   \n",
       "5                             [short worked, puts up]   \n",
       "6                                        [overbought]   \n",
       "7   [absolute garbage still up, stores TOTALLY EMP...   \n",
       "8                             [Biggest Market Losers]   \n",
       "9                      [Love this company long time.]   \n",
       "10                          [$GOOG $GOOGL would suck]   \n",
       "11                 [who won't pay anymore, REAL risk]   \n",
       "12                                  [No edge offered]   \n",
       "13      [runs into the 50sma on the acquisition news]   \n",
       "14                                  [t can't go down]   \n",
       "15         [now seems like its helping the downtrend]   \n",
       "16                      [mastered their supply chain]   \n",
       "17   [Most bullish stocks on Twitter during this dip]   \n",
       "18      [big dumping, would not touch it for a while]   \n",
       "19  [trade continuing very nicely from yesterday, ...   \n",
       "\n",
       "                                          token_spans  \n",
       "0                   [watching, for, bounce, tomorrow]  \n",
       "1                [record, number, passengers, served]  \n",
       "2                                         [out, nflx]  \n",
       "3   [looking, for, strong, bounce, lunchtime, rall...  \n",
       "4   [very, intrigued, with, the, technology, and, ...  \n",
       "5                               [short, worked, puts]  \n",
       "6                                        [overbought]  \n",
       "7   [absolute, garbage, still, stores, totally, em...  \n",
       "8                           [biggest, market, losers]  \n",
       "9                   [love, this, company, long, time]  \n",
       "10                         [goog, googl, would, suck]  \n",
       "11              [who, wont, pay, anymore, real, risk]  \n",
       "12                                    [edge, offered]  \n",
       "13          [runs, into, the, the, acquisition, news]  \n",
       "14                                       [cant, down]  \n",
       "15   [now, seems, like, its, helping, the, downtrend]  \n",
       "16                   [mastered, their, supply, chain]  \n",
       "17  [most, bullish, stocks, twitter, during, this,...  \n",
       "18      [big, dumping, would, not, touch, for, while]  \n",
       "19  [trade, continuing, very, nicely, from, yester...  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.processed_data.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('for', 128), ('stocks', 82), ('long', 74), ('buy', 70), ('short', 68), ('today', 56), ('good', 46), ('nice', 44), ('new', 44), ('looking', 43), ('still', 43), ('stock', 43), ('market', 38), ('bullish', 37), ('day', 36), ('call', 34), ('top', 34), ('like', 31), ('week', 29), ('rally', 27), ('trade', 27), ('looks', 27), ('sell', 26), ('tsla', 25), ('buying', 24), ('aapl', 24), ('strong', 23), ('calls', 23), ('time', 22), ('could', 22), ('support', 22), ('back', 22), ('higher', 22), ('puts', 21), ('shorts', 21), ('close', 21), ('going', 20), ('high', 20), ('sold', 20), ('weak', 20), ('holding', 20), ('breakout', 20), ('growth', 19), ('overbought', 19), ('see', 19), ('get', 19), ('may', 19), ('one', 19), ('selling', 18), ('next', 18), ('great', 18), ('bought', 18), ('play', 18), ('run', 17), ('earnings', 17), ('setups', 17), ('morning', 17), ('coming', 16), ('unusual', 16), ('positive', 16), ('downgrade', 16), ('bull', 16), ('ready', 16), ('losers', 15), ('big', 15), ('apple', 15), ('money', 15), ('shares', 15), ('added', 15), ('break', 15), ('take', 15), ('highs', 15), ('another', 15), ('last', 15), ('sales', 15), ('year', 15), ('getting', 15), ('biggest', 14), ('stochastic', 14), ('activity', 14), ('lower', 14), ('position', 14), ('sector', 14), ('point', 14), ('friday', 14), ('longs', 14), ('bounce', 13), ('dip', 13), ('price', 13), ('look', 13), ('oil', 13), ('start', 13), ('best', 13), ('would', 12), ('dont', 12), ('bottom', 12), ('resistance', 12), ('todays', 12), ('add', 12), ('china', 12), ('broke', 12), ('upside', 12), ('since', 12), ('insiders', 11), ('put', 11), ('bad', 11), ('green', 11), ('cuba', 11), ('rating', 11), ('well', 11), ('gap', 11), ('right', 11), ('markets', 11), ('small', 11), ('potential', 10), ('company', 10), ('news', 10), ('move', 10), ('watch', 10), ('closed', 10), ('chart', 10), ('gold', 10), ('adding', 10), ('gainers', 10), ('covered', 10), ('goog', 9), ('think', 9), ('term', 9), ('hold', 9), ('weeks', 9), ('highest', 9), ('cash', 9), ('prices', 9), ('gains', 9), ('google', 9), ('tech', 9), ('line', 9), ('around', 9), ('leading', 9), ('signs', 9), ('bear', 9), ('far', 9), ('make', 9), ('tesla', 8), ('taking', 8), ('volume', 8), ('facebook', 8), ('nicereally', 8), ('debt', 8), ('squeeze', 8), ('bollinger', 8), ('first', 8), ('weakness', 8), ('bid', 8), ('confident', 8), ('major', 8), ('want', 8), ('large', 8), ('drop', 8), ('flag', 8), ('interest', 8), ('daily', 8), ('setup', 8), ('cheap', 8), ('worst', 8), ('failed', 8), ('cloud', 8), ('dividend', 8), ('tomorrow', 7), ('googl', 7), ('pay', 7), ('real', 7), ('risk', 7), ('technical', 7), ('set', 7), ('stay', 7), ('days', 7), ('oversold', 7), ('moved', 7), ('upper', 7), ('band', 7), ('ratio', 7), ('breaking', 7), ('deal', 7), ('quality', 7), ('rise', 7), ('yahoo', 7), ('internet', 7), ('spy', 7), ('monday', 7), ('fall', 7), ('moving', 7), ('starting', 7), ('amzn', 7), ('afternoon', 7), ('business', 7), ('globally', 7), ('every', 7), ('target', 7), ('way', 7), ('trending', 7), ('trying', 7), ('yet', 7), ('near', 7), ('keep', 7), ('bearish', 7), ('performers', 7), ('gain', 7), ('fast', 7), ('weekly', 7), ('nflx', 6), ('twitter', 6), ('swing', 6), ('goes', 6), ('double', 6), ('careful', 6), ('already', 6), ('entry', 6), ('follow', 6), ('flow', 6), ('cuts', 6), ('little', 6), ('pie', 6), ('sky', 6), ('premarket', 6), ('million', 6), ('upgrades', 6), ('continues', 6), ('end', 6), ('twtr', 6), ('companies', 6), ('rate', 6), ('remains', 6), ('amazing', 6), ('name', 6), ('making', 6), ('below', 6), ('users', 6), ('black', 6), ('soon', 6), ('needs', 6), ('names', 6), ('better', 6), ('ripe', 6), ('airplane', 6), ('hospitality', 6), ('industries', 6), ('sights', 6), ('lead', 6), ('investors', 6), ('lowest', 6), ('favorite', 6), ('model', 6), ('signal', 6), ('less', 6), ('side', 6), ('msft', 6), ('buys', 6), ('crossovers', 6), ('yhoo', 6), ('worth', 6), ('hod', 6), ('bears', 6), ('breakouts', 6), ('retailers', 6), ('watching', 5), ('record', 5), ('wont', 5), ('yesterday', 5), ('lets', 5), ('momentum', 5), ('possible', 5), ('report', 5), ('level', 5), ('bidu', 5), ('trend', 5), ('found', 5), ('eps', 5), ('losses', 5), ('fed', 5), ('rates', 5), ('forecast', 5), ('tight', 5), ('plays', 5), ('second', 5), ('light', 5), ('pop', 5), ('red', 5), ('watchlist', 5), ('patience', 5), ('might', 5), ('lots', 5), ('really', 5), ('load', 5), ('awaits', 5), ('adds', 5), ('falling', 5), ('netpayoutyields', 5), ('acting', 5), ('caps', 5), ('outperform', 5), ('option', 5), ('analyst', 5), ('upgraded', 5), ('iphone', 5), ('bidding', 5), ('valuation', 5), ('recall', 5), ('helped', 5), ('fill', 5), ('shipments', 5), ('profits', 5), ('running', 5), ('profit', 5), ('bot', 5), ('reasons', 5), ('nothing', 5), ('wants', 5), ('ecommerce', 5), ('hard', 5), ('much', 5), ('amazon', 5), ('got', 5), ('theres', 5), ('conviction', 5), ('equity', 5), ('portfolio', 5), ('slid', 5), ('interesting', 5), ('billion', 5), ('shorting', 5), ('working', 5), ('expected', 5), ('vrx', 5), ('low', 5), ('almost', 5), ('head', 5), ('massive', 5), ('beat', 5), ('hope', 5), ('opportunity', 5), ('number', 4), ('passengers', 4), ('served', 4), ('lunchtime', 4), ('love', 4), ('seems', 4), ('longerterm', 4), ('basis', 4), ('breaks', 4), ('trading', 4), ('outcome', 4), ('chinese', 4), ('percent', 4), ('came', 4), ('increase', 4), ('mkt', 4), ('gets', 4), ('let', 4), ('luv', 4), ('guess', 4), ('shareholders', 4), ('japan', 4), ('ibb', 4), ('patents', 4), ('assets', 4), ('biz', 4), ('numbers', 4), ('crisis', 4), ('entered', 4), ('dangerous', 4), ('phase', 4), ('shld', 4), ('worried', 4), ('icloud', 4), ('catch', 4), ('finally', 4), ('jobs', 4), ('made', 4), ('two', 4), ('winners', 4), ('channel', 4), ('chevron', 4), ('held', 4), ('early', 4), ('overvalued', 4), ('even', 4), ('three', 4), ('bulls', 4), ('find', 4), ('turn', 4), ('didnt', 4), ('total', 4), ('list', 4), ('fund', 4), ('stop', 4), ('outperformers', 4), ('ranked', 4), ('average', 4), ('cars', 4), ('cover', 4), ('lows', 4), ('possibly', 4), ('downbeaten', 4), ('zone', 4), ('took', 4), ('sectors', 4), ('retail', 4), ('results', 4), ('googles', 4), ('show', 4), ('volatility', 4), ('lot', 4), ('push', 4), ('progress', 4), ('blood', 4), ('uptrend', 4), ('downgraded', 4), ('bank', 4), ('base', 4), ('enough', 4), ('ever', 4), ('reversal', 4), ('recalls', 4), ('triangle', 4), ('coal', 4), ('years', 4), ('aggressive', 4), ('receives', 4), ('finding', 4), ('growing', 4), ('cautious', 4), ('showing', 4), ('says', 4), ('ended', 4), ('class', 4), ('ahead', 4), ('quarterly', 4), ('picks', 4), ('anymore', 3), ('edge', 3), ('offered', 3), ('cant', 3), ('downtrend', 3), ('continuing', 3), ('nicely', 3), ('lung', 3), ('filing', 3), ('soros', 3), ('conditions', 3), ('imagine', 3), ('downturn', 3), ('watchingand', 3), ('targets', 3), ('revenue', 3), ('skf', 3), ('positions', 3), ('wish', 3), ('fell', 3), ('due', 3), ('consolidation', 3), ('crm', 3), ('also', 3), ('place', 3), ('airlines', 3), ('battle', 3), ('routes', 3), ('aal', 3), ('save', 3), ('jblu', 3), ('blackberry', 3), ('led', 3), ('bulb', 3), ('later', 3), ('exit', 3), ('heading', 3), ('qqq', 3), ('outlook', 3), ('people', 3), ('cost', 3), ('area', 3), ('recently', 3), ('leads', 3), ('november', 3), ('unemployment', 3), ('futures', 3), ('noncore', 3), ('ascending', 3), ('bet', 3), ('started', 3), ('seeing', 3), ('banks', 3), ('pcln', 3), ('winner', 3), ('moves', 3), ('downside', 3), ('notable', 3), ('among', 3), ('liquid', 3), ('alcohol', 3), ('jet', 3), ('fuel', 3), ('eligible', 3), ('used', 3), ('commercial', 3), ('flight', 3), ('pivotal', 3), ('research', 3), ('continue', 3), ('story', 3), ('declining', 3), ('dropping', 3), ('extends', 3), ('deadline', 3), ('grows', 3), ('mrkt', 3), ('washout', 3), ('wfm', 3), ('issues', 3), ('capital', 3), ('building', 3), ('consistent', 3), ('adbe', 3), ('tops', 3), ('slips', 3), ('setting', 3), ('neutral', 3), ('miner', 3), ('changed', 3), ('beast', 3), ('street', 3), ('relative', 3), ('profitable', 3), ('nov', 3), ('trendline', 3), ('everyone', 3), ('february', 3), ('resilient', 3), ('doesnt', 3), ('endp', 3), ('huge', 3), ('hit', 3), ('march', 3), ('quarter', 3), ('software', 3), ('problem', 3), ('sharp', 3), ('equities', 3), ('picking', 3), ('parties', 3), ('leave', 3), ('inc', 3), ('streets', 3), ('goldman', 3), ('sachs', 3), ('reiterates', 3), ('losing', 3), ('couple', 3), ('deutsche', 3), ('five', 3), ('starboard', 3), ('return', 3), ('thing', 3), ('dollar', 3), ('excited', 3), ('clear', 3), ('advantage', 3), ('priced', 3), ('rsi', 3), ('engulfing', 3), ('production', 3), ('alphabet', 3), ('candle', 3), ('jump', 3), ('global', 3), ('least', 3), ('keeps', 3), ('index', 3), ('raising', 3), ('keeping', 3), ('likely', 3), ('lift', 3), ('sale', 3), ('lost', 3), ('limping', 3), ('crutches', 3), ('media', 3), ('thats', 3), ('accumulation', 3), ('amd', 3), ('waiting', 3), ('holdings', 3), ('finish', 3), ('wold', 3), ('anly', 3), ('pzzi', 3), ('work', 3), ('soars', 3), ('dethrones', 3), ('round', 3), ('competition', 3), ('sounds', 3), ('auto', 3), ('stores', 2), ('empty', 2), ('suck', 2), ('acquisition', 2), ('reserves', 2), ('decline', 2), ('key', 2), ('idea', 2), ('half', 2), ('bac', 2), ('alibaba', 2), ('ipo', 2), ('mdxg', 2), ('without', 2), ('order', 2), ('defying', 2), ('action', 2), ('volatile', 2), ('beats', 2), ('estimize', 2), ('consensus', 2), ('float', 2), ('fxp', 2), ('ure', 2), ('slow', 2), ('fcx', 2), ('followed', 2), ('need', 2), ('pulls', 2), ('messenger', 2), ('user', 2), ('loyalty', 2), ('within', 2), ('returns', 2), ('considering', 2), ('breadth', 2), ('catalyst', 2), ('strategy', 2), ('piper', 2), ('improving', 2), ('rebound', 2), ('joint', 2), ('pays', 2), ('governance', 2), ('core', 2), ('igh', 2), ('concern', 2), ('feel', 2), ('technicals', 2), ('toppy', 2), ('billions', 2), ('consolidate', 2), ('decent', 2), ('beating', 2), ('someone', 2), ('points', 2), ('creep', 2), ('supermarket', 2), ('tesco', 2), ('plans', 2), ('axe', 2), ('pressure', 2), ('along', 2), ('undervalued', 2), ('wait', 2), ('stake', 2), ('cracking', 2), ('intc', 2), ('gild', 2), ('bwld', 2), ('online', 2), ('micron', 2), ('pounded', 2), ('fears', 2), ('loss', 2), ('hot', 2), ('levels', 2), ('expe', 2), ('mix', 2), ('ctrp', 2), ('trip', 2), ('reiterated', 2), ('flying', 2), ('santa', 2), ('blow', 2), ('surge', 2), ('personal', 2), ('sharing', 2), ('feeds', 2), ('saying', 2), ('fear', 2), ('smartphone', 2), ('increasing', 2), ('lol', 2), ('net', 2), ('liking', 2), ('hedge', 2), ('case', 2), ('joe', 2), ('wks', 2), ('inflows', 2), ('primed', 2), ('domestic', 2), ('automakers', 2), ('fwd', 2), ('htz', 2), ('lvmh', 2), ('affordable', 2), ('luxe', 2), ('fine', 2), ('dutyfree', 2), ('mainstream', 2), ('shoppers', 2), ('happen', 2), ('went', 2), ('wouldnt', 2), ('selloff', 2), ('recommendation', 2), ('trigger', 2), ('sentiment', 2), ('parabolic', 2), ('chasing', 2), ('boys', 2), ('never', 2), ('monthly', 2), ('etfs', 2), ('rather', 2), ('international', 2), ('exposure', 2), ('range', 2), ('detector', 2), ('autonomous', 2), ('handle', 2), ('fit', 2), ('nugt', 2), ('conversation', 2), ('crossing', 2), ('ill', 2), ('hype', 2), ('promising', 2), ('trails', 2), ('orcl', 2), ('value', 2), ('must', 2), ('caution', 2), ('congrats', 2), ('picked', 2), ('mgm', 2), ('player', 2), ('bit', 2), ('risky', 2), ('flop', 2), ('exits', 2), ('toprated', 2), ('cap', 2), ('impact', 2), ('financial', 2), ('say', 2), ('heat', 2), ('lnkd', 2), ('rumor', 2), ('downgrades', 2), ('grpn', 2), ('track', 2), ('exceed', 2), ('ive', 2), ('railroad', 2), ('retirement', 2), ('acct', 2), ('slowly', 2), ('heats', 2), ('thinks', 2), ('men', 2), ('sign', 2), ('plus', 2), ('irish', 2), ('based', 2), ('rid', 2), ('generic', 2), ('shows', 2), ('stakes', 2), ('judge', 2), ('forward', 2), ('min', 2), ('launching', 2), ('sad', 2), ('dips', 2), ('drag', 2), ('zuck', 2), ('vol', 2), ('mark', 2), ('whole', 2), ('space', 2), ('combine', 2), ('pullback', 2), ('ooks', 2), ('pretty', 2), ('options', 2), ('forming', 2), ('multiple', 2), ('anyone', 2), ('confirmed', 2), ('avail', 2), ('bounced', 2), ('ride', 2), ('deals', 2), ('hike', 2), ('recent', 2), ('sells', 2), ('tfm', 2), ('wise', 2), ('compete', 2), ('bubble', 2), ('ndx', 2), ('internals', 2), ('bullshit', 2), ('reason', 2), ('indexes', 2), ('runner', 2), ('sees', 2), ('refuse', 2), ('decisions', 2), ('opening', 2), ('sets', 2), ('pick', 2), ('hanging', 2), ('tough', 2), ('economy', 2), ('wall', 2), ('loving', 2), ('dry', 2), ('stares', 2), ('abyss', 2), ('offer', 2), ('eyeing', 2), ('takeover', 2), ('longterm', 2), ('cupandhandle', 2), ('shaping', 2), ('favours', 2), ('bail', 2), ('pypl', 2), ('profitability', 2), ('raised', 2), ('dirt', 2), ('roll', 2), ('horizontal', 2), ('rig', 2), ('accumulating', 2), ('intraday', 2), ('though', 2), ('eye', 2), ('investment', 2), ('nobody', 2), ('timing', 2), ('active', 2), ('venture', 2), ('month', 2), ('highflyers', 2), ('chance', 2), ('worry', 2), ('pride', 2), ('social', 2), ('gettin', 2), ('rolling', 2), ('strongest', 2), ('bleeding', 2), ('scenario', 2), ('patent', 2), ('content', 2), ('nervous', 2), ('flush', 2), ('trouble', 2), ('slipping', 2), ('moodys', 2), ('scalps', 2), ('elon', 2), ('charts', 2), ('comments', 2), ('ceo', 2), ('taxes', 2), ('brutal', 2), ('ton', 2), ('impressive', 2), ('chromebooks', 2), ('walmart', 2), ('staples', 2), ('faster', 2), ('pain', 2), ('shine', 2), ('stomp', 2), ('dreams', 2), ('motors', 2), ('old', 2), ('relatively', 2), ('compared', 2), ('broad', 2), ('tremendous', 2), ('launch', 2), ('screens', 2), ('beta', 2), ('bbry', 2), ('barclays', 2), ('institutions', 2), ('seem', 2), ('loading', 2), ('indices', 2), ('rallying', 2), ('highly', 2), ('underperforming', 2), ('demand', 2), ('updated', 2), ('everything', 2), ('pushing', 2), ('buyer', 2), ('leadership', 2), ('share', 2), ('sure', 2), ('yeah', 2), ('solid', 2), ('bmw', 2), ('entering', 2), ('rideshare', 2), ('pos', 2), ('isnt', 2), ('announces', 2), ('riskreward', 2), ('initiate', 2), ('hang', 2), ('whose', 2), ('yeartodate', 2), ('traders', 2), ('multiyear', 2), ('trust', 2), ('car', 2), ('maker', 2), ('together', 2), ('bricks', 2), ('mortar', 2), ('cheaper', 2), ('consumer', 2), ('party', 2), ('earning', 2), ('guidance', 2), ('infringement', 2), ('serious', 2), ('betting', 2), ('reverse', 2), ('ibm', 2), ('arm', 2), ('begin', 2), ('assault', 2), ('intels', 2), ('server', 2), ('franchise', 2), ('industry', 2), ('macd', 2), ('crushed', 2), ('recessions', 2), ('declines', 2), ('upward', 2), ('excellent', 2), ('mail', 2), ('businesses', 2), ('miners', 2), ('shipping', 2), ('rated', 2), ('customers', 2), ('increased', 2), ('conversations', 2), ('omg', 2), ('intrigued', 1), ('technology', 1), ('worked', 1), ('absolute', 1), ('garbage', 1), ('totally', 1), ('mispriced', 1), ('runs', 1), ('helping', 1), ('mastered', 1), ('supply', 1), ('chain', 1), ('dumping', 1), ('touch', 1), ('feed', 1), ('lately', 1), ('alltime', 1), ('chum', 1), ('hoopla', 1), ('hide', 1), ('fence', 1), ('drought', 1), ('soil', 1), ('hip', 1), ('sinking', 1), ('wanting', 1), ('weekend', 1), ('care', 1), ('simo', 1), ('basing', 1), ('pouring', 1), ('directions', 1), ('placed', 1), ('man', 1), ('wasnt', 1), ('appealing', 1), ('tapping', 1), ('reenter', 1), ('earlier', 1), ('wmt', 1), ('frustrating', 1), ('bar', 1), ('immediately', 1), ('thanks', 1), ('step', 1), ('offers', 1), ('currently', 1), ('rejoicing', 1), ('headed', 1), ('shit', 1), ('open', 1), ('mins', 1), ('mdlz', 1), ('welcome', 1), ('marriottstarwood', 1), ('peachy', 1), ('prospects', 1), ('steady', 1), ('gainsadvances', 1), ('slowe', 1), ('aggressively', 1), ('riser', 1), ('symantec', 1), ('renewal', 1), ('enterprise', 1), ('solar', 1), ('dnf', 1), ('crew', 1), ('dragging', 1), ('yahoos', 1), ('ones', 1), ('finger', 1), ('button', 1), ('filled', 1), ('resist', 1), ('holiday', 1), ('breakup', 1), ('fastest', 1), ('previous', 1), ('supt', 1), ('hereexpect', 1), ('steal', 1), ('multi', 1), ('mcd', 1), ('stability', 1), ('questioned', 1), ('tell', 1), ('mon', 1), ('games', 1), ('bankruptcy', 1), ('gogo', 1), ('closer', 1), ('charging', 1), ('android', 1), ('precedes', 1), ('brand', 1), ('fail', 1), ('recover', 1), ('oct', 1), ('splits', 1), ('lipstick', 1), ('pig', 1), ('concentrated', 1), ('correction', 1), ('sso', 1), ('tiffanys', 1), ('dow', 1), ('crunching', 1), ('diamond', 1), ('rough', 1), ('largest', 1), ('gaming', 1), ('stopped', 1), ('yesterdays', 1), ('uvxy', 1), ('leaps', 1), ('pstv', 1), ('retreats', 1), ('clockwork', 1), ('ifts', 1), ('system', 1), ('terribly', 1), ('tnh', 1), ('ringing', 1), ('register', 1), ('airplanes', 1), ('mtd', 1), ('bond', 1), ('signaling', 1), ('dollars', 1), ('course', 1), ('loaded', 1), ('bsx', 1), ('brianwieser', 1), ('brian', 1), ('success', 1), ('galaxy', 1), ('factor', 1), ('anything', 1), ('repeated', 1), ('succeeding', 1), ('dominated', 1), ('uco', 1), ('recuperates', 1), ('peak', 1), ('mongering', 1), ('competitions', 1), ('dying', 1), ('usage', 1), ('approximately', 1), ('revenues', 1), ('mln', 1), ('energy', 1), ('whiting', 1), ('crzo', 1), ('landed', 1), ('transports', 1), ('always', 1), ('attention', 1), ('nyse', 1), ('bring', 1), ('spx', 1), ('itting', 1), ('retest', 1), ('ses', 1), ('warrants', 1), ('poised', 1), ('peg', 1), ('historical', 1), ('enthusing', 1), ('reliability', 1), ('avoid', 1), ('bios', 1), ('crying', 1), ('lucky', 1), ('enuff', 1), ('quite', 1), ('implode', 1), ('stress', 1), ('tests', 1), ('failing', 1), ('carl', 1), ('kirst', 1), ('bmo', 1), ('cult', 1), ('stocktwits', 1), ('actually', 1), ('disgusting', 1), ('suvs', 1), ('seat', 1), ('issue', 1), ('financing', 1), ('fueling', 1), ('wave', 1), ('acquisitions', 1), ('widen', 1), ('alongside', 1), ('nsa', 1), ('leak', 1), ('weakest', 1), ('false', 1), ('recalling', 1), ('maintain', 1), ('yield', 1), ('announcing', 1), ('divy', 1), ('subject', 1), ('decay', 1), ('rebalancin', 1), ('life', 1), ('emerging', 1), ('diversifies', 1), ('capped', 1), ('meltdown', 1), ('cup', 1), ('makes', 1), ('comparable', 1), ('store', 1), ('consolidates', 1), ('past', 1), ('several', 1), ('flat', 1), ('suddenly', 1), ('realizing', 1), ('sune', 1), ('fucking', 1), ('mode', 1), ('kicking', 1), ('shirts', 1), ('pouch', 1), ('hospital', 1), ('reits', 1), ('slaughtered', 1), ('cry', 1), ('important', 1), ('stockco', 1), ('choice', 1), ('positve', 1), ('word', 1), ('allergen', 1), ('endo', 1), ('wynn', 1), ('crows', 1), ('breed', 1), ('room', 1), ('orgetting', 1), ('goo', 1), ('usio', 1), ('ops', 1), ('gmcr', 1), ('outperforms', 1), ('earish', 1), ('trader', 1), ('bets', 1), ('done', 1), ('pass', 1), ('onto', 1), ('humming', 1), ('easing', 1), ('ron', 1), ('johnson', 1), ('tried', 1), ('inject', 1), ('culture', 1), ('penney', 1), ('prettiest', 1), ('candlesticks', 1), ('agree', 1), ('closely', 1), ('posistion', 1), ('fences', 1), ('calling', 1), ('fader', 1), ('improved', 1), ('rest', 1), ('res', 1), ('implies', 1), ('agn', 1), ('gist', 1), ('phone', 1), ('rigl', 1), ('following', 1), ('apr', 1), ('stx', 1), ('hipments', 1), ('forecasts', 1), ('rose', 1), ('sap', 1), ('disappoints', 1), ('licenses', 1), ('uses', 1), ('turmoil', 1), ('mena', 1), ('region', 1), ('shippers', 1), ('perform', 1), ('alerted', 1), ('farm', 1), ('apparently', 1), ('intel', 1), ('women', 1), ('paid', 1), ('gender', 1), ('parity', 1), ('avengers', 1), ('sick', 1), ('stan', 1), ('lee', 1), ('court', 1), ('tells', 1), ('states', 1), ('alone', 1), ('unloaded', 1), ('ensign', 1), ('group', 1), ('nasdaqensg', 1), ('weekdid', 1), ('snatch', 1), ('lawsuit', 1), ('russian', 1), ('search', 1), ('engine', 1), ('kous', 1), ('cocacola', 1), ('margins', 1), ('redtogreen', 1), ('gate', 1), ('ffiv', 1), ('chrw', 1), ('thru', 1), ('dma', 1), ('reargument', 1), ('denied', 1), ('upcoming', 1), ('cafn', 1), ('cachet', 1), ('solutions', 1), ('surging', 1), ('post', 1), ('stochastics', 1), ('shopping', 1), ('intract', 1), ('aapls', 1), ('broker', 1), ('reduced', 1), ('picture', 1), ('brilliant', 1), ('boywonder', 1), ('visionary', 1), ('unhappy', 1), ('ending', 1), ('zuckerberg', 1), ('job', 1), ('history', 1), ('lack', 1), ('shame', 1), ('belooooooowwww', 1), ('semiconductor', 1), ('peaks', 1), ('awesomely', 1), ('grabbed', 1), ('tme', 1), ('grow', 1), ('triple', 1), ('pennies', 1), ('sexy', 1), ('foods', 1), ('vote', 1), ('activist', 1), ('initiatives', 1), ('southwest', 1), ('products', 1), ('fan', 1), ('tripadvisor', 1), ('jumps', 1), ('rumors', 1), ('vehicle', 1), ('upturn', 1), ('mjn', 1), ('reject', 1), ('commodity', 1), ('flong', 1), ('via', 1), ('flati', 1), ('pull', 1), ('kroger', 1), ('onetime', 1), ('zombie', 1), ('cat', 1), ('brands', 1), ('millennials', 1), ('tasty', 1), ('damn', 1), ('backs', 1), ('happily', 1), ('baba', 1), ('fouryear', 1), ('opec', 1), ('kept', 1), ('unchange', 1), ('sideways', 1), ('makeup', 1), ('ugliness', 1), ('slight', 1), ('product', 1), ('innovator', 1), ('xboxlive', 1), ('service', 1), ('gooo', 1), ('free', 1), ('brothers', 1), ('shadow', 1), ('kre', 1), ('accumulate', 1), ('something', 1), ('gonna', 1), ('winning', 1), ('popping', 1), ('cusp', 1), ('sitting', 1), ('ema', 1), ('force', 1), ('poking', 1), ('exceptionally', 1), ('romney', 1), ('midsingle', 1), ('digits', 1), ('ebitda', 1), ('posts', 1), ('steep', 1), ('view', 1), ('btu', 1), ('paint', 1), ('fundamentally', 1), ('wars', 1), ('wound', 1), ('samsung', 1), ('proprietary', 1), ('liquidity', 1), ('ousy', 1), ('spreads', 1), ('gained', 1), ('nine', 1), ('investor', 1), ('easonably', 1), ('institutional', 1), ('fave', 1), ('glamour', 1), ('gilead', 1), ('european', 1), ('dumped', 1), ('prlb', 1), ('cfo', 1), ('loved', 1), ('eating', 1), ('burn', 1), ('strictly', 1), ('minute', 1), ('touting', 1), ('rock', 1), ('cold', 1), ('dent', 1), ('bsmx', 1), ('trail', 1), ('applestores', 1), ('rail', 1), ('heard', 1), ('netflix', 1), ('ripping', 1), ('stops', 1), ('contract', 1), ('splat', 1), ('poor', 1), ('orders', 1), ('waves', 1), ('smaller', 1), ('stk', 1), ('clearly', 1), ('sued', 1), ('heads', 1), ('largecap', 1), ('fundholders', 1), ('witnessed', 1), ('truly', 1), ('stunning', 1), ('momo', 1), ('leaders', 1), ('taken', 1), ('mulls', 1), ('surprised', 1), ('decision', 1), ('robot', 1), ('exposes', 1), ('pains', 1), ('alpha', 1), ('expects', 1), ('adobe', 1), ('alot', 1), ('verizon', 1), ('bidder', 1), ('sectorbreadth', 1), ('industrials', 1), ('bullishness', 1), ('uch', 1), ('bloated', 1), ('vampirekeep', 1), ('ief', 1), ('everyday', 1), ('tue', 1), ('rewards', 1), ('patient', 1), ('future', 1), ('bright', 1), ('exp', 1), ('imo', 1), ('remember', 1), ('told', 1), ('fix', 1), ('email', 1), ('yesterda', 1), ('hoping', 1), ('calm', 1), ('advance', 1), ('announce', 1), ('licensing', 1), ('agreement', 1), ('neither', 1), ('gale', 1), ('stacks', 1), ('poorly', 1), ('insider', 1), ('ownership', 1), ('signed', 1), ('lease', 1), ('boeing', 1), ('cargo', 1), ('planes', 1), ('platform', 1), ('dailymail', 1), ('perfect', 1), ('demographics', 1), ('electronic', 1), ('arts', 1), ('attracts', 1), ('expansion', 1), ('intact', 1), ('regn', 1), ('plunging', 1), ('tablet', 1), ('biotechs', 1), ('timberrrr', 1), ('wittine', 1), ('longbow', 1), ('hires', 1), ('former', 1), ('executive', 1), ('george', 1), ('stathakopoulos', 1), ('corporate', 1), ('data', 1), ('security', 1), ('spec', 1), ('penny', 1), ('wirs', 1), ('invest', 1), ('minuscule', 1), ('ziv', 1), ('handsome', 1), ('receivable', 1), ('zero', 1), ('cisco', 1), ('rht', 1), ('illustrate', 1), ('wtsl', 1), ('footprint', 1), ('drugs', 1), ('biotech', 1), ('perky', 1), ('negative', 1), ('hardware', 1), ('eyes', 1), ('disposals', 1), ('appease', 1), ('city', 1), ('backers', 1), ('usual', 1), ('lose', 1), ('tank', 1), ('balance', 1), ('bollies', 1), ('services', 1), ('hulu', 1), ('cable', 1), ('max', 1), ('consumers', 1), ('stance', 1), ('broken', 1), ('balancing', 1), ('roic', 1), ('create', 1), ('locked', 1), ('tom', 1), ('upwards', 1), ('rotating', 1), ('strength', 1), ('test', 1), ('october', 1), ('baird', 1), ('mom', 1), ('grandmas', 1), ('yoku', 1), ('secure', 1), ('movers', 1), ('stellar', 1), ('beautifully', 1), ('orchestrated', 1), ('marke', 1), ('cures', 1), ('hardtotreat', 1), ('enjoy', 1), ('statement', 1), ('ceiling', 1), ('spot', 1), ('cheerleaders', 1), ('actual', 1), ('mbly', 1), ('nly', 1), ('overall', 1), ('improve', 1), ('gots', 1), ('aezs', 1), ('erinn', 1), ('murphy', 1), ('jaffray', 1), ('finl', 1), ('longer', 1), ('quickie', 1), ('pattern', 1), ('note', 1), ('lighter', 1), ('finance', 1), ('unfilled', 1), ('july', 1), ('significant', 1), ('addding', 1), ('xoma', 1), ('blurb', 1), ('knives', 1), ('shortput', 1), ('confirm', 1), ('doubled', 1), ('master', 1), ('conquer', 1), ('mobile', 1), ('astonishing', 1), ('turned', 1), ('fortunes', 1), ('biggies', 1), ('diminish', 1), ('inverse', 1), ('shoulders', 1), ('gpro', 1), ('embarrassing', 1), ('bumping', 1), ('overhead', 1), ('finds', 1), ('robust', 1), ('hopefully', 1), ('peeps', 1), ('camt', 1), ('tape', 1), ('infection', 1), ('worries', 1), ('prompt', 1), ('regulatory', 1), ('reviews', 1), ('reversals', 1), ('gld', 1), ('consider', 1), ('yes', 1), ('brent', 1), ('premier', 1), ('development', 1), ('ezpw', 1), ('sma', 1), ('chunk', 1), ('dec', 1), ('quick', 1), ('supports', 1), ('competitors', 1), ('inevitable', 1), ('successor', 1), ('northern', 1), ('telecom', 1), ('competent', 1), ('tired', 1), ('traction', 1), ('enjoyed', 1), ('anglo', 1), ('cleary', 1), ('surpassed', 1), ('expands', 1), ('repurchase', 1), ('program', 1), ('safe', 1), ('ewz', 1), ('doomed', 1), ('created', 1), ('destroyed', 1), ('starbucks', 1), ('premium', 1), ('gapping', 1), ('netapp', 1), ('macquarie', 1), ('underperform', 1), ('confirmation', 1), ('beautiful', 1), ('trendbase', 1), ('pointing', 1), ('obby', 1), ('garland', 1), ('probably', 1), ('loooooongggggg', 1), ('checkout', 1), ('recommending', 1), ('things', 1), ('stomach', 1), ('left', 1), ('svxy', 1), ('screen', 1), ('guy', 1), ('killing', 1), ('atching', 1), ('sto', 1), ('whisperbeat', 1), ('staying', 1), ('ratings', 1), ('jefferies', 1), ('till', 1), ('wkly', 1), ('downward', 1), ('wake', 1), ('completes', 1), ('icoa', 1), ('continuation', 1), ('actively', 1), ('shoulder', 1), ('dipping', 1), ('testing', 1), ('reward', 1), ('slowing', 1), ('tmrw', 1), ('war', 1), ('beaten', 1), ('hated', 1), ('esports', 1), ('viewership', 1), ('hours', 1), ('seriously', 1), ('played', 1), ('credit', 1), ('spread', 1), ('hitting', 1), ('sight', 1), ('somewhere', 1), ('annoying', 1), ('indicators', 1), ('turning', 1), ('logistics', 1), ('capex', 1), ('result', 1), ('missed', 1), ('comparision', 1), ('words', 1), ('stream', 1), ('mini', 1), ('trades', 1), ('retracement', 1), ('topping', 1), ('tail', 1), ('naturalgas', 1), ('settles', 1), ('hey', 1), ('barely', 1), ('bux', 1), ('analysts', 1), ('impressed', 1), ('offshoring', 1), ('dump', 1), ('exdividend', 1), ('sina', 1), ('tol', 1), ('ask', 1), ('buddy', 1), ('outside', 1), ('toxic', 1), ('flags', 1), ('sweet', 1), ('jigginess', 1), ('waking', 1), ('many', 1), ('dive', 1), ('breached', 1), ('revisit', 1), ('bus', 1), ('partial', 1), ('afterhour', 1), ('sharks', 1), ('raise', 1), ('ncreases', 1), ('premiumm', 1), ('kndi', 1), ('electric', 1), ('ciscosynata', 1), ('gamechanger', 1), ('csco', 1), ('baltic', 1), ('declares', 1), ('cyber', 1), ('kindle', 1), ('family', 1), ('giddy', 1), ('holy', 1), ('batman', 1), ('plan', 1), ('comes', 1), ('fck', 1), ('bird', 1), ('celg', 1), ('agio', 1), ('stableize', 1), ('transformed', 1), ('appear', 1), ('exhausted', 1), ('quietly', 1), ('covering', 1), ('zigzag', 1), ('aug', 1), ('sinks', 1), ('magically', 1), ('mww', 1), ('raises', 1), ('recovery', 1), ('toward', 1), ('give', 1), ('encouragement', 1), ('questcor', 1), ('wilful', 1), ('calculations', 1), ('damages', 1), ('late', 1), ('stocking', 1), ('targe', 1), ('tuesday', 1), ('night', 1), ('regional', 1), ('banking', 1), ('somewhat', 1), ('tempting', 1), ('bermuda', 1), ('plenty', 1), ('daylight', 1), ('epic', 1), ('sne', 1), ('learnt', 1), ('integrating', 1), ('devices', 1), ('dog', 1), ('ltm', 1), ('formed', 1), ('trov', 1), ('reliably', 1), ('cancer', 1), ('mutations', 1), ('jackpot', 1), ('blew', 1), ('zmh', 1), ('bouncing', 1), ('wayyy', 1), ('gaining', 1), ('signals', 1), ('surges', 1), ('slowmotion', 1), ('disruption', 1), ('bto', 1), ('apples', 1), ('intermediate', 1), ('closng', 1), ('shortterm', 1), ('omnivision', 1), ('producing', 1), ('xover', 1), ('bidders', 1), ('expecting', 1), ('expectations', 1), ('notice', 1), ('doomsaying', 1), ('disappear', 1), ('climbs', 1), ('prob', 1), ('hznp', 1), ('amended', 1), ('bell', 1), ('easily', 1), ('safety', 1), ('equipment', 1), ('apart', 1), ('minutes', 1), ('mentioned', 1), ('suitee', 1), ('suitor', 1), ('capture', 1), ('impossible', 1), ('factual', 1), ('argument', 1), ('apparent', 1), ('buyout', 1), ('driving', 1), ('tplm', 1), ('petroleum', 1), ('expanding', 1), ('caliber', 1), ('isrg', 1), ('pozn', 1), ('est', 1), ('prior', 1), ('revised', 1), ('enters', 1), ('acquire', 1), ('yah', 1), ('viab', 1), ('sudden', 1), ('optimism', 1), ('gives', 1), ('constructive', 1), ('unt', 1), ('sdrl', 1), ('reversing', 1), ('inside', 1), ('triggers', 1), ('grew', 1), ('doubledigit', 1), ('hangin', 1), ('thread', 1), ('click', 1), ('away', 1), ('cliff', 1), ('process', 1), ('streamlining', 1), ('structure', 1), ('especially', 1), ('divestment', 1), ('adx', 1), ('lines', 1), ('partnership', 1), ('talks', 1), ('sides', 1), ('nimble', 1), ('lei', 1), ('slide', 1), ('defect', 1), ('junior', 1), ('fees', 1), ('called', 1), ('yelp', 1), ('scam', 1), ('try', 1), ('using', 1), ('innovate', 1), ('develo', 1), ('quiet', 1), ('nearterm', 1), ('accuses', 1), ('znga', 1), ('copyright', 1), ('joke', 1), ('buyerbeware', 1), ('azpn', 1), ('come', 1), ('wfc', 1), ('damage', 1), ('underestimate', 1), ('express', 1), ('scripts', 1), ('loses', 1), ('shorted', 1), ('smh', 1), ('sweepers', 1), ('stepping', 1), ('hearing', 1), ('rumours', 1), ('announcement', 1), ('nlsn', 1), ('marketing', 1), ('dropped', 1), ('release', 1), ('windows', 1), ('popular', 1), ('reentry', 1), ('citing', 1), ('spending', 1), ('trends', 1), ('anywhere', 1), ('shoots', 1), ('moon', 1), ('pumper', 1), ('tagged', 1), ('breakin', 1), ('dnkn', 1), ('eod', 1), ('explosive', 1), ('caught', 1), ('moneylosing', 1), ('junk', 1), ('bigger', 1), ('win', 1), ('honestly', 1), ('disappointing', 1), ('grab', 1), ('aside', 1), ('qcom', 1), ('worse', 1), ('kinds', 1), ('marriott', 1), ('wins', 1), ('starwood', 1), ('portion', 1), ('values', 1), ('lithium', 1), ('smack', 1), ('legitimate', 1), ('playing', 1), ('ignorant', 1), ('allow', 1), ('middle', 1), ('falls', 1), ('littlediscussed', 1), ('pro', 1), ('movement', 1), ('stuck', 1), ('mud', 1), ('warning', 1), ('upbeat', 1), ('raid', 1), ('curing', 1), ('steeper', 1), ('tag', 1), ('kneecapped', 1), ('bexp', 1), ('becoming', 1), ('goto', 1), ('site', 1), ('categories', 1), ('invests', 1), ('southeast', 1), ('asian', 1), ('retailer', 1), ('lazada', 1), ('efut', 1), ('attractive', 1), ('suitors', 1), ('delivering', 1), ('goods', 1), ('spill', 1), ('lotto', 1), ('shot', 1), ('howing', 1), ('divergences', 1), ('fas', 1), ('woulda', 1), ('pulled', 1), ('death', 1), ('horror', 1), ('event', 1), ('solarcity', 1), ('produce', 1), ('electricity', 1), ('power', 1), ('fleet', 1), ('kog', 1), ('shorters', 1), ('learned', 1), ('lesson', 1), ('mess', 1), ('skyrocketed', 1), ('general', 1), ('received', 1), ('wells', 1), ('fargo', 1), ('chill', 1), ('pill', 1), ('miles', 1), ('beyond', 1), ('experiencing', 1), ('rejection', 1), ('freewater', 1), ('famous', 1), ('lunch', 1), ('hour', 1), ('fade', 1), ('routine', 1), ('hands', 1), ('flushed', 1), ('metals', 1), ('greed', 1), ('resolved', 1), ('trapped', 1), ('whats', 1), ('marry', 1), ('christmas', 1), ('deep', 1), ('machinelearning', 1), ('territory', 1), ('supercomputer', 1), ('chk', 1), ('sellers', 1), ('saw', 1), ('unless', 1), ('censored', 1), ('ross', 1), ('rbc', 1), ('wedbush', 1), ('lowered', 1), ('sqns', 1), ('drops', 1), ('torture', 1), ('feb', 1), ('meeting', 1), ('sanofi', 1), ('often', 1), ('wrong', 1), ('compass', 1), ('bias', 1), ('lowquality', 1), ('ongoing', 1), ('drive', 1), ('please', 1), ('accept', 1), ('condolences', 1), ('thinking', 1), ('enter', 1), ('buysupport', 1), ('gotta', 1), ('threatening', 1), ('descending', 1), ('breakdown', 1), ('surveillance', 1), ('cameras', 1), ('infected', 1), ('malware', 1), ('hat', 1), ('brings', 1), ('upscale', 1), ('concept', 1), ('ownitdonttradeit', 1), ('skin', 1), ('gamepositive', 1), ('upgrade', 1), ('overweight', 1), ('callput', 1), ('buyers', 1)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2135"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "data.build_wordlist()\n",
    "data.wordlist.sort()\n",
    "def save_list(lines, filename):\n",
    "\tdata = '\\n'.join(lines)\n",
    "\tfile = open(filename, 'w')\n",
    "\tfile.write(data)\n",
    "\tfile.close()\n",
    "    \n",
    "save_list(data.wordlist,\"list_word_copie.txt\")\n",
    "len(data.wordlist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-23 10:27:38,603 : INFO : collecting all words and their counts\n",
      "2018-05-23 10:27:38,605 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2018-05-23 10:27:38,611 : INFO : collected 2214 word types from a corpus of 8314 raw words and 1700 sentences\n",
      "2018-05-23 10:27:38,612 : INFO : Loading a fresh vocabulary\n",
      "2018-05-23 10:27:38,617 : INFO : min_count=2 retains 1051 unique words (47% of original 2214, drops 1163)\n",
      "2018-05-23 10:27:38,619 : INFO : min_count=2 leaves 7151 word corpus (86% of original 8314, drops 1163)\n",
      "2018-05-23 10:27:38,624 : INFO : deleting the raw counts dictionary of 2214 items\n",
      "2018-05-23 10:27:38,625 : INFO : sample=0.001 downsamples 73 most-common words\n",
      "2018-05-23 10:27:38,626 : INFO : downsampling leaves estimated 6068 word corpus (84.9% of prior 7151)\n",
      "2018-05-23 10:27:38,630 : INFO : estimated required memory for 1051 words and 100 dimensions: 1366300 bytes\n",
      "2018-05-23 10:27:38,632 : INFO : resetting layer weights\n",
      "2018-05-23 10:27:38,656 : INFO : training model with 4 workers on 1051 vocabulary and 100 features, using sg=0 hs=0 sample=0.001 negative=5 window=10\n",
      "2018-05-23 10:27:38,670 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-23 10:27:38,671 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-23 10:27:38,674 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-23 10:27:38,686 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-23 10:27:38,687 : INFO : EPOCH - 1 : training on 8314 raw words (6033 effective words) took 0.0s, 310148 effective words/s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-23 10:27:38,704 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-23 10:27:38,707 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-23 10:27:38,711 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-23 10:27:38,719 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-23 10:27:38,720 : INFO : EPOCH - 2 : training on 8314 raw words (6066 effective words) took 0.0s, 315296 effective words/s\n",
      "2018-05-23 10:27:38,732 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-23 10:27:38,733 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-23 10:27:38,735 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-23 10:27:38,745 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-23 10:27:38,746 : INFO : EPOCH - 3 : training on 8314 raw words (6078 effective words) took 0.0s, 396488 effective words/s\n",
      "2018-05-23 10:27:38,764 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-23 10:27:38,766 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-23 10:27:38,769 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-23 10:27:38,780 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-23 10:27:38,781 : INFO : EPOCH - 4 : training on 8314 raw words (6067 effective words) took 0.0s, 294010 effective words/s\n",
      "2018-05-23 10:27:38,795 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2018-05-23 10:27:38,796 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2018-05-23 10:27:38,798 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2018-05-23 10:27:38,812 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2018-05-23 10:27:38,813 : INFO : EPOCH - 5 : training on 8314 raw words (6088 effective words) took 0.0s, 293321 effective words/s\n",
      "2018-05-23 10:27:38,814 : INFO : training on a 41570 raw words (30332 effective words) took 0.2s, 193516 effective words/s\n",
      "2018-05-23 10:27:38,815 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "2018-05-23 10:27:38,818 : INFO : precomputing L2-norms of word weight vectors\n",
      "2018-05-23 10:27:38,832 : INFO : saving Word2Vec object under microblogs_word_vector, separately None\n",
      "2018-05-23 10:27:38,834 : INFO : not storing attribute vectors_norm\n",
      "2018-05-23 10:27:38,835 : INFO : not storing attribute cum_table\n",
      "2018-05-23 10:27:38,851 : INFO : saved microblogs_word_vector\n"
     ]
    }
   ],
   "source": [
    "#------------------Word2vec--------------------------------------------------\n",
    "# Importing the built-in logging module\n",
    "import logging\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "#list_tweet=get_tweets(data)\n",
    "list_tweet=list(data.processed_data[\"token_spans\"])\n",
    "#sentences=review_sentences(list_tweet, remove_stopwords=True)\n",
    "sentences=list_tweet\n",
    "\n",
    "# Creating the model and setting values for the various parameters\n",
    "num_features = 100  # Word vector dimensionality\n",
    "min_word_count = 2 # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 10        # Context window size\n",
    "downsampling = 1e-3 # (0.001) Downsample setting for frequent words\n",
    "\n",
    "# Initializing the train model\n",
    "from gensim.models import word2vec\n",
    "print(\"Training model....\")\n",
    "model = word2vec.Word2Vec(sentences,\\\n",
    "                          workers=num_workers,\\\n",
    "                          size=num_features,\\\n",
    "                          min_count=min_word_count,\\\n",
    "                          window=context,\n",
    "                          sample=downsampling)\n",
    "\n",
    "# To make the model memory efficient\n",
    "model.init_sims(replace=True)\n",
    "\n",
    "# Saving the model for later use. Can be loaded using Word2Vec.load()\n",
    "model_name = \"microblogs_word_vector\"\n",
    "model.save(model_name)\n",
    "#model = gensim.models.Word2Vec.load(model_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 1700\n",
      "Review 1000 of 1700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n",
      "C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "columns=list(map(lambda i: \"word2vec_{0}\".format(i), range(0, model.vector_size)))\n",
    "import numpy as np\n",
    "# Function to average all word vectors in a paragraph\n",
    "def featureVecMethod(words, model, num_features):\n",
    "    # Pre-initialising empty numpy array for speed\n",
    "    featureVec = np.zeros(num_features,dtype=\"float32\")\n",
    "    nwords = 0\n",
    "    \n",
    "    #Converting Index2Word which is a list to a set for better speed in the execution.\n",
    "    index2word_set = set(model.wv.index2word)\n",
    "    for word in  words:\n",
    "        if word in index2word_set:\n",
    "            nwords = nwords + 1\n",
    "            featureVec = np.add(featureVec,model[word])\n",
    "    \n",
    "    # Dividing the result by number of words to get average\n",
    "    featureVec = np.divide(featureVec, nwords)\n",
    "    return featureVec\n",
    "\n",
    "# Function for calculating the average feature vector\n",
    "def getAvgFeatureVecs(reviews, model, num_features):\n",
    "    counter = 0\n",
    "    reviewFeatureVecs = np.zeros((len(reviews),num_features),dtype=\"float32\")\n",
    "    for review in reviews:\n",
    "        # Printing a status message every 1000th review\n",
    "        if counter%1000 == 0:\n",
    "            print(\"Review %d of %d\"%(counter,len(reviews)))\n",
    "            \n",
    "        reviewFeatureVecs[counter] = featureVecMethod(review, model, num_features)\n",
    "        counter = counter+1\n",
    "        \n",
    "    return reviewFeatureVecs\n",
    "\n",
    "#transformer les donner de train en moyenne de vecteurs word2vec \n",
    "trainDataVecs = getAvgFeatureVecs(list_tweet, model, num_features)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word2vec_0</th>\n",
       "      <th>word2vec_1</th>\n",
       "      <th>word2vec_2</th>\n",
       "      <th>word2vec_3</th>\n",
       "      <th>word2vec_4</th>\n",
       "      <th>word2vec_5</th>\n",
       "      <th>word2vec_6</th>\n",
       "      <th>word2vec_7</th>\n",
       "      <th>word2vec_8</th>\n",
       "      <th>word2vec_9</th>\n",
       "      <th>...</th>\n",
       "      <th>word2vec_90</th>\n",
       "      <th>word2vec_91</th>\n",
       "      <th>word2vec_92</th>\n",
       "      <th>word2vec_93</th>\n",
       "      <th>word2vec_94</th>\n",
       "      <th>word2vec_95</th>\n",
       "      <th>word2vec_96</th>\n",
       "      <th>word2vec_97</th>\n",
       "      <th>word2vec_98</th>\n",
       "      <th>word2vec_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.051253</td>\n",
       "      <td>-0.023325</td>\n",
       "      <td>0.043538</td>\n",
       "      <td>0.027733</td>\n",
       "      <td>-0.000572</td>\n",
       "      <td>0.072607</td>\n",
       "      <td>-0.005318</td>\n",
       "      <td>0.010735</td>\n",
       "      <td>-0.041563</td>\n",
       "      <td>0.062954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036176</td>\n",
       "      <td>-0.014585</td>\n",
       "      <td>-0.039435</td>\n",
       "      <td>-0.111412</td>\n",
       "      <td>0.062930</td>\n",
       "      <td>-0.070495</td>\n",
       "      <td>0.016721</td>\n",
       "      <td>0.044607</td>\n",
       "      <td>-0.010384</td>\n",
       "      <td>-0.033296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.053529</td>\n",
       "      <td>0.006522</td>\n",
       "      <td>-0.010614</td>\n",
       "      <td>-0.044481</td>\n",
       "      <td>0.023780</td>\n",
       "      <td>-0.007542</td>\n",
       "      <td>0.077665</td>\n",
       "      <td>-0.049258</td>\n",
       "      <td>-0.004307</td>\n",
       "      <td>0.020375</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079903</td>\n",
       "      <td>-0.090251</td>\n",
       "      <td>-0.023795</td>\n",
       "      <td>0.085427</td>\n",
       "      <td>-0.039390</td>\n",
       "      <td>0.078473</td>\n",
       "      <td>-0.001703</td>\n",
       "      <td>-0.040614</td>\n",
       "      <td>0.043617</td>\n",
       "      <td>0.005226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.018119</td>\n",
       "      <td>0.035739</td>\n",
       "      <td>0.008004</td>\n",
       "      <td>-0.045642</td>\n",
       "      <td>0.007712</td>\n",
       "      <td>0.049206</td>\n",
       "      <td>-0.060233</td>\n",
       "      <td>0.079258</td>\n",
       "      <td>0.102539</td>\n",
       "      <td>0.133056</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012059</td>\n",
       "      <td>-0.042455</td>\n",
       "      <td>-0.052324</td>\n",
       "      <td>-0.046249</td>\n",
       "      <td>0.015461</td>\n",
       "      <td>-0.107806</td>\n",
       "      <td>0.007347</td>\n",
       "      <td>0.092894</td>\n",
       "      <td>0.089371</td>\n",
       "      <td>-0.064510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.059184</td>\n",
       "      <td>0.022577</td>\n",
       "      <td>0.047356</td>\n",
       "      <td>0.111928</td>\n",
       "      <td>0.028518</td>\n",
       "      <td>0.068501</td>\n",
       "      <td>0.046202</td>\n",
       "      <td>0.037847</td>\n",
       "      <td>0.029007</td>\n",
       "      <td>0.121551</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047720</td>\n",
       "      <td>-0.033584</td>\n",
       "      <td>0.097955</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>0.028270</td>\n",
       "      <td>-0.103141</td>\n",
       "      <td>0.007089</td>\n",
       "      <td>-0.005249</td>\n",
       "      <td>0.005323</td>\n",
       "      <td>0.025629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.013634</td>\n",
       "      <td>0.018760</td>\n",
       "      <td>-0.013325</td>\n",
       "      <td>0.128147</td>\n",
       "      <td>0.000789</td>\n",
       "      <td>0.068625</td>\n",
       "      <td>0.041031</td>\n",
       "      <td>0.072915</td>\n",
       "      <td>-0.028941</td>\n",
       "      <td>0.104396</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058941</td>\n",
       "      <td>-0.074064</td>\n",
       "      <td>0.077727</td>\n",
       "      <td>-0.026842</td>\n",
       "      <td>0.043481</td>\n",
       "      <td>-0.046453</td>\n",
       "      <td>0.048603</td>\n",
       "      <td>0.030498</td>\n",
       "      <td>0.084867</td>\n",
       "      <td>-0.012579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word2vec_0  word2vec_1  word2vec_2  word2vec_3  word2vec_4  word2vec_5  \\\n",
       "0    0.051253   -0.023325    0.043538    0.027733   -0.000572    0.072607   \n",
       "1   -0.053529    0.006522   -0.010614   -0.044481    0.023780   -0.007542   \n",
       "2   -0.018119    0.035739    0.008004   -0.045642    0.007712    0.049206   \n",
       "3   -0.059184    0.022577    0.047356    0.111928    0.028518    0.068501   \n",
       "4   -0.013634    0.018760   -0.013325    0.128147    0.000789    0.068625   \n",
       "\n",
       "   word2vec_6  word2vec_7  word2vec_8  word2vec_9     ...       word2vec_90  \\\n",
       "0   -0.005318    0.010735   -0.041563    0.062954     ...          0.036176   \n",
       "1    0.077665   -0.049258   -0.004307    0.020375     ...          0.079903   \n",
       "2   -0.060233    0.079258    0.102539    0.133056     ...          0.012059   \n",
       "3    0.046202    0.037847    0.029007    0.121551     ...          0.047720   \n",
       "4    0.041031    0.072915   -0.028941    0.104396     ...          0.058941   \n",
       "\n",
       "   word2vec_91  word2vec_92  word2vec_93  word2vec_94  word2vec_95  \\\n",
       "0    -0.014585    -0.039435    -0.111412     0.062930    -0.070495   \n",
       "1    -0.090251    -0.023795     0.085427    -0.039390     0.078473   \n",
       "2    -0.042455    -0.052324    -0.046249     0.015461    -0.107806   \n",
       "3    -0.033584     0.097955    -0.010803     0.028270    -0.103141   \n",
       "4    -0.074064     0.077727    -0.026842     0.043481    -0.046453   \n",
       "\n",
       "   word2vec_96  word2vec_97  word2vec_98  word2vec_99  \n",
       "0     0.016721     0.044607    -0.010384    -0.033296  \n",
       "1    -0.001703    -0.040614     0.043617     0.005226  \n",
       "2     0.007347     0.092894     0.089371    -0.064510  \n",
       "3     0.007089    -0.005249     0.005323     0.025629  \n",
       "4     0.048603     0.030498     0.084867    -0.012579  \n",
       "\n",
       "[5 rows x 100 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rows=list(trainDataVecs)\n",
    "data_model = pd.DataFrame(rows, columns=columns)\n",
    "data_model.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>aal</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aapls</th>\n",
       "      <th>absolute</th>\n",
       "      <th>abyss</th>\n",
       "      <th>accept</th>\n",
       "      <th>acct</th>\n",
       "      <th>accumulate</th>\n",
       "      <th>...</th>\n",
       "      <th>yoku</th>\n",
       "      <th>zero</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>ziv</th>\n",
       "      <th>zmh</th>\n",
       "      <th>znga</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zuckerberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>719659409228451840</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>719904304207962112</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5329774</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>719891468173844480</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20091246</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5819749</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>709741154393133056</td>\n",
       "      <td>-0.296</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>17892972</td>\n",
       "      <td>-0.546</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>709834259687710720</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>708481442079068160</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>31971935</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>710187873492983808</td>\n",
       "      <td>-0.349</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>13915103</td>\n",
       "      <td>0.025</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10448993</td>\n",
       "      <td>0.486</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>24886266</td>\n",
       "      <td>0.308</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>12793642</td>\n",
       "      <td>-0.372</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>9408369</td>\n",
       "      <td>0.461</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>719909604654624768</td>\n",
       "      <td>0.408</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>9674584</td>\n",
       "      <td>-0.699</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>10041008</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20 rows × 2137 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID  label  aal  aapl  aapls  absolute  abyss  accept  \\\n",
       "0   719659409228451840  0.366    0     0      0         0      0       0   \n",
       "1   719904304207962112  0.638    0     0      0         0      0       0   \n",
       "2              5329774 -0.494    0     0      0         0      0       0   \n",
       "3   719891468173844480  0.460    0     0      0         0      0       0   \n",
       "4             20091246  0.403    0     0      0         0      0       0   \n",
       "5              5819749  0.000    0     0      0         0      0       0   \n",
       "6   709741154393133056 -0.296    0     0      0         0      0       0   \n",
       "7             17892972 -0.546    0     0      0         1      0       0   \n",
       "8   709834259687710720 -0.438    0     0      0         0      0       0   \n",
       "9   708481442079068160  0.408    0     0      0         0      0       0   \n",
       "10            31971935 -0.398    0     0      0         0      0       0   \n",
       "11  710187873492983808 -0.349    0     0      0         0      0       0   \n",
       "12            13915103  0.025    0     0      0         0      0       0   \n",
       "13            10448993  0.486    0     0      0         0      0       0   \n",
       "14            24886266  0.308    0     0      0         0      0       0   \n",
       "15            12793642 -0.372    0     0      0         0      0       0   \n",
       "16             9408369  0.461    0     0      0         0      0       0   \n",
       "17  719909604654624768  0.408    0     0      0         0      0       0   \n",
       "18             9674584 -0.699    0     0      0         0      0       0   \n",
       "19            10041008  0.495    0     0      0         0      0       0   \n",
       "\n",
       "    acct  accumulate     ...      yoku  zero  zigzag  ziv  zmh  znga  zombie  \\\n",
       "0      0           0     ...         0     0       0    0    0     0       0   \n",
       "1      0           0     ...         0     0       0    0    0     0       0   \n",
       "2      0           0     ...         0     0       0    0    0     0       0   \n",
       "3      0           0     ...         0     0       0    0    0     0       0   \n",
       "4      0           0     ...         0     0       0    0    0     0       0   \n",
       "5      0           0     ...         0     0       0    0    0     0       0   \n",
       "6      0           0     ...         0     0       0    0    0     0       0   \n",
       "7      0           0     ...         0     0       0    0    0     0       0   \n",
       "8      0           0     ...         0     0       0    0    0     0       0   \n",
       "9      0           0     ...         0     0       0    0    0     0       0   \n",
       "10     0           0     ...         0     0       0    0    0     0       0   \n",
       "11     0           0     ...         0     0       0    0    0     0       0   \n",
       "12     0           0     ...         0     0       0    0    0     0       0   \n",
       "13     0           0     ...         0     0       0    0    0     0       0   \n",
       "14     0           0     ...         0     0       0    0    0     0       0   \n",
       "15     0           0     ...         0     0       0    0    0     0       0   \n",
       "16     0           0     ...         0     0       0    0    0     0       0   \n",
       "17     0           0     ...         0     0       0    0    0     0       0   \n",
       "18     0           0     ...         0     0       0    0    0     0       0   \n",
       "19     0           0     ...         0     0       0    0    0     0       0   \n",
       "\n",
       "    zone  zuck  zuckerberg  \n",
       "0      0     0           0  \n",
       "1      0     0           0  \n",
       "2      0     0           0  \n",
       "3      0     0           0  \n",
       "4      0     0           0  \n",
       "5      0     0           0  \n",
       "6      0     0           0  \n",
       "7      0     0           0  \n",
       "8      0     0           0  \n",
       "9      0     0           0  \n",
       "10     0     0           0  \n",
       "11     0     0           0  \n",
       "12     0     0           0  \n",
       "13     0     0           0  \n",
       "14     0     0           0  \n",
       "15     0     0           0  \n",
       "16     0     0           0  \n",
       "17     0     0           0  \n",
       "18     0     0           0  \n",
       "19     0     0           0  \n",
       "\n",
       "[20 rows x 2137 columns]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow, labels = data.build_data_model()\n",
    "bow.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2137"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(bow.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>label</th>\n",
       "      <th>aal</th>\n",
       "      <th>aapl</th>\n",
       "      <th>aapls</th>\n",
       "      <th>absolute</th>\n",
       "      <th>abyss</th>\n",
       "      <th>accept</th>\n",
       "      <th>acct</th>\n",
       "      <th>accumulate</th>\n",
       "      <th>...</th>\n",
       "      <th>word2vec_90</th>\n",
       "      <th>word2vec_91</th>\n",
       "      <th>word2vec_92</th>\n",
       "      <th>word2vec_93</th>\n",
       "      <th>word2vec_94</th>\n",
       "      <th>word2vec_95</th>\n",
       "      <th>word2vec_96</th>\n",
       "      <th>word2vec_97</th>\n",
       "      <th>word2vec_98</th>\n",
       "      <th>word2vec_99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>719659409228451840</td>\n",
       "      <td>0.366</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.036176</td>\n",
       "      <td>-0.014585</td>\n",
       "      <td>-0.039435</td>\n",
       "      <td>-0.111412</td>\n",
       "      <td>0.062930</td>\n",
       "      <td>-0.070495</td>\n",
       "      <td>0.016721</td>\n",
       "      <td>0.044607</td>\n",
       "      <td>-0.010384</td>\n",
       "      <td>-0.033296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>719904304207962112</td>\n",
       "      <td>0.638</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.079903</td>\n",
       "      <td>-0.090251</td>\n",
       "      <td>-0.023795</td>\n",
       "      <td>0.085427</td>\n",
       "      <td>-0.039390</td>\n",
       "      <td>0.078473</td>\n",
       "      <td>-0.001703</td>\n",
       "      <td>-0.040614</td>\n",
       "      <td>0.043617</td>\n",
       "      <td>0.005226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5329774</td>\n",
       "      <td>-0.494</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012059</td>\n",
       "      <td>-0.042455</td>\n",
       "      <td>-0.052324</td>\n",
       "      <td>-0.046249</td>\n",
       "      <td>0.015461</td>\n",
       "      <td>-0.107806</td>\n",
       "      <td>0.007347</td>\n",
       "      <td>0.092894</td>\n",
       "      <td>0.089371</td>\n",
       "      <td>-0.064510</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>719891468173844480</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.047720</td>\n",
       "      <td>-0.033584</td>\n",
       "      <td>0.097955</td>\n",
       "      <td>-0.010803</td>\n",
       "      <td>0.028270</td>\n",
       "      <td>-0.103141</td>\n",
       "      <td>0.007089</td>\n",
       "      <td>-0.005249</td>\n",
       "      <td>0.005323</td>\n",
       "      <td>0.025629</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20091246</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.058941</td>\n",
       "      <td>-0.074064</td>\n",
       "      <td>0.077727</td>\n",
       "      <td>-0.026842</td>\n",
       "      <td>0.043481</td>\n",
       "      <td>-0.046453</td>\n",
       "      <td>0.048603</td>\n",
       "      <td>0.030498</td>\n",
       "      <td>0.084867</td>\n",
       "      <td>-0.012579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 2237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   ID  label  aal  aapl  aapls  absolute  abyss  accept  acct  \\\n",
       "0  719659409228451840  0.366    0     0      0         0      0       0     0   \n",
       "1  719904304207962112  0.638    0     0      0         0      0       0     0   \n",
       "2             5329774 -0.494    0     0      0         0      0       0     0   \n",
       "3  719891468173844480  0.460    0     0      0         0      0       0     0   \n",
       "4            20091246  0.403    0     0      0         0      0       0     0   \n",
       "\n",
       "   accumulate     ...       word2vec_90  word2vec_91  word2vec_92  \\\n",
       "0           0     ...          0.036176    -0.014585    -0.039435   \n",
       "1           0     ...          0.079903    -0.090251    -0.023795   \n",
       "2           0     ...          0.012059    -0.042455    -0.052324   \n",
       "3           0     ...          0.047720    -0.033584     0.097955   \n",
       "4           0     ...          0.058941    -0.074064     0.077727   \n",
       "\n",
       "   word2vec_93  word2vec_94  word2vec_95  word2vec_96  word2vec_97  \\\n",
       "0    -0.111412     0.062930    -0.070495     0.016721     0.044607   \n",
       "1     0.085427    -0.039390     0.078473    -0.001703    -0.040614   \n",
       "2    -0.046249     0.015461    -0.107806     0.007347     0.092894   \n",
       "3    -0.010803     0.028270    -0.103141     0.007089    -0.005249   \n",
       "4    -0.026842     0.043481    -0.046453     0.048603     0.030498   \n",
       "\n",
       "   word2vec_98  word2vec_99  \n",
       "0    -0.010384    -0.033296  \n",
       "1     0.043617     0.005226  \n",
       "2     0.089371    -0.064510  \n",
       "3     0.005323     0.025629  \n",
       "4     0.084867    -0.012579  \n",
       "\n",
       "[5 rows x 2237 columns]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow=pd.concat([bow,data_model],axis=1)\n",
    "bow.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "      <th>token_spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$F</td>\n",
       "      <td>5540055</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Putting on a little $F short]</td>\n",
       "      <td>[putting, little, short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>10752226</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short some]</td>\n",
       "      <td>[short, some]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$BAC</td>\n",
       "      <td>10920221</td>\n",
       "      <td>0.445</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[buying opportunity]</td>\n",
       "      <td>[buying, opportunity]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$SHOR</td>\n",
       "      <td>12971398</td>\n",
       "      <td>0.661</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Scaling Up on Long Position]</td>\n",
       "      <td>[scaling, long, position]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$JPM</td>\n",
       "      <td>16142438</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[its time to sell banks]</td>\n",
       "      <td>[its, time, sell, banks]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>$LMT</td>\n",
       "      <td>14073133</td>\n",
       "      <td>0.627</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Entering long]</td>\n",
       "      <td>[entering, long]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>$DNN</td>\n",
       "      <td>18479024</td>\n",
       "      <td>0.653</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[picked some up]</td>\n",
       "      <td>[picked, some]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147106</td>\n",
       "      <td>0.668</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[time to accumulate for a long position, far m...</td>\n",
       "      <td>[time, accumulate, for, long, position, far, m...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147107</td>\n",
       "      <td>0.460</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Looking for a strong bounce, Lunchtime rally ...</td>\n",
       "      <td>[looking, for, strong, bounce, lunchtime, rall...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147108</td>\n",
       "      <td>0.403</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Very intrigued with the technology and growth...</td>\n",
       "      <td>[very, intrigued, with, the, technology, and, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147109</td>\n",
       "      <td>0.000</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[short2 48 + - ***worked, puts up]</td>\n",
       "      <td>[worked, puts]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147110</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[Biggest Market Losers]</td>\n",
       "      <td>[biggest, market, losers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>$CRK</td>\n",
       "      <td>34147111</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[$GOOG $GOOGL would suck]</td>\n",
       "      <td>[goog, googl, would, suck]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>$SBUX</td>\n",
       "      <td>719890387314335744</td>\n",
       "      <td>0.483</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[Buying $SBUX on dip]</td>\n",
       "      <td>[buying, sbux, dip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>$GOOGL</td>\n",
       "      <td>708668814427348992</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>twitter</td>\n",
       "      <td>[is a short below 740, and is overbought]</td>\n",
       "      <td>[short, below, and, overbought]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>$F</td>\n",
       "      <td>5540056</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>[don't Putting on a down little $F short]</td>\n",
       "      <td>[dont, putting, down, little, short]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cashtag                  id  sentiment score      source  \\\n",
       "0       $F             5540055           -0.454  stocktwits   \n",
       "1    $AAPL            10752226           -0.464  stocktwits   \n",
       "2     $BAC            10920221            0.445  stocktwits   \n",
       "3    $SHOR            12971398            0.661  stocktwits   \n",
       "4     $JPM            16142438           -0.763  stocktwits   \n",
       "5     $LMT            14073133            0.627  stocktwits   \n",
       "6     $DNN            18479024            0.653  stocktwits   \n",
       "7     $CRK            34147106            0.668  stocktwits   \n",
       "8     $CRK            34147107            0.460  stocktwits   \n",
       "9     $CRK            34147108            0.403  stocktwits   \n",
       "10    $CRK            34147109            0.000  stocktwits   \n",
       "11    $CRK            34147110           -0.438  stocktwits   \n",
       "12    $CRK            34147111           -0.398  stocktwits   \n",
       "13   $SBUX  719890387314335744            0.483     twitter   \n",
       "14  $GOOGL  708668814427348992           -0.480     twitter   \n",
       "15      $F             5540056           -0.454  stocktwits   \n",
       "\n",
       "                                                spans  \\\n",
       "0                      [Putting on a little $F short]   \n",
       "1                                        [short some]   \n",
       "2                                [buying opportunity]   \n",
       "3                       [Scaling Up on Long Position]   \n",
       "4                            [its time to sell banks]   \n",
       "5                                     [Entering long]   \n",
       "6                                    [picked some up]   \n",
       "7   [time to accumulate for a long position, far m...   \n",
       "8   [Looking for a strong bounce, Lunchtime rally ...   \n",
       "9   [Very intrigued with the technology and growth...   \n",
       "10                 [short2 48 + - ***worked, puts up]   \n",
       "11                            [Biggest Market Losers]   \n",
       "12                          [$GOOG $GOOGL would suck]   \n",
       "13                              [Buying $SBUX on dip]   \n",
       "14          [is a short below 740, and is overbought]   \n",
       "15          [don't Putting on a down little $F short]   \n",
       "\n",
       "                                          token_spans  \n",
       "0                            [putting, little, short]  \n",
       "1                                       [short, some]  \n",
       "2                               [buying, opportunity]  \n",
       "3                           [scaling, long, position]  \n",
       "4                            [its, time, sell, banks]  \n",
       "5                                    [entering, long]  \n",
       "6                                      [picked, some]  \n",
       "7   [time, accumulate, for, long, position, far, m...  \n",
       "8   [looking, for, strong, bounce, lunchtime, rall...  \n",
       "9   [very, intrigued, with, the, technology, and, ...  \n",
       "10                                     [worked, puts]  \n",
       "11                          [biggest, market, losers]  \n",
       "12                         [goog, googl, would, suck]  \n",
       "13                                [buying, sbux, dip]  \n",
       "14                    [short, below, and, overbought]  \n",
       "15               [dont, putting, down, little, short]  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.do_process()\n",
    "data_test.processed_data.head(20)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('short', 4), ('long', 3), ('putting', 2), ('little', 2), ('buying', 2), ('position', 2), ('time', 2), ('for', 2), ('opportunity', 1), ('scaling', 1), ('sell', 1), ('banks', 1), ('entering', 1), ('picked', 1), ('accumulate', 1), ('far', 1), ('upside', 1), ('downside', 1), ('looking', 1), ('strong', 1), ('bounce', 1), ('lunchtime', 1), ('rally', 1), ('coming', 1), ('intrigued', 1), ('technology', 1), ('growth', 1), ('potential', 1), ('worked', 1), ('puts', 1), ('biggest', 1), ('market', 1), ('losers', 1), ('goog', 1), ('googl', 1), ('would', 1), ('suck', 1), ('sbux', 1), ('dip', 1), ('below', 1), ('overbought', 1), ('dont', 1)]\n",
      "42\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>accumulate</th>\n",
       "      <th>banks</th>\n",
       "      <th>below</th>\n",
       "      <th>biggest</th>\n",
       "      <th>bounce</th>\n",
       "      <th>buying</th>\n",
       "      <th>coming</th>\n",
       "      <th>dip</th>\n",
       "      <th>dont</th>\n",
       "      <th>...</th>\n",
       "      <th>scaling</th>\n",
       "      <th>sell</th>\n",
       "      <th>short</th>\n",
       "      <th>strong</th>\n",
       "      <th>suck</th>\n",
       "      <th>technology</th>\n",
       "      <th>time</th>\n",
       "      <th>upside</th>\n",
       "      <th>worked</th>\n",
       "      <th>would</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5540055</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10752226</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10920221</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12971398</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>16142438</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14073133</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>18479024</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>34147106</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>34147107</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>34147108</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>34147109</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>34147110</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>34147111</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>719890387314335744</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>708668814427348992</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5540056</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 43 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                    ID  accumulate  banks  below  biggest  bounce  buying  \\\n",
       "0              5540055           0      0      0        0       0       0   \n",
       "1             10752226           0      0      0        0       0       0   \n",
       "2             10920221           0      0      0        0       0       1   \n",
       "3             12971398           0      0      0        0       0       0   \n",
       "4             16142438           0      1      0        0       0       0   \n",
       "5             14073133           0      0      0        0       0       0   \n",
       "6             18479024           0      0      0        0       0       0   \n",
       "7             34147106           1      0      0        0       0       0   \n",
       "8             34147107           0      0      0        0       1       0   \n",
       "9             34147108           0      0      0        0       0       0   \n",
       "10            34147109           0      0      0        0       0       0   \n",
       "11            34147110           0      0      0        1       0       0   \n",
       "12            34147111           0      0      0        0       0       0   \n",
       "13  719890387314335744           0      0      0        0       0       1   \n",
       "14  708668814427348992           0      0      1        0       0       0   \n",
       "15             5540056           0      0      0        0       0       0   \n",
       "\n",
       "    coming  dip  dont  ...    scaling  sell  short  strong  suck  technology  \\\n",
       "0        0    0     0  ...          0     0      1       0     0           0   \n",
       "1        0    0     0  ...          0     0      1       0     0           0   \n",
       "2        0    0     0  ...          0     0      0       0     0           0   \n",
       "3        0    0     0  ...          1     0      0       0     0           0   \n",
       "4        0    0     0  ...          0     1      0       0     0           0   \n",
       "5        0    0     0  ...          0     0      0       0     0           0   \n",
       "6        0    0     0  ...          0     0      0       0     0           0   \n",
       "7        0    0     0  ...          0     0      0       0     0           0   \n",
       "8        1    0     0  ...          0     0      0       1     0           0   \n",
       "9        0    0     0  ...          0     0      0       0     0           1   \n",
       "10       0    0     0  ...          0     0      0       0     0           0   \n",
       "11       0    0     0  ...          0     0      0       0     0           0   \n",
       "12       0    0     0  ...          0     0      0       0     1           0   \n",
       "13       0    1     0  ...          0     0      0       0     0           0   \n",
       "14       0    0     0  ...          0     0      1       0     0           0   \n",
       "15       0    0     1  ...          0     0      1       0     0           0   \n",
       "\n",
       "    time  upside  worked  would  \n",
       "0      0       0       0      0  \n",
       "1      0       0       0      0  \n",
       "2      0       0       0      0  \n",
       "3      0       0       0      0  \n",
       "4      1       0       0      0  \n",
       "5      0       0       0      0  \n",
       "6      0       0       0      0  \n",
       "7      1       1       0      0  \n",
       "8      0       0       0      0  \n",
       "9      0       0       0      0  \n",
       "10     0       0       1      0  \n",
       "11     0       0       0      0  \n",
       "12     0       0       0      1  \n",
       "13     0       0       0      0  \n",
       "14     0       0       0      0  \n",
       "15     0       0       0      0  \n",
       "\n",
       "[16 rows x 43 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.build_wordlist()\n",
    "data_test.wordlist.sort()\n",
    "\n",
    "print(len(data_test.wordlist))\n",
    "bow_test= data_test.build_data_model()\n",
    "bow_test.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['ID', 'accumulate', 'banks', 'below', 'biggest', 'bounce', 'buying',\n",
       "       'coming', 'dip', 'dont', 'downside', 'entering', 'far', 'for', 'goog',\n",
       "       'googl', 'growth', 'intrigued', 'little', 'long', 'looking', 'losers',\n",
       "       'lunchtime', 'market', 'opportunity', 'overbought', 'picked',\n",
       "       'position', 'potential', 'puts', 'putting', 'rally', 'sbux', 'scaling',\n",
       "       'sell', 'short', 'strong', 'suck', 'technology', 'time', 'upside',\n",
       "       'worked', 'would'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Review 0 of 16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Globalnet\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:14: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "#data.wordlist.index(\"fit\")\n",
    "test_tweet=list(data_test.processed_data[\"token_spans\"])\n",
    "testDataVecs = getAvgFeatureVecs(test_tweet, model, num_features)\n",
    "row_test=list(testDataVecs)\n",
    "data_test_model = pd.DataFrame(row_test, columns=columns)\n",
    "bow_test=pd.concat([bow_test,data_test_model],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#convert bow dataframe to numpy matrix\\n\\nY = bow.label.as_matrix()\\nX_drop = bow.drop({\"label\",\"ID\"},axis=1)\\n\\nX=X_drop.as_matrix()\\n'"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "#convert bow dataframe to numpy matrix\n",
    "\n",
    "Y = bow.label.as_matrix()\n",
    "X_drop = bow.drop({\"label\",\"ID\"},axis=1)\n",
    "\n",
    "X=X_drop.as_matrix()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>accumulate</th>\n",
       "      <th>banks</th>\n",
       "      <th>below</th>\n",
       "      <th>biggest</th>\n",
       "      <th>bounce</th>\n",
       "      <th>buying</th>\n",
       "      <th>coming</th>\n",
       "      <th>dip</th>\n",
       "      <th>dont</th>\n",
       "      <th>...</th>\n",
       "      <th>yoku</th>\n",
       "      <th>zero</th>\n",
       "      <th>zigzag</th>\n",
       "      <th>ziv</th>\n",
       "      <th>zmh</th>\n",
       "      <th>znga</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zone</th>\n",
       "      <th>zuck</th>\n",
       "      <th>zuckerberg</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16 rows × 2240 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    ID  accumulate  banks  below  biggest  bounce  buying  coming  dip  dont  \\\n",
       "0    0           0      0      0        0       0       0       0    0     0   \n",
       "1    0           0      0      0        0       0       0       0    0     0   \n",
       "2    0           0      0      0        0       0       1       0    0     0   \n",
       "3    0           0      0      0        0       0       0       0    0     0   \n",
       "4    0           0      1      0        0       0       0       0    0     0   \n",
       "5    0           0      0      0        0       0       0       0    0     0   \n",
       "6    0           0      0      0        0       0       0       0    0     0   \n",
       "7    0           1      0      0        0       0       0       0    0     0   \n",
       "8    0           0      0      0        0       1       0       1    0     0   \n",
       "9    0           0      0      0        0       0       0       0    0     0   \n",
       "10   0           0      0      0        0       0       0       0    0     0   \n",
       "11   0           0      0      0        1       0       0       0    0     0   \n",
       "12   0           0      0      0        0       0       0       0    0     0   \n",
       "13   0           0      0      0        0       0       1       0    1     0   \n",
       "14   0           0      0      1        0       0       0       0    0     0   \n",
       "15   0           0      0      0        0       0       0       0    0     1   \n",
       "\n",
       "       ...      yoku  zero  zigzag  ziv  zmh  znga  zombie  zone  zuck  \\\n",
       "0      ...         0     0       0    0    0     0       0     0     0   \n",
       "1      ...         0     0       0    0    0     0       0     0     0   \n",
       "2      ...         0     0       0    0    0     0       0     0     0   \n",
       "3      ...         0     0       0    0    0     0       0     0     0   \n",
       "4      ...         0     0       0    0    0     0       0     0     0   \n",
       "5      ...         0     0       0    0    0     0       0     0     0   \n",
       "6      ...         0     0       0    0    0     0       0     0     0   \n",
       "7      ...         0     0       0    0    0     0       0     0     0   \n",
       "8      ...         0     0       0    0    0     0       0     0     0   \n",
       "9      ...         0     0       0    0    0     0       0     0     0   \n",
       "10     ...         0     0       0    0    0     0       0     0     0   \n",
       "11     ...         0     0       0    0    0     0       0     0     0   \n",
       "12     ...         0     0       0    0    0     0       0     0     0   \n",
       "13     ...         0     0       0    0    0     0       0     0     0   \n",
       "14     ...         0     0       0    0    0     0       0     0     0   \n",
       "15     ...         0     0       0    0    0     0       0     0     0   \n",
       "\n",
       "    zuckerberg  \n",
       "0            0  \n",
       "1            0  \n",
       "2            0  \n",
       "3            0  \n",
       "4            0  \n",
       "5            0  \n",
       "6            0  \n",
       "7            0  \n",
       "8            0  \n",
       "9            0  \n",
       "10           0  \n",
       "11           0  \n",
       "12           0  \n",
       "13           0  \n",
       "14           0  \n",
       "15           0  \n",
       "\n",
       "[16 rows x 2240 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_columns=list(bow.columns)\n",
    "for word in bow_columns:\n",
    "    current_row=[]\n",
    "    if (not word  in data_test.wordlist):\n",
    "        # add label\n",
    "        for idx in bow_test.index:\n",
    "            current_row.append(0)\n",
    "        bow_test[word]=current_row\n",
    "\n",
    "    \n",
    "bow_test.head(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['ID', 'accumulate', 'banks', 'below', 'biggest', 'bounce', 'buying',\n",
      "       'coming', 'dip', 'dont',\n",
      "       ...\n",
      "       'yoku', 'zero', 'zigzag', 'ziv', 'zmh', 'znga', 'zombie', 'zone',\n",
      "       'zuck', 'zuckerberg'],\n",
      "      dtype='object', length=2240)\n"
     ]
    }
   ],
   "source": [
    "#bow_test=pd.concat([bow_test,data_test_model],axis=1)\n",
    "list(bow_test.columns).index(\"word2vec_90\")\n",
    "print((bow_test.columns))\n",
    "#bow_test.sort_index(axis = 1, ascending = False)\n",
    "#bow.sort_index(axis = 1, ascending = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#bow = bow.rename(columns = {'fit': 'fit_'})\n",
    "\n",
    "#X_train, X_test, y_train, y_test = train_test_split(bow.iloc[:,2:], bow.iloc[:,1])                                                  \n",
    "#id_vals=X_test.index\n",
    "y_train=bow[\"label\"]\n",
    "X_train=bow.drop({\"ID\",\"label\"},axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CV_time = time.time()\\n# A parameter grid for XGBoost\\nparams = {\\'min_child_weight\\':[4,5], \\'gamma\\':[i/10.0 for i in range(3,6)],  \\'subsample\\':[i/10.0 for i in range(6,11)],\\n\\'colsample_bytree\\':[i/10.0 for i in range(6,11)], \\'max_depth\\': [2,3,4]}\\n\\n# Initialize XGB and GridSearch\\nxgb = XGBRegressor(nthread=-1) \\ngrid = GridSearchCV(xgb, params)\\ngrid.fit(X_train, y_train)\\nprint(\"--- %s seconds ---\" % (time.time() - CV_time))\\n'"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"CV_time = time.time()\n",
    "# A parameter grid for XGBoost\n",
    "params = {'min_child_weight':[4,5], 'gamma':[i/10.0 for i in range(3,6)],  'subsample':[i/10.0 for i in range(6,11)],\n",
    "'colsample_bytree':[i/10.0 for i in range(6,11)], 'max_depth': [2,3,4]}\n",
    "\n",
    "# Initialize XGB and GridSearch\n",
    "xgb = XGBRegressor(nthread=-1) \n",
    "grid = GridSearchCV(xgb, params)\n",
    "grid.fit(X_train, y_train)\n",
    "print(\"--- %s seconds ---\" % (time.time() - CV_time))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'grid.best_params_'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"grid.best_params_\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
      "       colsample_bytree=1.0, gamma=0.3, learning_rate=0.1,\n",
      "       max_delta_step=0, max_depth=4, min_child_weight=4, missing=None,\n",
      "       n_estimators=100, n_jobs=1, nthread=None, objective='reg:linear',\n",
      "       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
      "       seed=None, silent=True, subsample=0.8)\n"
     ]
    }
   ],
   "source": [
    "#Fitting XGB regressor with parameters obtained by Grid searchCV\n",
    "params={'colsample_bytree': 1.0,\n",
    " 'gamma': 0.3,\n",
    " 'max_depth': 4,\n",
    " 'min_child_weight': 4,\n",
    " 'subsample': 0.8}\n",
    "model_xgb = xgb.XGBRegressor(max_depth=4, min_child_weight= 4,gamma=0.3,subsample=0.8,colsample_bytree=1.0)\n",
    "\n",
    "model_xgb.fit(X_train,y_train)\n",
    "print (model_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error: 0.03527199670865589\n",
      "R2 score: 0.4613759455472385\n"
     ]
    }
   ],
   "source": [
    "#test model accuracy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "print(\"mean squared error:\" ,mean_squared_error(model_xgb.predict(X_train), y_train))\n",
    "print(\"R2 score:\" ,r2_score(model_xgb.predict(X_train),y_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nX_test=bow_test.drop({\"ID\"},axis=1)\\nX_test = X_test.as_matrix()\\n\\ny_test=data_test.processed_data[\"sentiment score\"]\\nid_vals=y_test.index\\n'"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "X_test=bow_test.drop({\"ID\"},axis=1)\n",
    "X_test = X_test.as_matrix()\n",
    "\n",
    "y_test=data_test.processed_data[\"sentiment score\"]\n",
    "id_vals=y_test.index\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#reorganize test columns as the train columns\n",
    "\n",
    "#X_test = bow_test.as_matrix()\n",
    "\n",
    "#X_test\n",
    "\n",
    "bow_test=bow_test[bow.drop([\"label\"],axis=1).columns]\n",
    "X_test=bow_test.drop({\"ID\"},axis=1)\n",
    "y_test=data_test.processed_data[\"sentiment score\"]\n",
    "id_vals=X_test.index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2236\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "136"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(len(bow_test.columns))\n",
    "bow_test.head()\n",
    "col=list(bow_test.columns)\n",
    "col.index(\"bank\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tweet_row=[]\\nlabels=[\"ID\",\"tweet\"]\\nfor num in id_vals:\\n    current_row=[]\\n    current_id = bow.loc[num, \"ID\"]\\n                \\n    current_tweet=data.processed_data.loc[data.processed_data[\\'id\\'] == current_id,\"spans\"].iloc[0]\\n\\n                \\n    current_row.append(current_id)\\n    current_row.append(current_tweet)\\n    \\n\\n   \\n\\n    tweet_row.append(current_row) \\ntweet_row'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#if train_test_split used\n",
    "\"\"\"tweet_row=[]\n",
    "labels=[\"ID\",\"tweet\"]\n",
    "for num in id_vals:\n",
    "    current_row=[]\n",
    "    current_id = bow.loc[num, \"ID\"]\n",
    "                \n",
    "    current_tweet=data.processed_data.loc[data.processed_data['id'] == current_id,\"spans\"].iloc[0]\n",
    "\n",
    "                \n",
    "    current_row.append(current_id)\n",
    "    current_row.append(current_tweet)\n",
    "    \n",
    "\n",
    "   \n",
    "\n",
    "    tweet_row.append(current_row) \n",
    "tweet_row\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet</th>\n",
       "      <th>old_pred</th>\n",
       "      <th>Prediction</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[Putting on a little $F short]</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>0.222071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[short some]</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>0.222071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[buying opportunity]</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.278782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[Scaling Up on Long Position]</td>\n",
       "      <td>0.661</td>\n",
       "      <td>0.379209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[its time to sell banks]</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>0.119029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>[Entering long]</td>\n",
       "      <td>0.627</td>\n",
       "      <td>0.379209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>[picked some up]</td>\n",
       "      <td>0.653</td>\n",
       "      <td>0.259054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>[time to accumulate for a long position, far m...</td>\n",
       "      <td>0.668</td>\n",
       "      <td>0.407423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>[Looking for a strong bounce, Lunchtime rally ...</td>\n",
       "      <td>0.460</td>\n",
       "      <td>0.293277</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>[Very intrigued with the technology and growth...</td>\n",
       "      <td>0.403</td>\n",
       "      <td>0.289607</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>[short2 48 + - ***worked, puts up]</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.063929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>[Biggest Market Losers]</td>\n",
       "      <td>-0.438</td>\n",
       "      <td>0.238614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>[$GOOG $GOOGL would suck]</td>\n",
       "      <td>-0.398</td>\n",
       "      <td>0.117232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>[Buying $SBUX on dip]</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.278782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>[is a short below 740, and is overbought]</td>\n",
       "      <td>-0.480</td>\n",
       "      <td>0.222071</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>[don't Putting on a down little $F short]</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>0.222071</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                tweet  old_pred  Prediction\n",
       "0                      [Putting on a little $F short]    -0.454    0.222071\n",
       "1                                        [short some]    -0.464    0.222071\n",
       "2                                [buying opportunity]     0.445    0.278782\n",
       "3                       [Scaling Up on Long Position]     0.661    0.379209\n",
       "4                            [its time to sell banks]    -0.763    0.119029\n",
       "5                                     [Entering long]     0.627    0.379209\n",
       "6                                    [picked some up]     0.653    0.259054\n",
       "7   [time to accumulate for a long position, far m...     0.668    0.407423\n",
       "8   [Looking for a strong bounce, Lunchtime rally ...     0.460    0.293277\n",
       "9   [Very intrigued with the technology and growth...     0.403    0.289607\n",
       "10                 [short2 48 + - ***worked, puts up]     0.000    0.063929\n",
       "11                            [Biggest Market Losers]    -0.438    0.238614\n",
       "12                          [$GOOG $GOOGL would suck]    -0.398    0.117232\n",
       "13                              [Buying $SBUX on dip]     0.483    0.278782\n",
       "14          [is a short below 740, and is overbought]    -0.480    0.222071\n",
       "15          [don't Putting on a down little $F short]    -0.454    0.222071"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Predict \n",
    "#output=grid.best_estimator_.predict(X_test)\n",
    "tweet_row=list(data_test.processed_data[\"spans\"])\n",
    "old_pred=list(data_test.processed_data[\"sentiment score\"])\n",
    "output = model_xgb.predict(X_test)\n",
    "#final_df = pd.DataFrame(tweet_row, columns=labels)\n",
    "final_df = pd.DataFrame()\n",
    "final_df[\"tweet\"] = tweet_row\n",
    "\n",
    "final_df[\"old_pred\"] = old_pred\n",
    "\n",
    "final_df[\"Prediction\"] = output\n",
    "final_df.to_csv(\"Output_1.csv\",sep=\",\")\n",
    "final_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean squared error: 0.24105996442903568\n",
      "R2 score: -27.359540031512054\n"
     ]
    }
   ],
   "source": [
    "#test model accuracy\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "\n",
    "print(\"mean squared error:\" ,mean_squared_error(output, y_test))\n",
    "print(\"R2 score:\" ,r2_score(output,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
